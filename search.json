[
  {
    "objectID": "lychee/out.html",
    "href": "lychee/out.html",
    "title": "cynkra blog",
    "section": "",
    "text": "Status\nCount\n\n\n\n\nğŸ” Total\n1039\n\n\nâœ… Successful\n991\n\n\nâ³ Timeouts\n0\n\n\nğŸ”€ Redirected\n0\n\n\nğŸ‘» Excluded\n48\n\n\nâ“ Unknown\n0\n\n\nğŸš« Errors\n0\n\n\n\nFull Github Actions output"
  },
  {
    "objectID": "lychee/out.html#summary",
    "href": "lychee/out.html#summary",
    "title": "cynkra blog",
    "section": "",
    "text": "Status\nCount\n\n\n\n\nğŸ” Total\n1039\n\n\nâœ… Successful\n991\n\n\nâ³ Timeouts\n0\n\n\nğŸ”€ Redirected\n0\n\n\nğŸ‘» Excluded\n48\n\n\nâ“ Unknown\n0\n\n\nğŸš« Errors\n0\n\n\n\nFull Github Actions output"
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html",
    "href": "posts/2020-04-02-dm/index.html",
    "title": "Relational data models in R",
    "section": "",
    "text": "Relational databases are powerful tools for analyzing and manipulating data. However, many modeling workflows require a great deal of time and effort to wrangle data from databases to place it into a flat data frame or table format. Only then the actual data analysis can start."
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html#why-a-relational-model",
    "href": "posts/2020-04-02-dm/index.html#why-a-relational-model",
    "title": "Relational data models in R",
    "section": "Why a relational model?",
    "text": "Why a relational model?\nWith the proper tools, analysis can begin using a relational model that works directly with the database. And, if wrangling is still required, R users can leverage this powerful and proven SQL approach to data organization and manipulation.\nThe dm package takes the primary advantage of databases â€“ relational modeling â€“ and brings it to R. In relational databases, tables or data frames link through primary keys (PK) and foreign keys (FK). Instead of having a single, wide table to work with, data is segmented across multiple tables to eliminate or reduce redundancies. This process is called normalization. A classic example is storing unique identifiers in a large table and looking up values for these unique identifiers in a small data frame. This approach lets an analysis run without reading or writing look-up values until necessary because unique identifiers are enough for most of the runtime.\ndm 0.1.1 is available on CRAN. You can now download and install dm, from CRAN, with the following command:\ninstall.packages(\"dm\")"
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html#connect-to-a-database",
    "href": "posts/2020-04-02-dm/index.html#connect-to-a-database",
    "title": "Relational data models in R",
    "section": "Connect to a database",
    "text": "Connect to a database\nWe connect to a relational dataset repository with a database server that is publicly accessible without registration. There is a financial dataset that contains loan data, along with relevant information and transactions. We chose this loan dataset because the relationships between loan, account, and transcactions tables are good representations of databases that record real-world business transactions.\nThe dataset page lists the credentials required for connecting to the database:\n\nhostname: relational.fit.cvut.cz\nport: 3306\nusername: guest\npassword: relational\ndatabase: Financial_ijs\n\nThese can be used, for example, in MySQL Workbench to download the CSV data manually. To automate and keep the data in the database for as long as possible, we can connect to the database from R through its database interface to access the tables:\nlibrary(RMariaDB)\nmy_db &lt;- dbConnect(\n  MariaDB(),\n  user = 'guest',\n  password = 'relational',\n  dbname = 'Financial_ijs',\n  host = 'relational.fit.cvut.cz'\n)\ndbListTables(my_db)\n\n## [1] \"accounts\"  \"cards\"     \"clients\"   \"disps\"     \"districts\" \"loans\"\n## [7] \"orders\"    \"tkeys\"     \"trans\"\nBy creating a dm object from the connection, we get access to all tables:\nlibrary(dm)\nmy_dm &lt;- dm_from_src(my_db)\n\nmy_dm\n\n## â”€â”€ Table source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## src:  mysql  [guest@relational.fit.cvut.cz:NA/Financial_ijs]\n## â”€â”€ Metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Tables: `accounts`, `cards`, `clients`, `disps`, `districts`, â€¦ (9 total)\n## Columns: 57\n## Primary keys: 0\n## Foreign keys: 0\nnames(my_dm)\n\n## [1] \"accounts\"  \"cards\"     \"clients\"   \"disps\"     \"districts\" \"loans\"\n## [7] \"orders\"    \"tkeys\"     \"trans\"\nmy_dm$accounts\n\n## # Source:   table&lt;accounts&gt; [?? x 4]\n## # Database: mysql [guest@relational.fit.cvut.cz:NA/Financial_ijs]\n##      id district_id frequency        date\n##   &lt;int&gt;       &lt;int&gt; &lt;chr&gt;            &lt;date&gt;\n## 1     1          18 POPLATEK MESICNE 1995-03-24\n## 2     2           1 POPLATEK MESICNE 1993-02-26\n## 3     3           5 POPLATEK MESICNE 1997-07-07\n## 4     4          12 POPLATEK MESICNE 1996-02-21\n## 5     5          15 POPLATEK MESICNE 1997-05-30\n## 6     6          51 POPLATEK MESICNE 1994-09-27\n## # â€¦ with more rows\nThe components of this particular dm object are lazy tables powered by dbplyr. This package translates the dplyr grammar of data manipulation into queries the database server understands. The advantage to a lazy table is that there is no data download until results are collected for printing or local processing. Below, the summary operation is computed on the database, and only the results are sent back to the R session.\nlibrary(dplyr)\nmy_dm$accounts %&gt;%\n  group_by(district_id) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup()\n\n## # Source:   lazy query [?? x 2]\n## # Database: mysql [guest@relational.fit.cvut.cz:NA/Financial_ijs]\n##   district_id n\n##         &lt;int&gt; &lt;int64&gt;\n## 1           1 554\n## 2           2  42\n## 3           3  50\n## 4           4  48\n## 5           5  65\n## 6           6  48\n## # â€¦ with more rows\nIf the data fits into your RAM, a database connection is not required to use dm. The collect() command downloads all tables for our dm object.\nmy_local_dm &lt;-\n  my_dm %&gt;%\n  collect()\n\nobject.size(my_local_dm)\n## 77922024 bytes\nmy_local_dm$accounts %&gt;%\n  group_by(district_id) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup()\n\n## # A tibble: 77 x 2\n##   district_id     n\n##         &lt;int&gt; &lt;int&gt;\n## 1           1   554\n## 2           2    42\n## 3           3    50\n## 4           4    48\n## 5           5    65\n## 6           6    48\n## # â€¦ with 71 more rows\nA dm object can also be created from individual data frames with the dm() function."
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html#define-primary-and-foreign-keys",
    "href": "posts/2020-04-02-dm/index.html#define-primary-and-foreign-keys",
    "title": "Relational data models in R",
    "section": "Define primary and foreign keys",
    "text": "Define primary and foreign keys\nRelational database tables link to each other via primary and foreign keys. The model diagram provided by our test database illustrates the intended relationships.\nHowever, it turns out this is not an accurate representation of the entities and relationships within the database:\n\nTable names in our database have the plural form; in the diagram itâ€™s singular.\nThere is a tkeys table available in the database that is not listed in the model diagram.\nThe Financial_std database is similar, but different from the one that we work with, Financial_ijs.\n\nBearing these discrepancies in mind, we can define suitable primary and foreign keys for our dm object. The documentation suggests that the loans table is the most important one. We color the target table separately with dm_color().\n# Defining PKs and FKs\nmy_dm_keys &lt;-\n  my_local_dm %&gt;%\n  dm_add_pk(districts, id) %&gt;%\n  dm_add_pk(accounts, id) %&gt;%\n  dm_add_pk(clients, id) %&gt;%\n  dm_add_pk(loans, id) %&gt;%\n  dm_add_pk(orders, id) %&gt;%\n  dm_add_pk(trans, id) %&gt;%\n  dm_add_pk(disps, id) %&gt;%\n  dm_add_pk(cards, id) %&gt;%\n  dm_add_fk(loans, account_id, accounts) %&gt;%\n  dm_add_fk(orders, account_id, accounts) %&gt;%\n  dm_add_fk(trans, account_id, accounts) %&gt;%\n  dm_add_fk(disps, account_id, accounts) %&gt;%\n  dm_add_fk(disps, client_id, clients) %&gt;%\n  dm_add_fk(accounts, district_id, districts) %&gt;%\n  dm_add_fk(cards, disp_id, disps) %&gt;%\n  dm_set_colors(green = loans)\n\n# Draw the visual model\nmy_dm_keys %&gt;%\n  dm_draw()\n\nThe discrepancies highlight the importance of being able to define primary and foreign keys. Most of the challenges in manipulating data are not syntax knowledge gaps. The syntax can always be looked up with search engines. Knowledge gaps regarding how data is organized are much more common as stumbling blocks for R users when working with distributed data.\nInsight into the structure of a database using the built-in dm_draw() function provides an instant efficiency boost. Combined with defining unique identifiers (primary keys) and how they are found by other tables (foreign keys), an R user can quickly clarify the structures with which they are working.\nTo assist with this process of defining the structure, dm comes with a built-in helper to check the referential integrity of the dataset:\nmy_dm_keys %&gt;%\n  dm_examine_constraints()\n\n## â„¹ All constraints satisfied."
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html#create-a-dataset-ready-for-analysis",
    "href": "posts/2020-04-02-dm/index.html#create-a-dataset-ready-for-analysis",
    "title": "Relational data models in R",
    "section": "Create a dataset ready for analysis",
    "text": "Create a dataset ready for analysis\nFor modeling, a flat table or matrix is required as input. If normalization is the process of splitting up a table to reduce redundancies, joining multiple tables together is called denormalizing.\nThe dm_squash_to_tbl() function creates a denormalized table by performing a cascading join between cards and all outgoing foreign keys. A join is the SQL term for combining some or all of the unique columns between 2 or more tables into a single table using the appropriate keys. In this case, the cards table has a foreign key to disps table, which has a foreign key to accounts, which also has a foreign key to the districts table. These foreign key relationships are then used in a cascading join within the dm_squash_to_tbl() function, without having to specify the relationships because they are already encoded within the dm object.\nmy_dm_keys %&gt;%\n  dm_squash_to_tbl(cards)\n\n## Renamed columns:\n## * type -&gt; cards$cards.type, disps$disps.type\n## * district_id -&gt; accounts$accounts.district_id, clients$clients.district_id\n\n## # A tibble: 892 x 28\n##      id disp_id cards.type issued     client_id account_id disps.type\n##   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      &lt;date&gt;         &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n## 1     1       9 gold       1998-10-16         9          7 OWNER\n## 2     2      19 classic    1998-03-13        19         14 OWNER\n## 3     3      41 gold       1995-09-03        41         33 OWNER\n## 4     4      42 classic    1998-11-26        42         34 OWNER\n## 5     5      51 junior     1995-04-24        51         43 OWNER\n## 6     7      56 classic    1998-06-11        56         48 OWNER\n## # â€¦ with 886 more rows, and 21 more variables: accounts.district_id &lt;int&gt;,\n## #   frequency &lt;chr&gt;, date &lt;date&gt;, A2 &lt;chr&gt;, A3 &lt;chr&gt;, A4 &lt;int&gt;, A5 &lt;int&gt;,\n## #   A6 &lt;int&gt;, A7 &lt;int&gt;, A8 &lt;int&gt;, A9 &lt;int&gt;, A10 &lt;dbl&gt;, A11 &lt;int&gt;, A12 &lt;dbl&gt;,\n## #   A13 &lt;dbl&gt;, A14 &lt;int&gt;, A15 &lt;int&gt;, A16 &lt;int&gt;, birth_number &lt;chr&gt;,\n## #   clients.district_id &lt;int&gt;, tkey_id &lt;int&gt;\nWe have an analysis-ready dataset available to use!"
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html#transform-data-in-a-dm",
    "href": "posts/2020-04-02-dm/index.html#transform-data-in-a-dm",
    "title": "Relational data models in R",
    "section": "Transform data in a dm",
    "text": "Transform data in a dm\nData transformation in dm is done by zooming on the table with which you would like to work. A zoomed dm supports dplyr operations on the zoomed table: simple transformations, grouped operations, joins, and more.\nmy_dm_keys %&gt;%\n  dm_zoom_to(accounts) %&gt;%\n  group_by(district_id) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(districts)\n\n## # Zoomed table: accounts\n## # A tibble:     77 x 17\n##   district_id     n A2    A3        A4    A5    A6    A7    A8    A9   A10   A11\n##         &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n## 1           1   554 Hl.mâ€¦ Pragâ€¦ 1.20e6     0     0     0     1     1   100 12541\n## 2           2    42 Beneâ€¦ centâ€¦ 8.89e4    80    26     6     2     5    47  8507\n## 3           3    50 Beroâ€¦ centâ€¦ 7.52e4    55    26     4     1     5    42  8980\n## 4           4    48 Kladâ€¦ centâ€¦ 1.50e5    63    29     6     2     6    67  9753\n## 5           5    65 Kolin centâ€¦ 9.56e4    65    30     4     1     6    51  9307\n## 6           6    48 Kutnâ€¦ centâ€¦ 7.80e4    60    23     4     2     4    52  8546\n## # â€¦ with 71 more rows, and 5 more variables: A12 &lt;dbl&gt;, A13 &lt;dbl&gt;, A14 &lt;int&gt;,\n## #   A15 &lt;int&gt;, A16 &lt;int&gt;\nThe columns used by left_join() to consolidate tables are inferred from the primary and foreign keys already encoded within the dm object."
  },
  {
    "objectID": "posts/2020-04-02-dm/index.html#reproducible-dataflows-with-dm",
    "href": "posts/2020-04-02-dm/index.html#reproducible-dataflows-with-dm",
    "title": "Relational data models in R",
    "section": "Reproducible dataflows with dm",
    "text": "Reproducible dataflows with dm\nWalking through dmâ€™s data modeling fundamentals, adding keys, and drawing the structure, will help R users better understand data from external databases or apply best practices from relational data modeling to their local data.\nYou can immediately start testing on an Rstudio cloud instance! For more examples and explanations, check out the documentation page. Install this package today!"
  },
  {
    "objectID": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html",
    "href": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html",
    "title": "2024 road and para-cycling road world championships: preliminaRy analysis",
    "section": "",
    "text": "Codelibrary(gpx)\nlibrary(dplyr)\nlibrary(leaflet)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(rayshader)\nlibrary(ggrgl)\nlibrary(gtsummary)\n\nlibrary(rgl)\noptions(rgl.useNULL = TRUE)\nsetupKnitr(autoprint = TRUE)\nFrom Sept 21 to Sept 29, Zurich will welcome the 2024 road and para-cycling road world championships. To mark the occasion, my friends and I went to do the 2 first loops (â€œonlyâ€ 140km, 1700m elevation) of the Elite Mens circuit that will start from Winterthur on Sept 29. 273km and 4470m of pure pleasure! I am not sure whether riders will have time to enjoy the view. At least I hope they have a better weather than us."
  },
  {
    "objectID": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html#circuit-overview",
    "href": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html#circuit-overview",
    "title": "2024 road and para-cycling road world championships: preliminaRy analysis",
    "section": "Circuit overview",
    "text": "Circuit overview\nGet the GPX file\nThe road circuit is available as GPX format, which can be imported by any route planner like Komoot or Strava â€¦ or with R :).\nThere are various way to read such format in R, as shown in this other article. For this blog post, we leverage the gpx package:\n\nCodezch_gpx &lt;- read_gpx(\"GPX-22-Winterthur-Zurich-1.gpx\")\nglimpse(zch_gpx)\n\nList of 3\n $ routes   :List of 1\n  ..$ :'data.frame':    10687 obs. of  5 variables:\n  .. ..$ Elevation : num [1:10687] 438 438 438 438 438 ...\n  .. ..$ Time      : POSIXct[1:10687], format: \"2023-11-03 06:08:27\" \"2023-11-03 06:08:29\" ...\n  .. ..$ Latitude  : num [1:10687] 47.5 47.5 47.5 47.5 47.5 ...\n  .. ..$ Longitude : num [1:10687] 8.72 8.72 8.72 8.72 8.72 ...\n  .. ..$ extensions: logi [1:10687] NA NA NA NA NA NA ...\n $ tracks   :List of 1\n  ..$ :'data.frame':    0 obs. of  4 variables:\n  .. ..$ Elevation: logi(0) \n  .. ..$ Time     : logi(0) \n  .. ..$ Latitude : logi(0) \n  .. ..$ Longitude: logi(0) \n $ waypoints:'data.frame':  9 obs. of  6 variables:\n  ..$ Elevation  : num [1:9] 464 411 411 411 411 ...\n  ..$ Time       : POSIXct[1:9], format: NA NA ...\n  ..$ Latitude   : num [1:9] 47.5 47.4 47.4 47.4 47.4 ...\n  ..$ Longitude  : num [1:9] 8.76 8.55 8.55 8.55 8.55 ...\n  ..$ Name       : chr [1:9] \"km 0\" \"Info\" \"Info\" \"Info\" ...\n  ..$ Description: chr [1:9] NA NA NA NA ...\n\n\nWe obtain a list containing 3 dataframes, namely routes, tracks and waypoints.\nVisualize the route\nIn the following, we can visualize these data on an interactive map. To do so, I chose the leaflet package. First, we pass the data to leaflet(), then we select a map provider with addTiles(). I like to use the a rather light one as I want the user to focus on the route trace and not on any single mountain or village. Therefore, I went for the CartoDB.Positron tiles, which you can test here. The trace is injected with addPolylines, passing the Latitude and Longitude columns of our dataset, as well as few styling parameters such as color, line weight and opacity.\nThen, we add the starting point and end point of the race available in zch_gpx$waypoints. Note that since the last loop goes 7 times around the finish line, the GPS coordinates are duplicated so we only extract zch_gpx$waypoints[1, ] and zch_gpx$waypoints[2, ]. Those data are given to the addCircleMarkers() function, which allows to pass extra information like popups or labels. Finally, I wanted to highlight the 4 most significant climbs of this tour:\n- Buch am Irchel: 4.83km at 4.2%.\n- Kyburg: 1.28km at 10.3%.\n- Binz: 3.7km at 4.4%.\n- Witikon: 2.63km at 5.3%.\nI first had to locate the exact coordinates of each climb (the marker is put at the top). Thatâ€™s the reason why you can see a few JavaScript lines at the end of the script. This is a helper passed to htmlwidgets::onRender(), which allowed me to click on the map and get the coordinates in an alert window.\nfunction(x, el, data) {\n    var map = this;\n    map.on('click', function(e) {\n        var coord = e.latlng;\n        var lat = coord.lat;\n        var lng = coord.lng;\n        alert('You clicked the map at latitude: ' + lat + ' and       longitude: ' + lng);\n    });\n}\nI then copied the results and passed them to addMarkers(). I faced some challenges while trying to get the markers render well when zooming in and out. Be careful to fix the X and Y anchors and specify the size of the icon you use:\nicon = list(\n    iconUrl = \"https://www.vanuatubeachbar.com/wp-content/uploadleaflet-maps-marker-icons/mountains.png\",\n    iconWidth = 32,\n    iconHeight = 37,\n    iconAnchorX = 0,\n    iconAnchorY = 0\n)\nThe above setting ensures that at any level of zoom, the icon stays on the trace.\n\nCodeleaflet(zch_gpx$routes[[1]]) |&gt;\n    addTiles(\n        urlTemplate = \"https://{s}.basemaps.cartocdn.com/light_all/{z}/{x}/{y}{r}.png\",\n        attribution = '&copy; &lt;a href=\"https://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt; contributors &copy; &lt;a href=\"https://carto.com/attributions\"&gt;CARTO&lt;/a&gt;',\n        options = tileOptions(\n            subdomains = \"abcd\",\n            maxZoom = 20\n        )\n    ) |&gt;\n    addPolylines(lat = ~Latitude, lng = ~Longitude, color = \"#000000\", opacity = 0.8, weight = 3) |&gt;\n    addCircleMarkers(data = zch_gpx$waypoints[1, ], lat = ~Latitude, lng = ~Longitude, color = \"#3eaf15\", opacity = 0.8, weight = 5, radius = 10, label = \"Start of race\") |&gt;\n    addCircleMarkers(data = zch_gpx$waypoints[2, ], lat = ~Latitude, lng = ~Longitude, color = \"#e73939\", opacity = 0.8, weight = 5, radius = 10, label = \"End of race\") |&gt;\n    addMarkers(\n        lng = 8.64389380440116,\n        lat = 47.5413932128899,\n        icon = list(\n            iconUrl = \"https://www.vanuatubeachbar.com/wp-content/uploads/leaflet-maps-marker-icons/mountains.png\",\n            iconWidth = 32,\n            iconHeight = 37,\n            iconAnchorX = 0,\n            iconAnchorY = 0\n        ),\n        label = \"Buch am Irchel: 4.83km at 4.2% **\"\n    ) |&gt;\n    addMarkers(\n        lng = 8.743660245090725,\n        lat = 47.45665840019784,\n        icon = list(\n            iconUrl = \"https://www.vanuatubeachbar.com/wp-content/uploads/leaflet-maps-marker-icons/mountains.png\",\n            iconWidth = 32,\n            iconHeight = 37,\n            iconAnchorX = 0,\n            iconAnchorY = 0\n        ),\n        label = \"Kyburg: 1.28km at 10.3% ****\"\n    ) |&gt;\n    addMarkers(\n        lng = 8.624014738015832,\n        lat = 47.351512429613024,\n        icon = list(\n            iconUrl = \"https://www.vanuatubeachbar.com/wp-content/uploads/leaflet-maps-marker-icons/mountains.png\",\n            iconWidth = 32,\n            iconHeight = 37,\n            iconAnchorX = 0,\n            iconAnchorY = 0\n        ),\n        label = \"Maur-Binz: 3.7km at 4.4% **\"\n    ) |&gt;\n    addMarkers(\n        lng = 8.607488349080088,\n        lat = 47.36219723777833,\n        icon = list(\n            iconUrl = \"https://www.vanuatubeachbar.com/wp-content/uploads/leaflet-maps-marker-icons/mountains.png\",\n            iconWidth = 32,\n            iconHeight = 37,\n            iconAnchorX = 0,\n            iconAnchorY = 0\n        ),\n        label = \"ZurichbergStrasse/Witikon: 2.63km at 5.3% **\"\n    ) |&gt;\n    htmlwidgets::onRender(\n      \"function(x, el, data) {\n        var map = this;\n        map.on('click', function(e) {\n        var coord = e.latlng;\n        var lat = coord.lat;\n        var lng = coord.lng;\n        console.log('You clicked the map at latitude: ' + lat + ' and       longitude: ' + lng);\n        });\n      }\"  \n    )\n\n\n\n\n\nWhile the main climbs arenâ€™t particularly difficult, except Kyburg, repeating them 7 times after more than 200km will be certainly challenging. Besides, we canâ€™t only judge a climb by the average gradient as, sometimes a climb may be composed of a rather flat part, followed by very steep parts, making it more challenging than a regular gradient. Thatâ€™s the case of the Buch am Irchel climb.\n\n\nBuch am Irchel, Berg Am Irchel, Switzerland\n\nâ€¢ Distance: 4.8 km, Elevation: 201 m, Avg. Grade: 4.3 %\n\nAs a side note, the current GPX file indicates a duration of 8 hours, which gives roughly 35km/h average speed, substantially lower than what the pro will do during the race.\n\nCodeduration &lt;- max(zch_gpx$routes[[1]]$Time) - min(zch_gpx$routes[[1]]$Time)\navg_speed &lt;- 275 / as.numeric(duration)\n\n\nWhat about the elevation profile?\nThe previous map does not say much about the elevation profile. The cumulated positive elevation is obtained by summing the elevation difference between 2 consecutive time points, only taking positive results:\n\nCodegain &lt;- 0\ni &lt;- 1\nn_iter &lt;- nrow(zch_gpx$routes[[1]]) - 1\nwhile (i &lt;= n_iter) {\n    current_elevation &lt;- zch_gpx$routes[[1]][i, \"Elevation\"]\n    new_elevation &lt;- zch_gpx$routes[[1]][i + 1, \"Elevation\"]\n    diff &lt;- new_elevation - current_elevation\n    if  (diff &gt; 0) gain &lt;- gain + diff\n    i &lt;- i + 1\n}\n\n\nNote that the website gain is officially 4470m whereas ours is 4492m. This difference might be explained by the usage of different smoothing algorithms for the elevation. Funnily, we all had different bike computers and none of us had the same elevation result at the end of the ride.\nWe split the trace into 2 parts. The first loop takes place around Winterthur, north of Zurich. Then, a transition leads to the city loop, which is repeated 7 times.\n\nCoderace_route &lt;- zch_gpx$routes[[1]] |&gt;\n    filter(Time &lt;= \"2023-11-03 09:13:52\")\ncity_circuit &lt;- zch_gpx$routes[[1]] |&gt;\n    filter(Time &gt; \"2023-11-03 09:13:52\")\nggplot() +\n    geom_area(data = race_route, aes(x = Time, y = Elevation), fill = \"darkblue\") +\n    geom_area(data = city_circuit, aes(x = Time, y = Elevation), fill = \"darkred\") +\n    labs(\n        title = \"Zurich UCI 2024 Elevation profile\",\n        subtitle = \"men elite race\",\n        caption = sprintf(\"Cumulated elevation: + %sm\", round(gain))\n    ) +\n    ylab(\"Elevation (m)\") +\n    theme(\n        axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()\n    )\n\n\n\n\n\n\n\nAs you can see, altough the race never goes above 700m, we manage to reach 4500m elevation gain.\nA 3D elevation profile with rayshader\nWe could cumulate both information about elevation and x and y coordinates to get the 3D profile with rayshader. The make_3d_plot function first creates a ggplot object using coordinates and color_col as color aesthetic. We set color_col to Elevation and hide the x and y axis information (as they wonâ€™t be very useful). Could you guess where (47.5, 8.4) is? Probably not :). This plot object is passed to plot_gg, to proceed to the 3D conversion.\n\nCodemake_3d_plot &lt;- function(dat, color_col, legend_title, scale = 150, show_legend = TRUE) {\n    tmp_3d_plot &lt;- ggplot(dat) + \n geom_point(aes(x = Longitude, y = Latitude, color = .data[[color_col]])) +\n scale_color_continuous(type = \"viridis\", limits = c(0, max(dat[[color_col]])), name = legend_title) +\n theme(\n     axis.title.x = element_blank(),\n     axis.text.x = element_blank(),\n     axis.ticks.x = element_blank(),\n     axis.title.y = element_blank(),\n     axis.text.y = element_blank(),\n     axis.ticks.y = element_blank(),\n     legend.position = if (!show_legend) \"none\"\n )\n plot_gg(tmp_3d_plot, width = 3.5, multicore = TRUE, windowsize = c(1600, 1000), sunangle = 225, zoom = 0.40, phi = 15, theta = 80, scale = scale)\nrender_snapshot()\n}\n\nmake_3d_plot(zch_gpx$routes[[1]], \"Elevation\", \"Elevation (m)\", show_legend = FALSE)\n\n\n\n\n\n\n\nWouldnâ€™t it be nice to be able to move the plot around and have different angles? We can do this with the help of ggrgl, particularly the geom_path_3d(). We extract some Zurichâ€™s canton cities and project them on the map with geom_point() and geom_text() to add annotations. Since this plot does not render with quarto, we included an image after the code.\n\nCode# https://simplemaps.com/data/ch-cities\nch_cities &lt;- readr::read_csv(\"ch.csv\") \nzh_cities &lt;- ch_cities |&gt; filter(\n    city %in% c(\"ZÃ¼rich\", \"Winterthur\", \"Binz\", \"Uster\", \"DÃ¼bendorf\", \"KÃ¼snacht\")\n)\np &lt;- ggplot(zch_gpx$routes[[1]]) +\n  geom_path_3d(\n    aes(Longitude, Latitude, z = Elevation),\n        extrude = TRUE, extrude_edge_colour = 'grey20', extrude_face_fill = 'grey80',\n    extrude_edge_alpha = 0.2) +\n  geom_text(data = zh_cities, aes(lng, lat, label = city)) + \n  geom_point(data = zh_cities, aes(lng, lat), colour = 'red') + \n  theme_ggrgl() +  \n  labs(\n    title = \"Elevation 3D profile\",\n    subtitle = \"World UCI road men elite 2024, Zurich\"\n  )\ndevoutrgl::rgldev(fov = 30, view_angle = -30)\np\n\n\n\nOn the left, you can notice the steepest climb (Kyburg), that connects the two loops. I highly recommend you to play around locally so you can try out different angles and explore the different parts.\nOverall, 4490m for 275km is definitely not the most hilly ride for professional athletes, compared to the amateur Alpen Brevet Platinium, which offers 275km for 8907m elevation, just a tiny bit higher than Mount Everest. Here again, it all depends on the average speed at which this race will go. I personally expect a value between 40-42km/h, depending on the weather conditions (rain, wind, â€¦). Letâ€™s see â€¦"
  },
  {
    "objectID": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html#the-ride",
    "href": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html#the-ride",
    "title": "2024 road and para-cycling road world championships: preliminaRy analysis",
    "section": "The ride",
    "text": "The ride\nFIT TO CSV\nIn the below section, we analyse few logs of my ride, which are extracted from the my bike GPS fit file. We first convert this file to a format that R can read, for instance csv. I used this website, but you can also find cli alternatives like here.\n\nCode# I found this R package but could not make it work\n# Given that it is 5 years old.\n#remotes::install_github(\"muschellij2/fit2csv\")\nres &lt;- readr::read_csv(\"2024-09-08-063850.csv\")\nhead(res)\n\n# A tibble: 6 Ã— 123\n  GOTOES_CSV timestamp           position_lat position_long altitude heart_rate\n  &lt;lgl&gt;      &lt;dttm&gt;                     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;     \n1 NA         2024-09-08 00:38:50         47.4          8.55     438. NA        \n2 NA         2024-09-08 00:38:51         47.4          8.55     438. NA        \n3 NA         2024-09-08 00:38:52         47.4          8.55     438. NA        \n4 NA         2024-09-08 00:38:53         47.4          8.55     438. NA        \n5 NA         2024-09-08 00:38:54         47.4          8.55     438. NA        \n6 NA         2024-09-08 00:38:55         47.4          8.55     438. NA        \n# â„¹ 117 more variables: cadence &lt;dbl&gt;, distance &lt;dbl&gt;, speed &lt;dbl&gt;,\n#   power &lt;dbl&gt;, compressed_speed_distance &lt;lgl&gt;, grade &lt;dbl&gt;,\n#   resistance &lt;lgl&gt;, time_from_course &lt;lgl&gt;, cycle_length &lt;lgl&gt;,\n#   temperature &lt;dbl&gt;, ...17 &lt;lgl&gt;, ...18 &lt;lgl&gt;, ...19 &lt;lgl&gt;, speed_1s &lt;lgl&gt;,\n#   cycles &lt;lgl&gt;, total_cycles &lt;lgl&gt;, ...23 &lt;lgl&gt;, ...24 &lt;lgl&gt;, ...25 &lt;lgl&gt;,\n#   ...26 &lt;lgl&gt;, ...27 &lt;lgl&gt;, ...28 &lt;lgl&gt;, ...29 &lt;lgl&gt;, ...30 &lt;lgl&gt;,\n#   compressed_accumulated_power &lt;lgl&gt;, accumulated_power &lt;lgl&gt;, â€¦\n\n\nWe select only few interesting columns for the analysis and also remove the 43 minutes coffee break we took in the middle of the ride in Kyburgâ€™s castle:\n\nCoderes &lt;- res |&gt;\n    tibble::rowid_to_column() |&gt;\n    mutate(\n        Latitude = position_lat,\n        Longitude = position_long,\n        distance = distance / 1000,\n        timestamp = case_when(rowid &gt;= 9017 ~ timestamp - 43 * 60, .default = timestamp)\n    ) |&gt;\n    select(timestamp, cadence, distance, speed, grade, power, temperature, calories, altitude, Latitude, Longitude)\n\n\nData summary\nBelow are some continuous variable summary using gtsummary. Notice the maximum gradient which was 18.2%! The overall ride has a 1.1% grade, which means there is more climbings than downhills.\n\nCoderes |&gt; \n  tbl_summary(include = c(speed, cadence, grade, power), type = all_continuous() ~ \"continuous2\",\n    statistic = all_continuous() ~ c(\"{mean}\", \"{min}\", \"{max}\"), \n    missing = \"no\",\n    label = c(speed ~ \"Speed (km/h)\", cadence ~ \"Cadence (RPM)\", grade ~ \"Grade (%)\", power ~ \"Power (Watts)\")\n  ) |&gt;\n    modify_header(label ~ \"**Variable**\") |&gt;\n    modify_caption(\"**Table 1. Ride summary**\") |&gt;\n    bold_labels()\n\n\n\n\nTable 1. Ride summary\n\n\n\n\n\nVariable\nN = 17,356\n\n\n\nSpeed (km/h)\n\n\n\nÂ Â Â Â Mean\n27\n\n\nÂ Â Â Â Minimum\n0\n\n\nÂ Â Â Â Maximum\n68\n\n\nCadence (RPM)\n\n\n\nÂ Â Â Â Mean\n61\n\n\nÂ Â Â Â Minimum\n0\n\n\nÂ Â Â Â Maximum\n110\n\n\nGrade (%)\n\n\n\nÂ Â Â Â Mean\n1.1\n\n\nÂ Â Â Â Minimum\n-14.4\n\n\nÂ Â Â Â Maximum\n18.2\n\n\nPower (Watts)\n\n\n\nÂ Â Â Â Mean\n151\n\n\nÂ Â Â Â Minimum\n0\n\n\nÂ Â Â Â Maximum\n701\n\n\n\n\n\n\nPower analysis\nBackground\nPower measures how much work is done at a given time (in our case, on the bike). It is expressed in Watts (W. 1W = 1J/s). Power is expressed as follows:\nP = Strength x velocity\nThere are 2 ways to rise the power. At low velocity by putting more strength or increase the velocity while applying the same strength.\nIn cycling, we also calculate the Power/Body Weight ratio, as from physiological point of view, the more muscles, the more theoritical power. This is important in the climbs, where, because of the gravity, the weight becomes more important as the gradient increases. Therefore, taking cyclist 1 (bodyweight + bike 60kg) and cyclist 2 (bodyweight + bike 90kg) side by side on the same climb with similar bikes, cyclist 2 has to produce more power to climb at the same speed as cyclist 1.\nTherefore, a 58kg pro cyclist climber and 100kg pro track cyclist may have similar power ratio for a given duration, even though the former will likely be better at longer efforts. Talking about power without considering the effort duration does not make much sense. World class women cyclists can sustain &gt; 19W/kg during 5s (1360 for a 70kg athlete), men cyclists can sustain 24 W/kg during 5 seconds (2160W output for 90kg).\nWe wonâ€™t have time to cover all the theory, but keep in mind that knowing your threshold power (FTP) is critical for successful training. This is the power you can theoretically sustain for 1h. Based on this, one can establish power zones to plan the training. For profesional riders, FTP are respectively &gt; 5W/kg for women and &gt; 5.8 W/kg for men. You can find more here.\nResults\nTo proceed, we create a plot showing the power as a function of the distance. We also add the elevation profile in the background with geom_area() with a rather transparent alpha setting, so the user can focus on the power data. We add some geom_smooth() to see the relation between the power and distance ()power ~ distance) and display the mean power on an horizontal line with geom_hline(). On the second plot, we want to show the power distribution and leverage geom_histogram, the idea being to compare the mean power to the threshold power.\nThe power chart shows that my power is decreasing over time, not a surprise. There is an effect of the fatigue but also the weather conditions, as the last part of the ride was in the city and under heavy rain and we had to adjust the speed not to take too much risks. Besides, when looking at the power distribution, we notice that the average power is significantly below my threshold power (FTP), which is my theoretical maximum power for 1h. For a 5h ride, this makes sense as one wants to save energy to last as long as possible.\n\nCodemake_time_plot &lt;- function(dat, col, show_elevation = TRUE, elevation_scale = 1) {\n    p &lt;- ggplot(dat, aes(x = distance, y = .data[[col]])) +\n    geom_line() +\n    geom_smooth(method = \"lm\") +\n    geom_hline(yintercept = mean(dat[[col]]), linetype = \"dashed\", color = \"darkred\")\n\n    if (show_elevation) p + geom_area(aes(x = distance, y = altitude / elevation_scale), alpha = 0.15) else p\n}\n\nmake_distrib_plot &lt;- function(dat, col) {\n    ggplot(dat, aes(x = .data[[col]])) +\n    geom_histogram() +\n    geom_vline(xintercept = 250, linetype = \"dashed\", color = \"darkgreen\") +\n    geom_vline(xintercept = mean(dat[[col]]), linetype = \"dashed\", color = \"darkred\")\n}\n\npower_time &lt;- make_time_plot(res, \"power\") +\n    annotate(\n        \"text\",\n        x = 10,\n        y = 400,\n        label = \"Average power\",\n        fontface = \"bold\",\n        color = \"darkred\",\n        size = 4.5\n    ) +\n    ggtitle(\"Power over time\") +\n    xlab(\"Distance (km)\") +\n    ylab(\"Power (Watts)\")\n\npower_distrib &lt;- make_distrib_plot(res, \"power\") +\n    annotate(\n        \"text\",\n        x = 310,\n        y = 2500,\n        label = \"Threshold power (FTP)\",\n        fontface = \"bold\",\n        color = \"darkgreen\",\n        size = 2.5\n    ) +\n    annotate(\n        \"text\",\n        x = mean(res$power) + 20 - 60,\n        y = 2500,\n        label = \"Average power\",\n        fontface = \"bold\",\n        color = \"darkred\",\n        size = 2.5\n    ) +\n    theme(\n     axis.title.y = element_blank(),\n     axis.text.y = element_blank(),\n     axis.ticks.y = element_blank()\n    ) +\n    ggtitle(\"Power distribution\") +\n    xlab(\"Power (Watts)\")\n\npower_time / power_distrib + plot_annotation(\n  title = \"Power data\",\n  subtitle = \"Elevation data shown in the background\",\n  caption = \"More about power: https://support.strava.com/hc/en-us/articles/216918457-Power\"\n)\n\n\n\n\n\n\n\nSpeed\n\nCodemake_time_plot(res, \"speed\", elevation_scale = 10) +\n    ylab(\"Speed (km/h)\") +\n    xlab(\"Distance (km)\")\n\n\n\n\n\n\n\nThe ride was covered at 27km/h average speed on an open road with wind and rain, definitely not the best conditions.\nInterestingly, I found this nice article about the relation between power and speed. Overall, the simulator predicts 150W to maintain an average speed of 27km/h with a 0.5% gradient coefficient, not far from what we have here. Itâ€™s rather challenging to account for the wind, as it can sometimes help or makes things more challenging.\nCalories\nDuring that ride, I consumed about 2549 calories, which corresponds to the average daily energy needs for an adult man."
  },
  {
    "objectID": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html#conclusion",
    "href": "posts/2024-09-09-zurich-roadcycling-wc-2024/index.html#conclusion",
    "title": "2024 road and para-cycling road world championships: preliminaRy analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a lot of fun to ride part of this upcoming event, even more to analyse the underlying data."
  },
  {
    "objectID": "posts/2019-07-17-dm/index.html",
    "href": "posts/2019-07-17-dm/index.html",
    "title": "Introducing dm: easy juggling of tables and relations",
    "section": "",
    "text": "The dm package reduces your mental strain when working with many tables. It connects your data, while still keeping them in their original tables. You can easily perform operations on all tables, visualize how theyâ€™re connected, and integrate databases.\n\n\n\nPhoto by Tobias Fischer\n\n\n\nRather than writing out the relational structure in code while connecting the sources, dm packages them in data. This workflow prevents you from inconsistently updating tables, making copies due to redundancies and, more generally, losing track of decisions made in a data pipeline.\n\nA case study\nTo demonstrate dmâ€™s benefits, letâ€™s compare some workflows using data from the nycflights13 package.\nWe want to get the data to analyze arrival delays of flights of planes built more than 30 years ago. For these flights, we also want to check if they departed for destinations in the south. We use methods from the dplyr package, the grammar for data manipulation from the tidyverse.\nThis requires columns lon (longitude) from table airports, year from table planes and arr_delay (arrival delay) from flights. We can do this by performing multiple joins, one after another, as you might be used to:\nlibrary(nycflights13)\nlibrary(dplyr)\nplanes_join &lt;- planes %&gt;%\n  rename(production_year_plane = year) %&gt;%\n  left_join(flights, by = \"tailnum\") %&gt;%\n  left_join(airports, by = c(\"dest\" = \"faa\")) %&gt;%\n  select(lon, year, production_year_plane, dest)\n\nplanes_join %&gt;%\n  filter(production_year_plane &lt; 1983)\n## # A tibble: 1,432 x 4\n##      lon  year production_year_plane dest\n##    &lt;dbl&gt; &lt;int&gt;                 &lt;int&gt; &lt;chr&gt;\n##  1 -95.3  2013                  1965 IAH\n##  2 -95.3  2013                  1965 IAH\n##  3 -95.3  2013                  1965 IAH\n##  4 -95.3  2013                  1965 IAH\n##  5 -87.9  2013                  1959 ORD\n##  6 -97.0  2013                  1959 DFW\n##  7 -97.0  2013                  1959 DFW\n##  8 -97.0  2013                  1959 DFW\n##  9 -90.4  2013                  1959 STL\n## 10 -97.0  2013                  1959 DFW\n## # â€¦ with 1,422 more rows\nFair enough, you might say â€” to keep an overview, we can reduce the 34 columns using select(), then filter(). But what do you do once there are 30 tables, possibly from databases youâ€™re working with? Is this still a workable solution? And how sure are we that we truly only need these three columns?\n\n\ndm\nOn your dm object, you can directly apply operations like filter() to all your tables! How so? Because relations between the objects are already registered in the dm object, you donâ€™t need to indicate them.\nNot having to state how tables are connected each time you perform a join may make exploratory work a lot easier:\nlibrary(dm)\ncdm_nycflights13() %&gt;%\n  cdm_filter(planes, year &lt; 1983)\n## â”€â”€ Table source â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## src:  &lt;package: nycflights13&gt;\n## â”€â”€ Data model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Data model object:\n##   5 tables:  airlines, airports, flights, planes ...\n##   53 columns\n##   3 primary keys\n##   3 references\n## â”€â”€ Rows â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n## Total: 27583\n## airlines: 4, airports: 3, flights: 1432, planes: 29, weather: 26115\nThis produces an overview of your data sources â€” all tables still separated. Also note: there was no need to rename() the column year, and all tables stay separate.\nWhat if you still wanted to produce a join and check where those flights departed to? VoilÃ :\ncdm_nycflights13() %&gt;%\n  cdm_filter(planes, year &lt; 1983) %&gt;%\n  cdm_join_tbl(flights, airports)\n## # A tibble: 1,432 x 19\n##     year month   day dep_time sched_dep_time dep_delay arr_time\n##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n##  1  2013     1     1      839            850       -11     1027\n##  2  2013     1     1      858            900        -2     1102\n##  3  2013     1     1     1219           1220        -1     1415\n##  4  2013     1     1     1317           1325        -8     1454\n##  5  2013     1     1     1422           1410        12     1613\n##  6  2013     1     1     1806           1810        -4     2002\n##  7  2013     1     1     1911           1910         1     2050\n##  8  2013     1     1     2030           2045       -15     2150\n##  9  2013     1     2      535            540        -5      831\n## 10  2013     1     2      945            945         0     1113\n## # â€¦ with 1,422 more rows, and 12 more variables: sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,\n## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n## #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nVisualization\nYou can easily gain a visual overview using cdm_draw(). This results in pretty, customizable (using cdm_set_colors()) visualizations of how your data connect.\ncdm_nycflights13() %&gt;%\n  cdm_set_colors(\n    flights = \"blue_nb\",\n    airlines = \"orange_nb\",\n    weather = \"green_nb\",\n    airports = \"yellow_nb\",\n    planes = \"light_grey_nb\"\n  ) %&gt;%\n  cdm_draw()\n\n\n\nDatabases\ndm works for both in-memory data and databases. You can easily integrate it into your dbplyr workflow, making for new, powerful ways of analysis.\n\n\nFeedback\nThis was just a short glimpse of what dm can do â€” more posts will follow. Feel free to contribute to this package on Github. Funding for dm is generously provided by Energie360 and cynkra. This package is authored by Tobias Schieferdecker and Kirill MÃ¼ller."
  },
  {
    "objectID": "posts/2024-05-13-shinyMobile-2.0.0/index.html",
    "href": "posts/2024-05-13-shinyMobile-2.0.0/index.html",
    "title": "shinyMobile 2.0.0: a preview",
    "section": "",
    "text": "shinyMobile has been enabling the creation of exceptional R Shiny apps for both iOS and Android for nearly five years, thanks to the impressive open-source Framework7 template that drives its capabilities.\nThis year shinyMobile gets a major update to v2.2.0. Iâ€™d like to warmly thank Veerle van Leemput and Michael S. Czahor from AthlyticZ for providing significant support during this marathon."
  },
  {
    "objectID": "posts/2024-05-13-shinyMobile-2.0.0/index.html#major-changes",
    "href": "posts/2024-05-13-shinyMobile-2.0.0/index.html#major-changes",
    "title": "shinyMobile 2.0.0: a preview",
    "section": "Major changes",
    "text": "Major changes\nNew multi pages experimental support\nWe are very excited to bring this feature out for this new release. Under the hood, this is possible owing to the {brochure} package from Colin Fay as well as the internal Framework7 router component.\nWhat does this mean?\nYou can now develop real multi pages Shiny applications and have different url endpoints and redirections. For instance, https://my-app/home can be the home page while https://my-app/settings brings to the settings page.\nHow does this work?\nAt the time of writting of this blog post, you must install a patched {brochure} version with devtools::install_github(\"DivadNojnarg/brochure\").\nIn the below code, we basically have 3 pages having their own content and a common layout for consistency. The router ensure beautiful transitions from one page to another. We invite you to look at the getting started article which provides more technical details.\n\nCodelibrary(shiny)\n# Needs a specific version of brochure for now.\n# This allows to pass wrapper functions with options\n# as list. We need it because of the f7Page options parameter\n# and to pass the routes list object for JS.\n# devtools::install_github(\"DivadNojnarg/brochure\")\nlibrary(brochure)\nlibrary(shinyMobile)\n\n# Allows to use the app on a server like \n# shinyapps.io where basepath is /app_name\n# instead of \"/\" or \"\".\nmake_link &lt;- function(path = NULL, basepath = \"\") {\n  if (is.null(path)) {\n    if (nchar(basepath) &gt; 0) {\n      return(basepath)\n    } else {\n      return(\"/\")\n    }\n  }\n  sprintf(\"%s/%s\", basepath, path)\n}\n\nlinks &lt;- lapply(2:3, function(i) {\n  tags$li(\n    f7Link(\n      routable = TRUE,\n      label = sprintf(\"Link to page %s\", i),\n      href = make_link(i)\n    )\n  )\n})\n\npage_1 &lt;- function() {\n  page(\n    href = \"/\",\n    ui = function(request) {\n      shiny::tags$div(\n        class = \"page\",\n        # top navbar goes here\n        f7Navbar(title = \"Home page\"),\n        # NOTE: when the main toolbar is enabled in\n        # f7MultiLayout, we can't use individual page toolbars.\n        # f7Toolbar(\n        #  position = \"bottom\",\n        #  tags$a(\n        #    href = \"/2\",\n        #    \"Second page\",\n        #    class = \"link\"\n        #  )\n        # ),\n        # Page content\n        tags$div(\n          class = \"page-content\",\n          f7List(\n            inset = TRUE,\n            strong = TRUE,\n            outline = TRUE,\n            dividers = TRUE,\n            mode = \"links\",\n            links\n          ),\n          f7Block(\n            f7Text(\"text\", \"Text input\", \"default\"),\n            f7Select(\"select\", \"Select\", colnames(mtcars)),\n            textOutput(\"res\"),\n            textOutput(\"res2\")\n          )\n        )\n      )\n    }\n  )\n}\n\npage_2 &lt;- function() {\n  page(\n    href = \"/2\",\n    ui = function(request) {\n      shiny::tags$div(\n        class = \"page\",\n        # top navbar goes here\n        f7Navbar(\n          title = \"Second page\",\n          # Allows to go back to main\n          leftPanel = tagList(\n            tags$a(\n              href = make_link(),\n              class = \"link back\",\n              tags$i(class = \"icon icon-back\"),\n              tags$span(\n                class = \"if-not-md\",\n                \"Back\"\n              )\n            )\n          )\n        ),\n        shiny::tags$div(\n          class = \"page-content\",\n          f7Block(f7Button(inputId = \"update\", label = \"Update stepper\")),\n          f7List(\n            strong = TRUE,\n            inset = TRUE,\n            outline = FALSE,\n            f7Stepper(\n              inputId = \"stepper\",\n              label = \"My stepper\",\n              min = 0,\n              max = 10,\n              size = \"small\",\n              value = 4,\n              wraps = TRUE,\n              autorepeat = TRUE,\n              rounded = FALSE,\n              raised = FALSE,\n              manual = FALSE\n            )\n          ),\n          f7Block(textOutput(\"test\"))\n        )\n      )\n    }\n  )\n}\n\npage_3 &lt;- function() {\n  page(\n    href = \"/3\",\n    ui = function(request) {\n      shiny::tags$div(\n        class = \"page\",\n        # top navbar goes here\n        f7Navbar(\n          title = \"Third page\",\n          # Allows to go back to main\n          leftPanel = tagList(\n            tags$a(\n              href = make_link(),\n              class = \"link back\",\n              tags$i(class = \"icon icon-back\"),\n              tags$span(\n                class = \"if-not-md\",\n                \"Back\"\n              )\n            )\n          )\n        ),\n        shiny::tags$div(\n          class = \"page-content\",\n          f7Block(\"Nothing to show yet ...\")\n        )\n      )\n    }\n  )\n}\n\nbrochureApp(\n  basepath = make_link(),\n  # Pages\n  page_1(),\n  page_2(),\n  page_3(),\n  # Important: in theory brochure makes\n  # each page having its own shiny session/ server function.\n  # That's not what we want here so we'll have\n  # a global server function.\n  server = function(input, output, session) {\n    output$res &lt;- renderText(input$text)\n    output$res2 &lt;- renderText(input$select)\n    output$test &lt;- renderText(input$stepper)\n\n    observeEvent(input$update, {\n      updateF7Stepper(\n        inputId = \"stepper\",\n        value = 0.1,\n        step = 0.01,\n        size = \"large\",\n        min = 0,\n        max = 1,\n        wraps = FALSE,\n        autorepeat = FALSE,\n        rounded = TRUE,\n        raised = TRUE,\n        color = \"pink\",\n        manual = TRUE,\n        decimalPoint = 2\n      )\n    })\n  },\n  wrapped = f7MultiLayout,\n  wrapped_options = list(\n    basepath = make_link(),\n    # Common toolbar\n    toolbar = f7Toolbar(\n      f7Link(icon = f7Icon(\"house\"), href = make_link(), routable = TRUE)\n    ),\n    options = list(\n      dark = TRUE,\n      theme = \"md\",\n      routes = list(\n        # Important: don't remove keepAlive\n        # for pages as this allows\n        # to save the input state when switching\n        # between pages. If FALSE, each time a page is\n        # changed, inputs are reset.\n        list(path = make_link(), url = make_link(), name = \"home\", keepAlive = TRUE),\n        list(path = make_link(\"2\"), url = make_link(\"2\"), name = \"2\", keepAlive = TRUE),\n        list(path = make_link(\"3\"), url = make_link(\"3\"), name = \"3\", keepAlive = TRUE)\n      )\n    )\n  )\n)\n\n\nUpdated material design style\nBy updating to the latest Framework7 version, we now benefit from a totally revamped Android (md) design, which looks more modern.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(shinyMobile)\n\n# source modules\ne &lt;- environment()\npath &lt;- system.file(\"examples/gallery/tabs/\", package = \"shinyMobile\")\nsapply(\n  list.files(\n    path,\n    include.dirs = FALSE,\n    pattern = \".R\",\n    ignore.case = TRUE\n  ),\n  function(f) {\n    source(file.path(path, f), local = e)\n  }\n)\n\napp_options &lt;- list(\n  theme = \"md\",\n  dark = TRUE,\n  filled = FALSE,\n  preloader = TRUE,\n  color = \"#007aff\",\n  navbar = list(\n    hideOnPageScroll = TRUE,\n    mdCenterTitle = TRUE\n  )\n)\n\n# shiny app\nshinyApp(\n  ui = f7Page(\n    allowPWA = TRUE,\n    options = app_options,\n    f7TabLayout(\n      title = \"shinyMobile Gallery\",\n      messagebar = f7MessageBar(inputId = \"mymessagebar\",\n                                placeholder = \"Message\"),\n      panels = tagList(\n        f7Panel(\n          id = \"panelLeft\",\n          title = \"Left Panel\",\n          side = \"left\",\n          f7Block(\"A panel with push effect\"),\n          f7PanelMenu(\n            inset = TRUE,\n            outline = TRUE,\n            # Use items as tab navigation only\n            f7PanelItem(\n              tabName = \"tabset-Inputs\",\n              title = \"Input tabs\",\n              icon = f7Icon(\"largecircle_fill_circle\"),\n              active = TRUE\n            ),\n            f7PanelItem(\n              tabName = \"tabset-FABs\",\n              title = \"Buttons tabs\",\n              icon = f7Icon(\"largecircle_fill_circle\")\n            )\n          ),\n          effect = \"push\",\n          options = list(swipe = TRUE)\n        ),\n        f7Panel(\n          title = \"Right Panel\",\n          side = \"right\",\n          f7Radio(\n            inputId = \"theme\",\n            label = \"Theme\",\n            choices = c(\"md\", \"ios\"),\n            selected = app_options$theme\n          ),\n          f7Radio(\n            inputId = \"dark\",\n            label = \"Mode\",\n            choices = c(\"dark\", \"light\"),\n            selected = ifelse(app_options$dark, \"dark\", \"light\")\n          ),\n          f7Radio(\n            inputId = \"color\",\n            label = \"Color\",\n            choices = getF7Colors(),\n            selected = \"primary\"\n          ),\n          effect = \"floating\",\n          options = list(swipe = TRUE)\n        )\n      ),\n      navbar = f7Navbar(\n        title = \"shinyMobile Gallery\",\n        hairline = TRUE,\n        leftPanel = TRUE,\n        rightPanel = TRUE\n      ),\n      f7Login(\n        id = \"loginPage\",\n        title = \"You really think you can go here?\",\n        footer = \"This section simulates an authentication process. There\n        is actually no user and password database. Put whatever you want but\n        don't leave blank!\",\n        startOpen = FALSE\n      ),\n      # recover the color picker input and update the text background\n      # color accordingly.\n      tags$script(\n        \"$(function() {\n          Shiny.addCustomMessageHandler('text-color', function(message) {\n            $('#colorPickerVal').css('background-color', message);\n          });\n\n          // toggle message bar based on the currently selected tab\n          Shiny.addCustomMessageHandler('toggleMessagebar', function(message) {\n            if (message === 'chat') {\n              $('#mymessagebar').show();\n              $('.toolbar.tabLinks').hide();\n            } else {\n              $('#mymessagebar').hide();\n              $('.toolbar.tabLinks').show();\n            }\n          });\n        });\n        \"\n      ),\n      f7Tabs(\n        id = \"tabset\",\n        animated = FALSE,\n        swipeable = TRUE,\n        tabInputs,\n        tabBtns,\n        tabCards,\n        tabLists,\n        tabText,\n        tabInfo,\n        tabOthers\n      )\n    )\n  ),\n  server = function(input, output, session) {\n\n    # update theme\n    observeEvent(input$theme, ignoreInit = TRUE, {\n      updateF7App(\n        options = list(\n          theme = input$theme\n        )\n      )\n    })\n\n    # update mode\n    observeEvent(input$dark, ignoreInit = TRUE, {\n      updateF7App(\n        options = list(\n          dark = ifelse(input$dark == \"dark\", TRUE, FALSE)\n        )\n      )\n    })\n\n    # update color\n    observeEvent(input$color, ignoreInit = TRUE, {\n      updateF7App(\n        options = list(\n          color = input$color\n        )\n      )\n    })\n\n    # input validation\n    observe({\n      validateF7Input(inputId = \"text\", info = \"Whatever\")\n      validateF7Input(\n        inputId = \"password\",\n        pattern = \"[0-9]*\",\n        error = \"Only numbers please!\"\n      )\n    })\n\n    # toggle message bar: should only be dislayed when on the messages tab\n    observeEvent(input$tabset, {\n      session$sendCustomMessage(type = \"toggleMessagebar\", input$tabset)\n      session$sendCustomMessage(type = \"toggleSearchbar\", input$tabset)\n    })\n\n    # user send new message\n    observeEvent(input[[\"mymessagebar-send\"]], {\n      updateF7Messages(\n        id = \"mymessages\",\n        list(\n          f7Message(\n            text = input$mymessagebar,\n            name = \"David\",\n            type = \"sent\",\n            header = \"Message Header\",\n            footer = \"Message Footer\",\n            textHeader = \"Text Header\",\n            textFooter = \"text Footer\",\n            avatar = \"https://cdn.framework7.io/placeholder/people-100x100-7.jpg\"\n          )\n        )\n      )\n    })\n\n    # fake to receive random messages\n    observe({\n      invalidateLater(5000)\n      names &lt;- c(\"Victor\", \"John\")\n      name &lt;- sample(names, 1)\n\n      updateF7Messages(\n        id = \"mymessages\",\n        list(\n          f7Message(\n            text = \"Message\",\n            name = name,\n            type = \"received\",\n            avatar = \"https://cdn.framework7.io/placeholder/people-100x100-9.jpg\"\n          )\n        )\n      )\n    })\n\n    # trigger for login\n    trigger &lt;- reactive({\n      req(input$tabset == \"chat\")\n    })\n    # login server module\n    f7LoginServer(\n      id = \"loginPage\",\n      ignoreInit = TRUE,\n      trigger = trigger\n    )\n\n    output$sin &lt;- renderPlot(plot(sin, -pi, 2 * pi))\n    output$cos &lt;- renderPlot(plot(cos, -pi, 2 * pi))\n\n    output$text &lt;- renderPrint(input$text)\n    output$password &lt;- renderPrint(input$password)\n    output$textarea &lt;- renderPrint(input$textarea)\n    output$slider &lt;- renderPrint(input$sliderInput)\n    output$sliderRange &lt;- renderPrint(input$sliderRangeInput)\n    output$stepper &lt;- renderPrint(input$stepper)\n    output$check &lt;- renderPrint(input$check)\n    output$checkgroup &lt;- renderPrint(input$checkgroup)\n    output$checkgroup2 &lt;- renderPrint(input$checkgroup2)\n    output$radio &lt;- renderPrint(input$radio)\n    output$radio2 &lt;- renderPrint(input$radio2)\n    output$toggle &lt;- renderPrint(input$toggle)\n    output$select &lt;- renderPrint(input$select)\n    output$smartdata &lt;- renderTable(\n      {\n        head(mtcars[, c(\"mpg\", input$smartsel), drop = FALSE])\n      },\n      rownames = TRUE\n    )\n    output$dateval &lt;- renderPrint(input$mydatepicker)\n    output$autocompleteval &lt;- renderPrint(input$myautocomplete)\n\n    lapply(1:12, function(i) {\n      output[[paste0(\"res\", i)]] &lt;- renderText(paste0(\"Button\", i, \" is \", input[[paste0(\"btn\", i)]]))\n    })\n    output$pickerval &lt;- renderText(input$mypicker)\n    output$colorPickerVal &lt;- renderPrint(input$mycolorpicker$hex)\n\n    # send the color picker input to JS\n    observeEvent(input$mycolorpicker, {\n      session$sendCustomMessage(type = \"text-color\", message = input$mycolorpicker$hex)\n    })\n\n\n    # popup\n    output$popupContent &lt;- renderPrint(input$popupText)\n\n    observeEvent(input$togglePopup, {\n      f7Popup(\n        id = \"popup1\",\n        title = \"My first popup\",\n        f7Block(\n          p(\"Popup can push the view behind. By default it has effect only when\n            'safe-area-inset-top' is more than zero (iOS fullscreen webapp or iOS cordova app)\"),\n          f7Text(inputId = \"popupText\",\n                 label = \"Popup content\",\n                 value = \"This is my first popup, I swear!\"),\n          verbatimTextOutput(\"popupContent\")\n        )\n      )\n    })\n\n    observeEvent(input$popup1, {\n      if (input$tabset == \"Popups\") {\n        popupStatus &lt;- if (input$popup1) \"opened\" else \"closed\"\n        f7Toast(\n          position = \"top\",\n          text = paste(\"Popup is\", popupStatus)\n        )\n      }\n    })\n\n    # sheet plot\n    output$sheetPlot &lt;- renderPlot({\n      hist(rnorm(input$sheetObs))\n    })\n\n    observeEvent(input$toggleSheet, {\n      updateF7Sheet(id = \"sheet1\")\n    })\n\n    # notifications\n    lapply(1:3, function(i) {\n      observeEvent(input[[paste0(\"goNotif\", i)]], {\n        icon &lt;- if (i %% 2 == 0) f7Icon(\"bolt_fill\") else NULL\n\n        f7Notif(\n          text = \"test\",\n          icon = icon,\n          title = paste(\"Notification\", i),\n          subtitle = \"A subtitle\",\n          titleRightText = i\n        )\n      })\n    })\n\n    # Dialogs\n    # notifications\n    lapply(1:3, function(i) {\n      observeEvent(input[[paste0(\"goDialog\", i)]], {\n        if (i == 1) {\n          f7Dialog(\n            title = \"Dialog title\",\n            text = \"This is an alert dialog\"\n          )\n        } else if (i == 2) {\n          f7Dialog(\n            id = \"confirmDialog\",\n            title = \"Dialog title\",\n            type = \"confirm\",\n            text = \"This is an confirm dialog\"\n          )\n        } else if (i == 3) {\n          f7Dialog(\n            id = \"promptDialog\",\n            title = \"Dialog title\",\n            type = \"prompt\",\n            text = \"This is a prompt dialog\"\n          )\n        }\n      })\n    })\n\n    observeEvent(input$confirmDialog, {\n      f7Toast(text = paste(\"Alert input is:\", input$confirmDialog))\n    })\n\n    output$promptres &lt;- renderUI({\n      if (is.null(input$promptDialog)) {\n        f7BlockTitle(title = \"Click on dialog button 3\")\n      }\n      f7BlockTitle(title = input$promptDialog)\n    })\n\n    # popovers\n    observeEvent(input$popoverButton, {\n      addF7Popover(\n        id = \"popoverButton\",\n        options = list(content = \"This is a f7Button\")\n      )\n    })\n\n    # toasts\n    observeEvent(input$toast, {\n      f7Toast(\n        position = \"bottom\",\n        text = \"I am a toast. Eat me!\"\n      )\n    })\n\n    # action sheet\n    observeEvent(input$goActionSheet, {\n      f7ActionSheet(\n        grid = TRUE,\n        id = \"action1\",\n        buttons = list(\n          list(\n            text = \"Notification\",\n            icon = f7Icon(\"info\"),\n            color = NULL\n          ),\n          list(\n            text = \"Dialog\",\n            icon = f7Icon(\"lightbulb_fill\"),\n            color = NULL\n          )\n        )\n      )\n    })\n\n    observeEvent(input$action1_button, {\n      if (input$action1_button == 1) {\n        f7Notif(\n          text = \"You clicked on the first button\",\n          icon = f7Icon(\"bolt_fill\"),\n          title = \"Notification\",\n          titleRightText = \"now\"\n        )\n      } else if (input$action1_button == 2) {\n        f7Dialog(\n          id = \"actionSheetDialog\",\n          title = \"Click me to launch a Toast!\",\n          type = \"confirm\",\n          text = \"You clicked on the second button\"\n        )\n      }\n    })\n\n    observeEvent(input$swipeAction_button, {\n      if (input$swipeAction_button == 1) {\n        f7Notif(\n          text = \"You clicked on the first button\",\n          icon = f7Icon(\"bolt_fill\"),\n          title = \"Notification\",\n          titleRightText = \"now\"\n        )\n      } else if (input$swipeAction_button == 2) {\n        f7Dialog(\n          id = \"actionSheetDialog\",\n          title = \"Click me to launch a Toast!\",\n          type = \"confirm\",\n          text = \"You clicked on the second button\"\n        )\n      }\n    })\n\n    observeEvent(input$actionSheetDialog, {\n      f7Toast(text = paste(\"Alert input is:\", input$actionSheetDialog))\n    })\n\n    # update progress bar\n    observeEvent(input$updatepg1, {\n      updateF7Progress(id = \"pg1\", value = input$updatepg1)\n    })\n\n    # update gauge\n    observeEvent(input$updategauge1, {\n      updateF7Gauge(id = \"mygauge1\", value = input$updategauge1)\n    })\n\n    # expand card 3\n    observeEvent(input$goCard, {\n      updateF7Card(id = \"card3\")\n    })\n\n    # toggle accordion\n    observeEvent(input$goAccordion, {\n      updateF7Accordion(\n        id = \"accordion1\",\n        selected = 1\n      )\n    })\n\n    # update panel\n    observeEvent(input$goPanel, {\n      updateF7Panel(id = \"panelLeft\")\n    })\n\n    # swipeout\n    observeEvent(input$swipeNotif, {\n      f7Notif(\n        text = \"test\",\n        icon = f7Icon(\"bolt_fill\"),\n        title = \"Notification\",\n        subtitle = \"A subtitle\",\n        titleRightText = \"now\"\n      )\n    })\n\n    observeEvent(input$swipeAlert, {\n      f7Dialog(\n        title = \"Dialog title\",\n        text = \"This is an alert dialog\"\n      )\n    })\n\n    observeEvent(input$swipeActionSheet, {\n      f7ActionSheet(\n        grid = TRUE,\n        id = \"swipeAction\",\n        buttons = list(\n          list(\n            text = \"Notification\",\n            icon = f7Icon(\"info\"),\n            color = NULL\n          ),\n          list(\n            text = \"Dialog\",\n            icon = f7Icon(\"lightbulb_fill\"),\n            color = NULL\n          )\n        )\n      )\n    })\n\n    # preloaders\n    observeEvent(input$showLoader, {\n      showF7Preloader(target = \"#preloaderPlot\", color = \"blue\")\n      Sys.sleep(2)\n      hideF7Preloader(target = \"#preloaderPlot\")\n    })\n    output$preloaderPlot &lt;- renderPlot({\n      hist(rnorm(100))\n    })\n\n    # photo browser\n    observeEvent(input$togglePhoto, {\n      f7PhotoBrowser(\n        theme = \"dark\",\n        type = \"standalone\",\n        photos = c(\n          \"https://cdn.framework7.io/placeholder/sports-1024x1024-1.jpg\",\n          \"https://cdn.framework7.io/placeholder/sports-1024x1024-2.jpg\",\n          \"https://cdn.framework7.io/placeholder/sports-1024x1024-3.jpg\"\n        )\n      )\n    })\n\n    # Menus\n    observeEvent(input$toggleMenu, {\n      updateF7MenuDropdown(\"menu1\")\n    })\n\n    observeEvent(input$menuItem1, {\n      f7Notif(text = \"Well done!\")\n    })\n\n    # skeleton\n    observe({\n      invalidateLater(4000)\n      f7Skeleton(\".skeleton-list\", \"fade\", 2)\n    })\n\n    # Treeview\n    output$treeview &lt;- renderText({\n      input$treeview\n    })\n\n    # Table\n    output$table &lt;- renderUI({\n      f7Table(mtcars[1:15,])\n    })\n\n    # pull to refresh\n    # observeEvent(input$ptr, {\n    #\n    #   ptrStatus &lt;- if (input$ptr) \"on\"\n    #\n    #   f7Dialog(\n    #     text = paste('ptr is', ptrStatus),\n    #     type = \"alert\"\n    #   )\n    # })\n  }\n)\n\n\n\n\nRefined inputs layout and style\nWhenever you have multiple inputs, we now recommend to wrap all of them within f7List() so as to benefit from new styling options such as outline, inset, strong. Internally, we use a function able to detect whether the input is inside a f7List(). If this is the case, you can style this list by passing parameters like f7List(outline = TRUE, inset = TRUE, ...). If not, the input is internally wrapped in a list to have correct rendering, but no styling is possible. Besides, some inputs like f7Text() can have custom styling (add an icon, clear button, outline style), which is independent from the external list wrapper style. Hence, we donâ€™t recommend doing f7List(outline = TRUE, f7Text(outline = TRUE)) since it wonâ€™t render well and instead use f7List(outline = TRUE, f7Text()).\nBesides, independently from f7List(), some inputs having more specific styling options:\n\n\nf7AutoComplete().\n\nf7Text(), f7Password(), f7TextArea().\n\nf7Select().\n\nf7Picker(), f7ColorPicker() and f7DatePicker().\n\nf7Radio() and f7CheckboxGroup().\n\nIn practices, you can design a supercharged f7Text() like so:\n\nCodef7Text(\n    inputId = \"text\",\n    label = \"Text input\",\n    value = \"Some text\",\n    placeholder = \"Your text here\",\n    style = list(\n      description = \"A cool text input\",\n      outline = TRUE,\n      media = f7Icon(\"house\"),\n      clearable = TRUE,\n      floating = TRUE\n    )\n)\n\n\nThis adds a description to the input (below its main content), as well as the outline style option and an icon on the left side. clearable is TRUE by default meaning that all text-based inputs can be cleared. floating is an effect that makes the label move in and out the input area depending on the content state. When empty, the label is inside and when there is text, the label is pushed outside into its usual location.\nf7Stepper() and f7Toggle() label is now displayed on the left.\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    options = list(dark = FALSE, theme = \"ios\"),\n    title = \"Inputs Layout\",\n    f7SingleLayout(\n      navbar = f7Navbar(\n        title = \"Inputs Layout\",\n        hairline = FALSE\n      ),\n      f7List(\n        inset = TRUE,\n        dividers = TRUE,\n        strong = TRUE,\n        outline = FALSE,\n        f7Text(\n          inputId = \"text\",\n          label = \"Text input\",\n          value = \"Some text\",\n          placeholder = \"Your text here\"\n        ),\n        f7TextArea(\n          inputId = \"textarea\",\n          label = \"Text area input\",\n          value = \"Some text\",\n          placeholder = \"Your text here\"\n        ),\n        f7Select(\n          inputId = \"select\",\n          label = \"Make a choice\",\n          choices = 1:3,\n          selected = 1\n        ),\n        f7AutoComplete(\n          inputId = \"myautocomplete\",\n          placeholder = \"Some text here!\",\n          openIn = \"dropdown\",\n          label = \"Type a fruit name\",\n          choices = c(\n            \"Apple\", \"Apricot\", \"Avocado\", \"Banana\", \"Melon\",\n            \"Orange\", \"Peach\", \"Pear\", \"Pineapple\"\n          )\n        ),\n        f7Stepper(\n          inputId = \"stepper\",\n          label = \"My stepper\",\n          min = 0,\n          color = \"default\",\n          max = 10,\n          value = 4\n        ),\n        f7Toggle(\n          inputId = \"toggle\",\n          label = \"Toggle me\"\n        ),\n        f7Picker(\n          inputId = \"picker\",\n          placeholder = \"Some text here!\",\n          label = \"Picker Input\",\n          choices = c(\"a\", \"b\", \"c\"),\n          options = list(sheetPush = TRUE)\n        ),\n        f7DatePicker(\n          inputId = \"date\",\n          label = \"Pick a date\",\n          value = Sys.Date()\n        ),\n        f7ColorPicker(\n          inputId = \"mycolorpicker\",\n          placeholder = \"Some text here!\",\n          label = \"Select a color\"\n        )\n      ),\n      f7CheckboxGroup(\n        inputId = \"checkbox\",\n        label = \"Checkbox group\",\n        choices = c(\"a\", \"b\", \"c\"),\n        selected = \"a\",\n        style = list(\n          inset = TRUE,\n          dividers = TRUE,\n          strong = TRUE,\n          outline = FALSE\n        )\n      ),\n      f7Radio(\n        inputId = \"radio\",\n        label = \"Radio group\",\n        choices = c(\"a\", \"b\", \"c\"),\n        selected = \"a\",\n        style = list(\n          inset = TRUE,\n          dividers = TRUE,\n          strong = TRUE,\n          outline = FALSE\n        )\n      )\n    )\n  ),\n  server = function(input, output) {\n  }\n) \n\n\n\n\n\nCodelibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    options = list(dark = FALSE, theme = \"ios\"),\n    title = \"Inputs Layout\",\n    f7SingleLayout(\n      navbar = f7Navbar(\n        title = \"Inputs Layout\",\n        hairline = FALSE\n      ),\n      f7List(\n        inset = TRUE,\n        dividers = TRUE,\n        strong = TRUE,\n        outline = FALSE,\n        f7Text(\n          inputId = \"text\",\n          label = \"Text input\",\n          value = \"Some text\",\n          placeholder = \"Your text here\"\n        ),\n        f7TextArea(\n          inputId = \"textarea\",\n          label = \"Text area input\",\n          value = \"Some text\",\n          placeholder = \"Your text here\"\n        ),\n        f7Select(\n          inputId = \"select\",\n          label = \"Make a choice\",\n          choices = 1:3,\n          selected = 1\n        ),\n        f7AutoComplete(\n          inputId = \"myautocomplete\",\n          placeholder = \"Some text here!\",\n          openIn = \"dropdown\",\n          label = \"Type a fruit name\",\n          choices = c(\n            \"Apple\", \"Apricot\", \"Avocado\", \"Banana\", \"Melon\",\n            \"Orange\", \"Peach\", \"Pear\", \"Pineapple\"\n          )\n        ),\n        f7Stepper(\n          inputId = \"stepper\",\n          label = \"My stepper\",\n          min = 0,\n          color = \"default\",\n          max = 10,\n          value = 4\n        ),\n        f7Toggle(\n          inputId = \"toggle\",\n          label = \"Toggle me\"\n        ),\n        f7Picker(\n          inputId = \"picker\",\n          placeholder = \"Some text here!\",\n          label = \"Picker Input\",\n          choices = c(\"a\", \"b\", \"c\"),\n          options = list(sheetPush = TRUE)\n        ),\n        f7DatePicker(\n          inputId = \"date\",\n          label = \"Pick a date\",\n          value = Sys.Date()\n        ),\n        f7ColorPicker(\n          inputId = \"mycolorpicker\",\n          placeholder = \"Some text here!\",\n          label = \"Select a color\"\n        )\n      ),\n      f7CheckboxGroup(\n        inputId = \"checkbox\",\n        label = \"Checkbox group\",\n        choices = c(\"a\", \"b\", \"c\"),\n        selected = \"a\",\n        style = list(\n          inset = TRUE,\n          dividers = TRUE,\n          strong = TRUE,\n          outline = FALSE\n        )\n      ),\n      f7Radio(\n        inputId = \"radio\",\n        label = \"Radio group\",\n        choices = c(\"a\", \"b\", \"c\"),\n        selected = \"a\",\n        style = list(\n          inset = TRUE,\n          dividers = TRUE,\n          strong = TRUE,\n          outline = FALSE\n        )\n      )\n    )\n  ),\n  server = function(input, output) {\n  }\n)\n\n\nMoreover, we added a new way to pass options to f7Radio() and f7CheckboxGroup(), namely f7CheckboxChoice() and f7RadioChoice() (note: you canâ€™t use update_* functions on them yet), so that you can pass more metadata and a description to each option (instead of just the choice name in basic shiny inputs):\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    title = \"Update radio\",\n    f7SingleLayout(\n      navbar = f7Navbar(title = \"Update f7Radio\"),\n      f7Block(\n        f7Radio(\n          inputId = \"radio\",\n          label = \"Custom choices\",\n          choices = list(\n            f7RadioChoice(\n              \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n            Nulla sagittis tellus ut turpis condimentum,\n            ut dignissim lacus tincidunt\",\n              title = \"Choice 1\",\n              subtitle = \"David\",\n              after = \"March 16, 2024\"\n            ),\n            f7RadioChoice(\n              \"Cras dolor metus, ultrices condimentum sodales sit\n            amet, pharetra sodales eros. Phasellus vel felis tellus.\n            Mauris rutrum ligula nec dapibus feugiat\",\n              title = \"Choice 2\",\n              subtitle = \"Veerle\",\n              after = \"March 17, 2024\"\n            )\n          ),\n          selected = 2,\n          style = list(\n            outline = TRUE,\n            strong = TRUE,\n            inset = TRUE,\n            dividers = TRUE\n          )\n        ),\n        textOutput(\"res\")\n      )\n    )\n  ),\n  server = function(input, output, session) {\n    output$res &lt;- renderText(input$radio)\n  }\n)\n\n\n\n\n\nCodelibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    title = \"Update radio\",\n    f7SingleLayout(\n      navbar = f7Navbar(title = \"Update f7Radio\"),\n      f7Block(\n        f7Radio(\n          inputId = \"radio\",\n          label = \"Custom choices\",\n          choices = list(\n            f7RadioChoice(\n              \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n            Nulla sagittis tellus ut turpis condimentum,\n            ut dignissim lacus tincidunt\",\n              title = \"Choice 1\",\n              subtitle = \"David\",\n              after = \"March 16, 2024\"\n            ),\n            f7RadioChoice(\n              \"Cras dolor metus, ultrices condimentum sodales sit\n            amet, pharetra sodales eros. Phasellus vel felis tellus.\n            Mauris rutrum ligula nec dapibus feugiat\",\n              title = \"Choice 2\",\n              subtitle = \"Veerle\",\n              after = \"March 17, 2024\"\n            )\n          ),\n          selected = 2,\n          style = list(\n            outline = TRUE,\n            strong = TRUE,\n            inset = TRUE,\n            dividers = TRUE\n          )\n        ),\n        textOutput(\"res\")\n      )\n    )\n  ),\n  server = function(input, output, session) {\n    output$res &lt;- renderText(input$radio)\n  }\n)\n\n\nNew f7Treeview() component\nThe new release welcomes a brand new input widget. As its name suggests, f7Treewiew() enables sorting items hierarchically within a collapsible nested list of items. This is ideal, for instance, to select files within multiple folders, as an alternative to the classic fileInput().\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nlibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    title = \"My app\",\n    f7SingleLayout(\n      navbar = f7Navbar(title = \"f7Treeview\"),\n      # group treeview with selectable items\n      f7BlockTitle(\"Selectable items\"),\n      f7Block(\n        f7Treeview(\n          id = \"treeview1\",\n          selectable = TRUE,\n          f7TreeviewGroup(\n            title = \"Selected images\",\n            icon = f7Icon(\"folder_fill\"),\n            itemToggle = TRUE,\n            lapply(1:3, function(i) f7TreeviewItem(label = paste0(\"image\", i, \".png\"),\n                                                   icon = f7Icon(\"photo_fill\")))\n          )\n        ),\n        textOutput(\"selected\")\n      ),\n\n      # group treeview with checkbox items\n      f7BlockTitle(\"Checkbox\"),\n      f7Block(\n        f7Treeview(\n          id = \"treeview2\",\n          withCheckbox = TRUE,\n          f7TreeviewGroup(\n            title = \"Selected images\",\n            icon = f7Icon(\"folder_fill\"),\n            itemToggle = TRUE,\n            lapply(1:3, function(i) f7TreeviewItem(label = paste0(\"image\", i, \".png\"),\n                                                   icon = f7Icon(\"photo_fill\")))\n          )\n        ),\n        textOutput(\"selected2\")\n      )\n    )\n  ),\n  server = function(input, output) {\n    output$selected &lt;- renderText(input$treeview1)\n    output$selected2 &lt;- renderText(input$treeview2)\n  }\n)\n\n\n\n\n\nCodelibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    title = \"My app\",\n    f7SingleLayout(\n      navbar = f7Navbar(title = \"f7Treeview\"),\n      # group treeview with selectable items\n      f7BlockTitle(\"Selectable items\"),\n      f7Block(\n        f7Treeview(\n          id = \"treeview1\",\n          selectable = TRUE,\n          f7TreeviewGroup(\n            title = \"Selected images\",\n            icon = f7Icon(\"folder_fill\"),\n            itemToggle = TRUE,\n            lapply(1:3, function(i) f7TreeviewItem(label = paste0(\"image\", i, \".png\"),\n                                                   icon = f7Icon(\"photo_fill\")))\n          )\n        ),\n        textOutput(\"selected\")\n      ),\n\n      # group treeview with checkbox items\n      f7BlockTitle(\"Checkbox\"),\n      f7Block(\n        f7Treeview(\n          id = \"treeview2\",\n          withCheckbox = TRUE,\n          f7TreeviewGroup(\n            title = \"Selected images\",\n            icon = f7Icon(\"folder_fill\"),\n            itemToggle = TRUE,\n            lapply(1:3, function(i) f7TreeviewItem(label = paste0(\"image\", i, \".png\"),\n                                                   icon = f7Icon(\"photo_fill\")))\n          )\n        ),\n        textOutput(\"selected2\")\n      )\n    )\n  ),\n  server = function(input, output) {\n    output$selected &lt;- renderText(input$treeview1)\n    output$selected2 &lt;- renderText(input$treeview2)\n  }\n)\n\n\nNew f7Form()\n\nShiny does not provide HTML forms handling out of the box (a form being composed of multiple input elements). Thatâ€™s why we introduce f7Form(). Contrary to basic shiny inputs, we donâ€™t get one input value per element but a single input value with a nested list for all inputs within the form, thereby allowing a reduction in the number of inputs on the server side. updateF7Form() can quickly update any input from the form. As a side note, the current list of supported inputs is:\n\nf7Text()\nf7TextArea()\nf7Password()\nf7Select()\n\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\n#| viewerWidth: 390\nlibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    f7SingleLayout(\n      navbar = f7Navbar(title = \"Inputs form\"),\n      f7Block(f7Button(\"update\", \"Click me\")),\n      f7BlockTitle(\"A list of inputs in a form\"),\n      f7List(\n        inset = TRUE,\n        dividers = FALSE,\n        strong = TRUE,\n        f7Form(\n          id = \"myform\",\n          f7Text(\n            inputId = \"text\",\n            label = \"Text input\",\n            value = \"Some text\",\n            placeholder = \"Your text here\",\n            style = list(\n              description = \"A cool text input\",\n              outline = TRUE,\n              media = f7Icon(\"house\"),\n              clearable = TRUE,\n              floating = TRUE\n            )\n          ),\n          f7TextArea(\n            inputId = \"textarea\",\n            label = \"Text Area\",\n            value = \"Lorem ipsum dolor sit amet, consectetur\n              adipiscing elit, sed do eiusmod tempor incididunt ut\n              labore et dolore magna aliqua\",\n            placeholder = \"Your text here\",\n            resize = TRUE,\n            style = list(\n              description = \"A cool text input\",\n              outline = TRUE,\n              media = f7Icon(\"house\"),\n              clearable = TRUE,\n              floating = TRUE\n            )\n          ),\n          f7Password(\n            inputId = \"password\",\n            label = \"Password:\",\n            placeholder = \"Your password here\",\n            style = list(\n              description = \"A cool text input\",\n              outline = TRUE,\n              media = f7Icon(\"house\"),\n              clearable = TRUE,\n              floating = TRUE\n            )\n          )\n        )\n      ),\n      verbatimTextOutput(\"form\")\n    )\n  ),\n  server = function(input, output, session) {\n    output$form &lt;- renderPrint(input$myform)\n\n    observeEvent(input$update, {\n      updateF7Form(\n        \"myform\",\n        data = list(\n          \"text\" = \"New text\",\n          \"textarea\" = \"New text area\",\n          \"password\" = \"New password\"\n        )\n      )\n    })\n  }\n)\n\n\n\n\n\nCodelibrary(shiny)\nlibrary(shinyMobile)\n\nshinyApp(\n  ui = f7Page(\n    f7SingleLayout(\n      navbar = f7Navbar(title = \"Inputs form\"),\n      f7Block(f7Button(\"update\", \"Click me\")),\n      f7BlockTitle(\"A list of inputs in a form\"),\n      f7List(\n        inset = TRUE,\n        dividers = FALSE,\n        strong = TRUE,\n        f7Form(\n          id = \"myform\",\n          f7Text(\n            inputId = \"text\",\n            label = \"Text input\",\n            value = \"Some text\",\n            placeholder = \"Your text here\",\n            style = list(\n              description = \"A cool text input\",\n              outline = TRUE,\n              media = f7Icon(\"house\"),\n              clearable = TRUE,\n              floating = TRUE\n            )\n          ),\n          f7TextArea(\n            inputId = \"textarea\",\n            label = \"Text Area\",\n            value = \"Lorem ipsum dolor sit amet, consectetur\n              adipiscing elit, sed do eiusmod tempor incididunt ut\n              labore et dolore magna aliqua\",\n            placeholder = \"Your text here\",\n            resize = TRUE,\n            style = list(\n              description = \"A cool text input\",\n              outline = TRUE,\n              media = f7Icon(\"house\"),\n              clearable = TRUE,\n              floating = TRUE\n            )\n          ),\n          f7Password(\n            inputId = \"password\",\n            label = \"Password:\",\n            placeholder = \"Your password here\",\n            style = list(\n              description = \"A cool text input\",\n              outline = TRUE,\n              media = f7Icon(\"house\"),\n              clearable = TRUE,\n              floating = TRUE\n            )\n          )\n        )\n      ),\n      verbatimTextOutput(\"form\")\n    )\n  ),\n  server = function(input, output, session) {\n    output$form &lt;- renderPrint(input$myform)\n\n    observeEvent(input$update, {\n      updateF7Form(\n        \"myform\",\n        data = list(\n          \"text\" = \"New text\",\n          \"textarea\" = \"New text area\",\n          \"password\" = \"New password\"\n        )\n      )\n    })\n  }\n)"
  },
  {
    "objectID": "posts/2024-05-13-shinyMobile-2.0.0/index.html#breaking-changes",
    "href": "posts/2024-05-13-shinyMobile-2.0.0/index.html#breaking-changes",
    "title": "shinyMobile 2.0.0: a preview",
    "section": "Breaking changes",
    "text": "Breaking changes\nSome components have disappeared from Framework7 and we had to deprecate them as they no longer work. Other long time deprecated shinyMobile elements are also removed to cleanup the codebase. We invite you to review the changelog to see a list of all changes in this release."
  },
  {
    "objectID": "posts/2024-05-13-shinyMobile-2.0.0/index.html#soft-deprecation",
    "href": "posts/2024-05-13-shinyMobile-2.0.0/index.html#soft-deprecation",
    "title": "shinyMobile 2.0.0: a preview",
    "section": "Soft deprecation",
    "text": "Soft deprecation\nSome function parameters have changed and are now deprecated with lifecycle. Youâ€™ll see a warning message if you use them and we invite you to accordingly update your code."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "",
    "text": "â€œGoogle Season of Docs (GSoD) provides support for open source projects to improve their documentation and gives professional technical writers an opportunity to gain experience in open source.â€ (Source: Program website)\nThe program makes it possible for technical writers to work closely with an open-source community they may or may not have been engaged with, to solve real problems with high-quality documentation.\nIn the end, an awareness of open source, of documentation and technical writing is raised, while participating open source organizations benefit from an improvement in their documentation. The R Project participated in GSoD as an open-source organization for the first time this year after several years of participating in Google Summer of Code (GSoC), another open-source program focused on coding."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#the-user-conference",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#the-user-conference",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "The useR! conference",
    "text": "The useR! conference\nuseR! is the main annual conference for the R user and developer community that is organized by a community of volunteers and supported by the R Foundation. It is organized by a different team of community organizers each year and has been held since 2004. The useR! conference program consists of both invited and user-contributed presentations in addition to tutorials and other social events.\nWith so much historical information about useR! scattered around Git repositories, useR! websites, and organizersâ€™ hard-disks, the R Project proposed to organize useR! documentation with two outputs- an information board and a knowledgebase. The knowledgebase was proposed to take the form of an online book, inspired by examples such as the satRdays knowledgebase. The information board was proposed as a dashboard to interactively browse historical information. These two projects were carried on concurrently over a span of 6 months (May - November, 2021) during which my primary responsibility as a technical writer was to curate historical useR! conference data and develop the information board with this data."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#the-information-board",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#the-information-board",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "The Information board",
    "text": "The Information board\nWhy an information board?\nAfter participating in useR! 2021 as a part of the organizing committee, I identified several gaps within the organizational process that an information board could fill up. Organizers spend a lot of time looking for information from past useR! conference websites, past organizers, and other archives of un-structured or semi-structured data. This process repeats each year for every useR! conference and this seems to put a burden on past organizers or co-ordinators to continuously provide information to future organizers.\nTo fill in these gaps, I proposed to: - gather data in a structured format for at least the past six useR! conferences - build a dashboard using this data-set - structure things in such a way that updating the data files leads to an updated dashboard after a rebuild process\nThe final product could be found by accessing the following URL: https://bit.ly/infoboard-cynkra"
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#use-cases",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#use-cases",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Use cases",
    "text": "Use cases\nA typical useR! conference program consists of keynotes, regular talks, lightning talks, poster sessions, tutorials and social events. The organizing team for each conference would need to identify keynote speakers, select talks and tutorials, and determine which social events to offer. In addition to those, the organizing team would need to set up partnerships for the conference - sponsors (that can contribute in different ways) and partner organizations. To prospective organizers who have never organized useR! or a conference of such capacity, it is burdensome to sift through 15+ past useR! websites to search out the type of tutorials that have been offered in the past, the number of keynote presentations to offer and around what topics, or what kind of social events to host.\nIt is difficult to ascertain if a talk has been presented in a useR! in the past without access to structured and filterable historical data.\nFurthermore, with diversity and inclusion in mind, it could be difficult to determine presenters that have already presented many times before (perhaps on a similar topic) and those from under-represented groups that have only had a few chances to present talks at a useR!\nFor organizers to target sponsors who are interested in R and who may have sponsored in the past within the location of the conference is hard without data-driven assistance. For potential sponsors, it provides some insight into the scope and reach of useR! conferences.\nFor entities like the R Foundation, R Consortium and R Forwards, the information board provides an easy way to gain insights into the history of useR! while planning for the future in a global and diverse context.\nOther conferences beyond useR! could benefit from the information board as it provides organized data around people, organizations, and presentations that could be helpful in planning local and regional R events."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#tools",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#tools",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Tools",
    "text": "Tools\nI used flexdashboard for the structure and layout of the dashboard, echarts4r for charts, and reactable for interactive tables."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#technical-information",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#technical-information",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Technical information",
    "text": "Technical information\nThe source for this dashboard lives in a GitLab repository where issues or merge requests can be raised.\nAll the data are located in the data directory and a description of the years which each dataset covers.\nThe charts and tables are produced from scripts in the R directory - hopefully making it easier to reproduce them or use them in other contexts.\nThe sidebar menu, footer and JavaScript codes are saved as HTML fragments that are included via the YAML header of the index.Rmd file."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#license",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#license",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "License",
    "text": "License\nThe data is available for download under a CC BY 4.0 license, while the R code is available under a GPL-3.0 license."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#technical-writing-experience",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#technical-writing-experience",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Technical Writing experience",
    "text": "Technical Writing experience\nHaving written R articles for Open Data Science in the past, the experience gained from this GSoD project improved my technical writing and project management skills. In a collaborative sense: I received and implemented feedback several times a month from different volunteers across several timezones, while working to produce the deliverable per project-phase and covering the general scope of the project."
  },
  {
    "objectID": "posts/2022-01-05-gsod-user-infoboard/index.html#acknowledgements",
    "href": "posts/2022-01-05-gsod-user-infoboard/index.html#acknowledgements",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis project would not have been possible at this time without support from Google via the GSoD program. Much appreciation goes to the GSoD admins for the R Project - Heather Turner and Matt Bannert - who did a lot of admin work from the proposal to the final report. I also appreciate Noa Tamir, who excellently managed the week-to-week supervision of this work. I appreciate the help I received from several volunteers on this project including past organizers who provided data from their archives.\nFinally, cynkra is passionate about open-source and the R community, and this has provided an enabling environment for this project to succeed and to continue succeeding."
  },
  {
    "objectID": "posts/2019-08-12-tsbox-02/index.html",
    "href": "posts/2019-08-12-tsbox-02/index.html",
    "title": "tsbox 0.2: supporting additional time series classes",
    "section": "",
    "text": "The tsbox package makes life with time series in R easier. It is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic and allow the user to combine time series of multiple non-standard and irregular frequencies. A detailed overview of the package functionality is given in the documentation page (or in a previous blog-post).\nVersion 0.2 is now on CRAN and provides a larger number of bug fixes. Non-standard column names are now handled correctly, and non-standard column orders are treated consistently."
  },
  {
    "objectID": "posts/2019-08-12-tsbox-02/index.html#new-classes",
    "href": "posts/2019-08-12-tsbox-02/index.html#new-classes",
    "title": "tsbox 0.2: supporting additional time series classes",
    "section": "New Classes",
    "text": "New Classes\nThere are two more time series classes supported: tis time series, from the tis package, and irts time series, from the tseries package.\nTo create an object of these classes, it is sufficient to use the appropriate converter.\nE.g., for tis time series:\nlibrary(tsbox)\nts_tis(fdeaths)\n##       Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n## 1974  901  689  827  677  522  406  441  393  387  582  578  666\n## 1975  830  752  785  664  467  438  421  412  343  440  531  771\n## 1976  767 1141  896  532  447  420  376  330  357  445  546  764\n## 1977  862  660  663  643  502  392  411  348  387  385  411  638\n## 1978  796  853  737  546  530  446  431  362  387  430  425  679\n## 1979  821  785  727  612  478  429  405  379  393  411  487  574\n## class: tis\nOr for irts time series:\nhead(ts_irts(fdeaths))\n## 1974-01-01 00:00:00 GMT 901\n## 1974-02-01 00:00:00 GMT 689\nConversion works from all classes to all classes, and we can easily convert these objects to any other time series class, or a data frame:\nx.tis &lt;- ts_tis(fdeaths)\nhead(ts_df(x.tis))\n##         time value\n## 1 1974-01-01   901\n## 2 1974-02-01   689\n## 3 1974-03-01   827\n## 4 1974-04-01   677\n## 5 1974-05-01   522\n## 6 1974-06-01   406"
  },
  {
    "objectID": "posts/2019-08-12-tsbox-02/index.html#class-agnostic-functions",
    "href": "posts/2019-08-12-tsbox-02/index.html#class-agnostic-functions",
    "title": "tsbox 0.2: supporting additional time series classes",
    "section": "Class-agnostic functions",
    "text": "Class-agnostic functions\nBecause coercion works reliably and is well tested, we can use it to make functions class-agnostic. If a class-agnostic function works for one class, it works for all:\nts_pc(ts_tis(fdeaths))\nts_pc(ts_irts(fdeaths))\nts_pc(ts_df(fdeaths))\nts_pc(fdeaths)\nts_pc calculates percentage change rates towards the previous period. It works like a â€˜genericâ€™ function: You can apply it on any time series object, and it will return an object of the same class as its input.\nSo, whether we want to smooth, scale, differentiate, chain-link, forecast, regularize, or seasonally adjust a series, we can use the same commands to all time series classes. tsbox offers a comprehensive toolkit for the basics of time series manipulation. Here are some additional examples:\nts_pcy(fdeaths)                # p.c., compared do same period of prev. year\nts_forecast(fdeaths)           # forecast, by exponential smoothing\nts_seas(fdeaths)               # seasonal adjustment, by X-13\nts_frequency(fdeaths, \"year\")  # convert to annual frequency\nts_span(fdeaths, \"-1 year\")    # limit time span to final year\nThere are many more. Because they all start with ts_, you can use auto-complete to see whatâ€™s around. Most conveniently, there is a time series plot function that works for all classes and frequencies:\nts_plot(\n  `Airline Passengers` = AirPassengers,\n  `Lynx trappings` = ts_tis(lynx),\n  `Deaths from Lung Diseases` = ts_xts(fdeaths),\n  title = \"Airlines, trappings, and deaths\",\n  subtitle = \"Monthly passengers, annual trappings, monthly deaths\"\n)\n\n\n\ntime series plot"
  },
  {
    "objectID": "posts/2020-02-09-tempdisagg/index.html",
    "href": "posts/2020-02-09-tempdisagg/index.html",
    "title": "tempdisagg: converting quarterly time series to daily",
    "section": "",
    "text": "Not having a time series at the desired frequency is a common problem for researchers and analysts. For example, instead of quarterly sales, they only have annual sales. Instead of a daily stock market index, they only have a weekly index. While there is no way to fully make up for the missing data, there are useful workarounds: with the help of one or more high-frequency indicator series, the low-frequency series may be disaggregated into a high-frequency series.\nThe package tempdisagg implements the standard methods for temporal disaggregation: Denton, Denton-Cholette, Chow-Lin, Fernandez and Litterman. Our article on temporal disaggregation of time series in the R-Journal describes the package and the theory of temporal disaggregation in more detail.\nThe package has been around for eight years, enabling the standard year or quarter to month or quarter disaggregation. With version 1.0, there are now some major new features: disaggregation can be performed from any frequency to any frequency. Also, tempdisagg now supports time series classes other than ts."
  },
  {
    "objectID": "posts/2020-02-09-tempdisagg/index.html#time-series-can-be-stored-in-data-frames",
    "href": "posts/2020-02-09-tempdisagg/index.html#time-series-can-be-stored-in-data-frames",
    "title": "tempdisagg: converting quarterly time series to daily",
    "section": "Time series can be stored in data frames",
    "text": "Time series can be stored in data frames\nBecause we are dealing with daily data, we keep the data in a data.frame, rather than in a ts object. Other time series objects, such as xts and tsibble, are possible as well. For conversion and visualization, we use the tsbox package.\nlibrary(tsbox)\nts_plot(gdp.q, title = \"Swiss GDP\", subtitle = \"real, not seasonally adjusted\")\n\n\n\nSeries to disaggregate: quarterly gross domestic product of Switzerland"
  },
  {
    "objectID": "posts/2020-02-09-tempdisagg/index.html#disaggregation-to-daily-frequency",
    "href": "posts/2020-02-09-tempdisagg/index.html#disaggregation-to-daily-frequency",
    "title": "tempdisagg: converting quarterly time series to daily",
    "section": "Disaggregation to daily frequency",
    "text": "Disaggregation to daily frequency\nWhile disaggregation can also be performed without other series, we use Swiss stock market data as an indicator series to disaggregate GDP. Data of the stock market index, the SMI, is also included in tempdisagg. Weekend and holiday values have been interpolated, because td does not allow the presence of missing values.\nts_plot(spi.d, title = \"Swiss Performance Index\", subtitle = \"daily values, interpolated\")\n\n\n\nDaily indicator series: Swiss Performance Index\n\n\n\nThe following uses the Chow-Lin method to disaggregate the series. A high rho parameter takes into account that the two series are unlikely to be co-integrated.\nm.d.stocks &lt;- td(gdp.q ~ spi.d, method = \"chow-lin-fixed\", fixed.rho = 0.9)\nsummary(m.d.stocks)\n##\n## Call:\n## td(formula = gdp.q ~ spi.d, method = \"chow-lin-fixed\", fixed.rho = 0.9)\n##\n## Residuals:\n##    Min     1Q Median     3Q    Max\n## -10656  -1760   1076   3796   8891\n##\n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept) 1.320e+03  2.856e+01   46.22   &lt;2e-16 ***\n## spi.d       5.512e-02  3.735e-03   14.76   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## 'chow-lin-fixed' disaggregation with 'sum' conversion\n## 59 low-freq. obs. converted to 5493 high-freq. obs.\n## Adjusted R-squared: 0.7928 AR1-Parameter:   0.9\nAnd here is the result: A daily series of GDP\ngdp.d.stocks &lt;- predict(m.d.stocks)\nts_plot(\n  ts_scale(\n    ts_c(gdp.d.stocks, gdp.q)\n  ),\n  title = \"Daily disaggregated GDP\",\n  subtitle = \"one indicator\"\n)\n\n\n\nSwiss GDP, disaggregated to daily\n\n\n\nLike with all disaggregation methods in tempdisagg, the resulting series fulfills the aggregation constraint (the resulting series is as long as the indicator, and needs to be shortened for a comparison):\nall.equal(\n  ts_span(\n    ts_frequency(gdp.d.stocks, \"quarter\", aggregate = \"sum\"),\n    end = \"2019-07-01\"\n  ),\n  gdp.q\n)\n## [1] TRUE"
  },
  {
    "objectID": "posts/2021-04-09-deprecating-pkgdown/index.html",
    "href": "posts/2021-04-09-deprecating-pkgdown/index.html",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "",
    "text": "Websites for R packages built with {pkgdown} have become a de-facto standard over the last few years. Many R packages build their site during Continuous Integration (CI) runs, pushing the assets to the special gh-pages branch (even though now any branch can be used to deploy a website).\nSometimes, repositories are transferred to a new user/organization or the package is renamed. While GitHub takes care of redirecting repository URLs, the pkgdown URLs (https://&lt;username&gt;.github.io/&lt;rpackage&gt;) are not redirected. Since some users might have bookmarked specific URLs or the URLs appear in their browsing history, it would be great if these links do not return a 404.\nThis blog post proposes several ways to handle this gracefully:\nAll options hinge on the observation that users and organizations can create a user or organization site that will be the source for https://&lt;username&gt;.github.io/&lt;package&gt; after the renaming. The user site will also serve robots.txt that advises crawlers to avoid deprecated contents."
  },
  {
    "objectID": "posts/2021-04-09-deprecating-pkgdown/index.html#user-or-organization-site",
    "href": "posts/2021-04-09-deprecating-pkgdown/index.html#user-or-organization-site",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "User or organization site",
    "text": "User or organization site\nIn GitHub, users can create a user repository &lt;username&gt;/&lt;username&gt;.github.io. This repo will be served automatically as a web page on https://&lt;username&gt;.github.io/ . In this repo, a directory can be created which corresponds to the respective GitHub Pages site of the original repo. Example: The rpackage/ directory in the &lt;username&gt;/&lt;username&gt;.github.io repository corresponds to https://&lt;username&gt;.github.io/rpackage. If both &lt;username&gt;/&lt;rpackage&gt; and &lt;username&gt;/&lt;username&gt;.github.io/&lt;rpackage&gt; exist, the former takes precedence. This means that you can prepare everything in your user repository &lt;username&gt;/&lt;username&gt;.github.io and it will work immediately after you rename your package repository. The following has worked for https://krlmlr.github.io/fledge/, which has moved to https://cynkra.github.io/fledge/:\n\nCreate repository &lt;username&gt;/&lt;username&gt;.github.io\nIn &lt;username&gt;/&lt;username&gt;.github.io create directory &lt;rpackage&gt;\nPopulate the &lt;rpackage&gt; directory using one of the methods described below\nPush to GitHub\nRename repository\n\nAll of this works the same way for organizations. The munch package was previously located at https://cynkra.github.io/SwissCommunes/ The original pages, with a warning, are defined at cynkra/cynkra.github.io."
  },
  {
    "objectID": "posts/2021-04-09-deprecating-pkgdown/index.html#redirection",
    "href": "posts/2021-04-09-deprecating-pkgdown/index.html#redirection",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Redirection",
    "text": "Redirection\nBasic idea: set up an HTML redirect from https://&lt;username-old&gt;.github.io/&lt;package&gt; to https://&lt;username-new&gt;.github.io/&lt;package&gt;.\nTo achieve this, create an index.html in &lt;username&gt;/&lt;username&gt;.github.io/rpackage with the following contents:\n&lt;meta http-equiv=\"refresh\" content=\"0; url=&lt;url to redirect to&gt;\" /&gt;\nHowever, some redirection practices like this one are considered bad practice (â€œUse of meta refresh is discouraged by the World Wide Web Consortium (W3C).â€)[^1]. Also, users might find it sketchy to see some redirection happening shortly after they visited a site. Last, the redirection shown above only works for the top-level domain. Level 2 or level 3 links like &lt;url&gt;/level1/level2 will not work and return a 404."
  },
  {
    "objectID": "posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-css",
    "href": "posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-css",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Deprecation via CSS",
    "text": "Deprecation via CSS\nA better way to deprecate a pkgdown/GitHub Pages site is to serve a static version of the last state before the package was moved, and add information to the user that the site has moved.\nAn easy way to achieve this is to include a little CSS snippet. The following will add a colored line before the page-header div in the pkgdown site.\n.page-header:after {\n  content: \"You are viewing an outdated page which is not going to be updated anymore. Please go to &lt;https:/new-url.com&gt; for the latest version.\";\n  font-size: 12px;\n  font-style: italic;\n  color: #f03333;\n}\n\n\n\nDeprecation information in the header via CSS\n\n\n\nPlace this code in the pkgdown/ directory of your package and it will be automatically picked up when the site is built next time:\n\nIn your package, add the CSS snippet from above to pkgdown/extra.css (CSS name can be different) in the repository/R package which should be deprecated\nCall pkgdown::build_site() one last time\nCopy the contents of docs/ to &lt;username&gt;/&lt;username&gt;.github.io/&lt;packagename&gt;\n\nUnfortunately, the :after operator does not allow hyperlinks, so the new URL will not be clickable."
  },
  {
    "objectID": "posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-bulk-edit",
    "href": "posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-bulk-edit",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Deprecation via bulk edit",
    "text": "Deprecation via bulk edit\nFor the URL to be clickable, the HTML files must be edited. The find, xargs and sed utilities help to automate this.\npkgdown uses the Bootstrap framework, which has alerts that serve the purpose. They look best just before the closing &lt;/header&gt; element. The following command line adds an alert to each HTML page, advertising https://cynkra.github.io/munch as the target URL. It must be run in the rpackage directory of &lt;username&gt;/&lt;username&gt;.github.io:\nfind -name \"*.html\" |\n  xargs sed -i -r 's#(^.*[&lt;]/header[&gt;])#&lt;div class=\"alert alert-warning\" role=\"alert\"&gt;&lt;strong&gt;Warning!&lt;/strong&gt; This content has moved to &lt;a href=\"https://cynkra.github.io/munch\"&gt;https://cynkra.github.io/munch&lt;/a&gt;.&lt;/div&gt;\\n\\1#'\nThis assumes GNU sed. MacOS users will need to use gsed, or -i.bak instead of -i and deal with the leftover *.bak files.\n\n\n\nDeprecation information in the header via editing HTML\n\n\n\nAlways advertising the new root works well enough because it is very likely that the structure of the site will eventually change after the repository rename."
  },
  {
    "objectID": "posts/2021-04-09-deprecating-pkgdown/index.html#web-crawlers",
    "href": "posts/2021-04-09-deprecating-pkgdown/index.html#web-crawlers",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Web crawlers",
    "text": "Web crawlers\nIt is a good idea to make the deprecated contents invisible to web crawlers. Add a file robots.txt to the root of &lt;username&gt;/&lt;username&gt;.github.io. The following contents forbids crawling the /SwissCommunes/ directory which contains the old snapshot with pointers to the new location:\nUser-agent: *\nDisallow: /SwissCommunes/"
  },
  {
    "objectID": "posts/2019-04-10-tsbox-01/index.html",
    "href": "posts/2019-04-10-tsbox-01/index.html",
    "title": "tsbox 0.1: class-agnostic time series",
    "section": "",
    "text": "The R ecosystem knows a vast number of time series classes: ts, xts, zoo, tsibble, tibbletime or timeSeries. The plethora of standards causes confusion. As different packages rely on different classes, it is hard to use them in the same analysis. tsbox provides a set of tools that make it easy to switch between these classes. It also allows the user to treat time series as plain data frames, facilitating the use with tools that assume rectangular data.\n\n\n\n\n\ncomic by xkcd\n\n\n\n\nThe tsbox package is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic and allow the user to combine multiple non-standard and irregular frequencies. Because coercion works reliably, it is easy to write functions that work identically for all classes. So whether we want to smooth, scale, differentiate, chain-link, forecast, regularize, or seasonally adjust a time series, we can use the same tsbox-command for any time series class.\nThis blog gives a short overview of the changes introduced in 0.1. A detailed overview of the package functionality is given in the documentation page (or in a previous blog-post).\n\nKeeping explicit missing values\nVersion 0.1, now on CRAN, brings many bug fixes and improvements. A substantial change involves the treatment of NA values in data frames. Previously, all NAs in data frames were treated as implicit and were only made explicit by a call to ts_regular.\nThis has changed now. If you convert a ts object to a data frame, all NA values will be preserved. To replicate previous behavior, apply the ts_na_omit function:\nlibrary(tsbox)\nx.ts &lt;- ts_c(mdeaths, austres)\nx.ts\nts_df(x.ts)\nts_na_omit(ts_df(x.ts))\n\n\nts_span extends outside of series span\nThis lays the groundwork for ts_span to be extensible. With extend = TRUE, ts_span extends a regular series with NA values, up to the specified limits, similar to base window. Like all functions in tsbox, this is frequency-agnostic. For example, in the following, the monthly series mdeaths is extended by monthly NA values, while the quarterly series austres is extended by quarterly NA values.\nx.df &lt;- ts_df(ts_c(mdeaths, austres))\nts_span(x.df, end = \"1999-12-01\", extend = TRUE)\n\n\nts_default standardizes column names in a data frame\nIn rectangular data structures, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the â€˜longâ€™ format. By default, tsbox detects a value, a time and zero, one or several id columns. Alternatively, the time column and the value column can be explicitly named time and value. If explicit names are used, the column order will be ignored.\nWhile automatic column name detection is useful in interactive mode, it produces unnecessary overhead in longer workflows. The helper function ts_default detects and renames the time and the value column so that auto-detection will be turned off in subsequent steps (note that the names of the id columns are not affected):\nx.df &lt;- ts_df(ts_c(mdeaths, austres))\nnames(x.df) &lt;- c(\"a fancy id name\", \"date\", \"count\")\nts_plot(x.df)  # tsbox is fine with that\nts_default(x.df)\n\n\nts_summary summarizes time series\nts_summary provides a frequency agnostic summary of a ts-boxable object:\nts_summary(ts_c(mdeaths, austres))\n#&gt;        id obs    diff freq      start        end\n#&gt; 1 mdeaths  72 1 month   12 1974-01-01 1979-12-01\n#&gt; 2 austres  89 3 month    4 1971-04-01 1993-04-01\nts_summary returns a plain data frame that can be used for any purpose. It is also recommended for the extraction of various time series properties, such as start, freq or id:\nts_summary(austres)$id\n#&gt; [1] \"austres\"\nts_summary(austres)$start\n#&gt; [1] \"1971-04-01\"\n\n\nAnd a cheat sheet!\nFinally, we fabricated a tsbox cheat sheet that summarizes most functionality. Print and enjoy working with time series."
  },
  {
    "objectID": "posts/2022-09-14-rstudio-efs-nfs/index.html",
    "href": "posts/2022-09-14-rstudio-efs-nfs/index.html",
    "title": "EFS vs.Â NFS for RStudio on Kubernetes (AWS): Configuration and considerations",
    "section": "",
    "text": "As a consulting company with a strong focus on R and Posit (formerly RStudio) products, we have gathered quite a bit of experience in configuration and deployment of Workbench and Connect products in the cloud. We would like to share some of these insights in the following blog post and shine some light on the file system debate around these applications. While the â€œhow toâ€ is very well outlined by various RStudio/Posit resources (1, 2, 3, 4), thereâ€™s one important part of the setup which is actively debated among system engineers: the choice and handling of the file system.\nBut letâ€™s start at the beginning: in the cloud, one has essentially two choices for self-hosting RStudio applications: a single-instance installation on a virtual machine or going with Kubernetes (k8s) for a container-based, high-availability (HA) setup. The latter comes with many benefits, both for users and administrators: HA ensures stability of the service and allows for controlled upgrades, in the best case without any interruption or downtime for users. Users profit from the possible option of requesting CPU and memory resources as needed for their analysis - assuming the k8s cluster runs with an autoscaler which allows provisioning new worker nodes as needed. Potential downsides could be an increased administration complexity, decreased file system performance and eventually higher costs. Though these points cannot be generalized and always depend on the cluster at hand, its purpose and the team managing it."
  },
  {
    "objectID": "posts/2022-09-14-rstudio-efs-nfs/index.html#file-systems---general",
    "href": "posts/2022-09-14-rstudio-efs-nfs/index.html#file-systems---general",
    "title": "EFS vs.Â NFS for RStudio on Kubernetes (AWS): Configuration and considerations",
    "section": "File systems - general",
    "text": "File systems - general\nUsers are in continuous contact with the file system even though most are not actively thinking about it. Each time a file is read or written, e.g., during the installation of an R package, the choice of the file system matters. It determines\n\nHow fast the request is processed (latency)\nHow many bytes are transferred per seconds (throughput; might differ between read and write operations)\nHow many files can be processed at the same time (concurrency)\n\nTo understand the following discussion about Elastic File System (EFS) and Network File System (NFS) (including explanations about both), a bit of background is needed how read/write operations can generally be categorized.\n\nRead-write-once (RWO), read-only-many (ROX) and read-write-many (RWX) file systems\nApplications running on on-premise machines or single cloud instances usually read and write to a read-write-once (RWO) file system. The â€œonceâ€ here means that this specific file system can only be attached to one instance. Read-only-many (ROX) allows multiple instances to attach a volume but only for reading - only one instance is allowed to mount it in read-write mode. Given their restriction on a single mount point, RWO file systems are usually faster than ROX or RWX. However, RWO comes with the following limitations for HA usage:\n\nthey can only operate in a single AZ (i.e., an RWO volume can only be mounted by a node in the same zone) and this one must match the instanceâ€™s AZ\nthey cannot be used in (real) high-availability setups as this usually requires pods to run on different nodes - and RWO file systems can only be mounted by a single node\n\nFile systems which support read-write-many (RWX) operations allow the attachment by multiple instances, usually across availability zones. They usually have a central (remote) address which can be used to mount them into an instance. This allows for a central deployment of RWX providers on standalone machines.\nNOTE1: Availability zones in the cloud are discrete data centers with redundant power, networking, and connectivity in a certain region. Issues appearing in one particular zone do not affect others. Their main purpose of different AZs is to increase reliability.\nNOTE2: Thereâ€™s also read-write-once-pod (RWOP) which means that only one pod on a certain node can read/write to a volume. This is only supported for CSI volumes and k8s &gt; 1.22 [source].\n\nImage source"
  },
  {
    "objectID": "posts/2022-09-14-rstudio-efs-nfs/index.html#kubernetes-high-availability-and-nfs-server",
    "href": "posts/2022-09-14-rstudio-efs-nfs/index.html#kubernetes-high-availability-and-nfs-server",
    "title": "EFS vs.Â NFS for RStudio on Kubernetes (AWS): Configuration and considerations",
    "section": "Kubernetes high-availability and NFS server",
    "text": "Kubernetes high-availability and NFS server\nTo allow for high-availability (HA) in k8s installations, an individual HA-pod needs to communicate with the respective others and access the same files. As the load is usually balanced across replicas, this requires a file system which allows for multiple r/w mounts, i.e., a RWX one.\nThis is also the case for RStudio applications in HA configuration. There are multiple options for RWX file systems. The most common one, which is also used in many custom multi-server on-premise environments, is NFS.\nWhile NFS is great, well known and fast, it is not so easy to make use of it in the cloud, especially in isolated environments like k8s clusters. On AWS, there is no default k8s support for NFS and hence one needs to deploy their own NFS server which serves as the central access point for mount requests. (An NFS server could in theory live anywhere, as long as itâ€™s reachable by the cluster nodes.)\nThereâ€™s an official NFS Ganesha provider within the kubernetes-sigs (SIG = special-interest-group) organization on GitHub. However, it does not support HA and we are not aware of any other one which does. Hence, the NFS server within a k8s cluster is usually provided by a single pod. This comes with some severe implications: all pods which rely on some NFS mounts are dependent on this single NFS pod. If the NFS pod gets recreated for some reason (e.g., because its being moved to a different node or the deployment is updated), all pods which a NFS mount will be unable to read and write data until the new node has attached the NFS volume again (which is usually still bound to the old node for ~ 5 min).\nThis is a pivotal and tricky issue: the pod health of a pod making use of an NFS mount does not necessarily depend on the NFS mount once its running. This means if the NFS server pod is down (for whatever reason), dependent deployments appear as â€œhealthyâ€ to the cluster (admins) but the users will fail to read and write data.\nThe number of times a pod gets recreated/restarted/moved on a cluster depends on multiple factors:\n\nHow often are the individual nodes recreated?\nWho is responsible for scheduling pods to individual nodes?\nAre nodes running on permanent/reserved instances or on SPOT instances (an AWS-only term for cheaper nodes which can be reclaimed any time) which are more often reclaimed/recreated by the cloud provider?\n\n\nOur experiences with NFS on Kubernetes\nWe at cynkra started out with an NFS server for hosting RStudio applications on a k8s cluster, mainly because existing guides suggested doing so and warn using EFS. Judging from our experiences on a 3-5 node cluster on AWS with a self-managed autoscaler and node refresh enabled (which means that nodes get replaced frequently, e.g., whenever a new machine image is available), we have seen the NFS pod getting relocated every few days/weeks. Often this caused the mentioned problems above, i.e., RSC and/or RSW not being able to use due to stale file handles or other r/w issues. If the NFS server ran in an HA setup, this task would be completely trouble-free as the other pods would take over the incoming requests. However, in a non-HA deployment, this causes potentially unwanted downtime for multiple dependent applications.\nWhile there are ways in k8s to reduce/avoid the automatic recreation of non-HA pods, serving content on a k8s cluster usually comes with the philosophy of being able to move/recreate every pod in the cluster without immediate consequences. Yet the fact that this seems impossible (as of right now) for an NFS server deployment represents a constant stability risk. It makes the HA deployments of all RStudio services essentially meaningless as they would all fail once the NFS pod would be down.\nOne could also host the NFS server outside of the cluster on a normal VM which does not reboot that frequently and does not recreate the NFS server service. However, this would mean opening up the k8s network to include this machine and eventually hosting a dedicated machine only for the NFS server. This approach would also not solve the potential bottleneck issues - whenever this machine needs a reboot, all dependent services would face some downtime. In addition, this approach would not be multi-AZ capable because the private subnets, which are used for a fast and secure connection between the nodes, do not support this - private subnets can only operate within a single AZ. A k8s cluster is essentially about two things: high-availability and scalability. Adding a static, non-scalable component (= remote NFS server) which acts as a dependency for many services, into the cluster contradicts with this principle.\nThe main happenings which made us overthink our current setup was seeing spurious behavior after NFS pod recreations: with both the NFS pods and the RStudio pods in a healthy state, sometimes â€˜stale file handleâ€™ issues occurred for users when trying to write data. RSC was not able to serve content as it could not read data anymore. All of these were issues detected by users and reported to the responsible engineers (i.e., us) - which is exactly what engineers try to avoid at all cost. The issue could often be solved by simply recreating the RSC pod, which then triggered a fresh reconnection to the NFS server. We assume this behavior to most likely be a regression caused by a previous NFS pod relocation.\nIt was time to look into alternatives. Since some years, most cloud providers offer their own, cloud-based RWX file systems. These claim to be very flexible in the way that they are available for mounts in all AZs. Cluster admins only need to configure a driver (which allows to define certain mount options) but the file system itself is hosted by the cloud provider in an HA setting. AWS calls it EFS (elastic file system), on Azure itâ€™s called â€œazure files.â€ Google does not provide a native RWX solution (only ROX) and one is required to use a self-managed NFS server for RWX.\nSince we are on AWS, we thought about giving EFS, the native AWS multi-AZ RWX offering, a try. We were aware of the warnings given out by RStudio about possible performance issues. Luckily, we were not the first ones to try EFS with RStudio products. The RStudio engineers already wrote an article about RStudio applications with EFS, especially targeting the performance topic (see linked article).\nBesides the higher price (around 3x) of EFS compared to EBS (elastic block storage = default RWO file system), which is used behind the scenes by an NFS server deployment, EFS is somewhat lacking in performance compared to NFS. The following figures show some file system benchmark results which we have carried out on different instance types. Network-optimized instance types (r5n on AWS) were said to improve EFS performance which is backed up by our results.\n Fig. 1: Sequential benchmarking results of EFS (executed on r5n.xlarge and t3.xlarge instances) and NFS (executed on a t3.xlarge instance). Benchmark source: rstudio/fsbench.\nApparently, one of the main reasons (according to discussions on the web) is that EFS is used by many people around the world and needs to split its available resources dynamically across all clients. There are ways to increase performance (e.g., by selecting a different mode, increasing the size of the file system or tweaking certain mount options). However, in the end EFS will always stay slower than NFS.\n Fig. 2: Parallel benchmarking results of EFS (executed on r5n.xlarge and t3.xlarge instances) and NFS (executed on an t3.xlarge instance). Benchmark source: rstudio/fsbench.\nWeâ€™ve also compared Azureâ€™s Azure Files NFS offering to AWS EFS using the fsbench tool from RStudio. At first we were optimistic giving the higher maximum values reported for the Azure NFS service. However, the benchmark results show a different picture and do not encourage moving existing RStudio k8s deployments from AWS to Azure.\n\nFig. 3: Sequential benchmarking results of AWS EFS (aws-mem = r5n.large) and Azure Files NFS (azure-gp = D2s_v3; azure-mem = E2bds_v5). Benchmark source: rstudio/fsbench.\nHaving compared NFS and EFS performance-wise now and given that file system decisions are usually deliberate ones, the question for us was: how much slower is it compared to NFS and do the upsides of EFS outweigh it? After trying it out for ourselves, we concluded yes, the advantages of EFS outweigh its decreased performance and we should use it as the file system for our RStudio k8s clusters. With EFS, we\n\ndonâ€™t need to worry about a single NFS server deployment as a potential bottleneck\ncan more easily make use of multiple availability zones\nhave a file system which dynamically adjusts to storage use (expansion and shrinking)\neventually improves (in speed) over time\n\nWe are aware that users coming from a k8s-NFS or on-premise installation will experience a reduced speed due to some increases in latency and especially when working with many small files. However, these will only be apparent for people coming from existing RSW installations and not for ones starting out on a new cluster. Especially users coming from an RSW on-premise installation will feel a substantial decrease in speed as in such an environment usually a single-instance RSW runs on an RWO file system (usually ext4) which is a completely different ballpark. FWIW, AWS has also improved EFS latency in February 2022, making it more performant for existing and new customers at no additional cost.\nAnother (important) point worth mentioning is that on AWS, bottlerocket node images (more on that in a separate blog post) do not have native support for NFS but work with EFS. This becomes of interest when considering running a cluster on bottlerocket images or if the cluster is already doing so."
  },
  {
    "objectID": "posts/2022-09-14-rstudio-efs-nfs/index.html#rstudio-k8s-clusters-on-efs---deployment",
    "href": "posts/2022-09-14-rstudio-efs-nfs/index.html#rstudio-k8s-clusters-on-efs---deployment",
    "title": "EFS vs.Â NFS for RStudio on Kubernetes (AWS): Configuration and considerations",
    "section": "RStudio k8s clusters on EFS - Deployment",
    "text": "RStudio k8s clusters on EFS - Deployment\nDuring our EFS evaluation period, we came across potential pitfalls and other RStudio peculiarities which system admins need to be aware of when aiming for a k8s RStudio cluster. These are described in the following.\nNote: This section makes a few assumptions:\n\nA k8s cluster on AWS\nFull cluster admin permissions\nRStudio Workbench and RStudio Connect â€œEnterpriseâ€ licenses\nThe use of terraform and the RStudio helm charts\n\nFirst, we would like to refer to the official implementation instructions for setting up RStudio applications on Kubernetes. While these do not necessarily target EFS, they contain a lot of information about NFS and other important points to be aware of.\n\nDeploying the EFS driver and storage class(es)\nBefore EFS can be used, an EFS driver and corresponding storage class must be deployed in the cluster. The latter is then referenced within the RStudio deployments. There is an official kubernetes-sigs helm chart available at https://github.com/kubernetes-sigs/aws-efs-csi-driver which one can use.\nresource \"helm_release\" \"aws-efs-csi-driver\" {\n  name             = \"aws-efs-csi-driver\"\n  chart            = \"aws-efs-csi-driver\"\n  repository       = \"https://kubernetes-sigs.github.io/aws-efs-csi-driver\"\n  version          = \"2.2.7\"\n  create_namespace = true\n  namespace        = \"aws-efs-csi-driver\"\n  count            = 1\n\n  values = [\n    \"${file(\"${path.module}/values/aws_efs_csi_values.yml\")}\"\n  ]\n\n  # rsc\n  set {\n    name  = \"storageClasses[0].parameters.fileSystemId\"\n    value = aws_efs_file_system.eks_efs.id\n  }\n\n  # rspm\n  set {\n    name  = \"storageClasses[2].parameters.fileSystemId\"\n    value = aws_efs_file_system.eks_efs.id\n  }\n}\nThe important part is the configuration of the chart values, which defines the storage classes. And yes, it is required to define multiple ones here, one for each RStudio service. The reason is that each service works different with respect to mount permissions of the respective application user (rstudio-server, rstudio-connect, rstudio-pm) and the way EFS is implemented (i.e., access points). There are two ways EFS can be used:\n\ndynamic provisioning\nstatic provisioning\n\nThe former is usually preferred. It means that persistent volumes can be created in a dynamic way, i.e.Â if a deployment asks for one, a new one gets (dynamically) created during the application deployment.\nStatic provisioning, on the other hand, means that one needs to create a persistent volume manually, referring to specific EFS mount points and in addition also configure its claim (PVC) (semi-)manually (â€œsemiâ€ because the PVC will find the PV it the names match and the chart offers to create it).\nNow EFS behaves like so that if dynamic provisioning is used, a single uid and gid must be set which then owns the mounted directories recursively. This way does not respect individual user permissions for the respective directory tree. This is something that is OK (and even needed) for RSC and RSPM but unacceptable for RSW. With this state in an RSW deployment, each user would have full permissions on the files of all other users. Hence, for RSW, static provisioning is required as this allows for a more fine-grained control. In the efs-sc-rsw storageClass definition below, this is reflected by\n\nnot setting a uid and gid value\nsetting the base path to / instead of a dynamically provisioned subpath\n\nNote that the basePath name for RSC and RSPM storage classes could be any name and \"/dynamic_provisioning\" is a subjective choice on our side.\nFor RSC and RSPM one also needs to supply the fileSystemId of the EFS file system to use. As this is done dynamically in our case, it is specified via the set{} notation.\n# only needed for dynamic provisioning - static provisioning must use a custom PV\nstorageClasses:\n  - name: efs-sc-rsc\n    reclaimPolicy: Retain\n    volumeBindingMode: Immediate\n    allowVolumeExpansion: true\n    mountOptions:\n      - tls\n    parameters:\n      provisioningMode: efs-ap\n      directoryPerms: \"700\"\n      uid: \"999\"\n      gid: \"999\"\n      basePath: \"/dynamic_provisioning\"\n  - name: efs-sc-rsw\n    reclaimPolicy: Retain\n    volumeBindingMode: Immediate\n    allowVolumeExpansion: true\n    mountOptions:\n      - tls\n    parameters:\n      basePath: \"/\"\n  - name: efs-sc-rspm\n    reclaimPolicy: Retain\n    volumeBindingMode: Immediate\n    allowVolumeExpansion: true\n    mountOptions:\n      - tls\n    parameters:\n      provisioningMode: efs-ap\n      directoryPerms: \"700\"\n      uid: \"99\"\n      gid: \"99\"\n      basePath: \"/dynamic_provisioning\"\n\n\nConfiguring RStudio helm charts\nNext, these storage classes need to be referenced in the respective RStudio deployments. As we are using dynamic provisioning for RSC and RSPM, we can let the helm chart create the PV and PVC. In the definition below, the create: and mount: fields only refer to the PVC. The PV will be provisioned dynamically in the background.\nsharedStorage:\n  create: true\n  mount: true\n  name: \"pv-rsc-shared\"\n  storageClassName: efs-sc-rsc\n  requests:\n    storage: \"20Gi\"\nFor RSW, more work is required. Here, the PV does not get provisioned dynamically and needs to be created manually via terraform. It helps to use the same name as the PVC, which is created by the helm chart, would search for. This also avoids having to create the PVC within terraform.\nNote that here data.aws_efs_file_system.efs.id is the same value which was being referenced in the aws-efs-csi-driver helm chart above. To retrieve it, one needs to filter it from all existing file system IDs listed which are present in your account. In our case we use a tag for filtering but the terraform module also lists alternative ways to filter the correct ID. To protect the PV, we also set prevent_destroy = true so that potential terraform applies would not allow this PV to get deleted.\nresource \"kubernetes_persistent_volume\" \"pv-rsw-home-efs\" {\n  metadata {\n    name = \"pv-rsw-home-efs\"\n  }\n  spec {\n    capacity = {\n      storage = \"50Gi\"\n    }\n    access_modes       = [\"ReadWriteMany\"]\n    storage_class_name = \"efs-sc-rsw\"\n    persistent_volume_source {\n      csi {\n        driver        = \"efs.csi.aws.com\"\n        volume_handle = data.aws_efs_file_system.efs.id # filesystem ID\n      }\n    }\n  }\n\n  lifecycle {\n    prevent_destroy = true\n  }\n}\n\ndata \"aws_efs_file_system\" \"efs\" {\n  tags = {\n    Env = \"prod\"\n  }\n}\nIn the RSW helm chart, one would then define the following\nhomeStorage:\n  create: true\n  mount: true\n  name: \"pvc-rsw-home-efs\"\n  storageClassName: \"efs-sc-rsw\"\n  requests:\n    storage: \"50Gi\"\nIn contrast to RSC and RSPM, RSW comes with two persistent volumes: home storage and shared storage. Due to the nature of EFS, the RSW deployment will complain about missing permissions for the default location under /var/lib/ - see also this GitHub issue. To bypass this issue, one can change the default mount location of the shared storage and set it to the EFS home mount under /home to make it persistent.\nconfig:\n  server:\n    rserver.conf:\n      server-shared-storage-path: /home/rstudio-shared-storage\nThis should be it! With this setup, individual user permissions are honored within /home and RSW should be running on EFS. When inspecting /home in more detail, you will notice that all the other base_path mounts from the dynamic EFS provisioning are also there. But donâ€™t worry, RSW users wonâ€™t have permission to look into them. You can just safely ignore it. If you still donâ€™t like it, you need to create a dedicated EFS file system ID which is only used for RSW and does not contain additional access points used by the dynamic storage class provisioner."
  },
  {
    "objectID": "posts/2022-09-14-rstudio-efs-nfs/index.html#improving-rsw-performance-on-efs-bottlerocket-images-and-instance-type-choice",
    "href": "posts/2022-09-14-rstudio-efs-nfs/index.html#improving-rsw-performance-on-efs-bottlerocket-images-and-instance-type-choice",
    "title": "EFS vs.Â NFS for RStudio on Kubernetes (AWS): Configuration and considerations",
    "section": "Improving RSW performance on EFS: bottlerocket images and instance type choice",
    "text": "Improving RSW performance on EFS: bottlerocket images and instance type choice\nAfter having a running deployment of RSW on EFS, one can optionally make use of bottlerocket images to substantially decrease the session startup times on freshly provisioned nodes (i.e., after an autoscaler scale-up). We will cover this topic in more detail in another post."
  },
  {
    "objectID": "posts/2024-07-27-llama3.1/index.html",
    "href": "posts/2024-07-27-llama3.1/index.html",
    "title": "Playing with Llama 3.1 in R",
    "section": "",
    "text": "Meta recently announced Llama 3.1, and thereâ€™s a lot of excitement. I finally had some time to experiment with locally run open-source models. The small 8B model, in particular, produces surprisingly useful output, with reasonable speed. Getting started is straightforward."
  },
  {
    "objectID": "posts/2024-07-27-llama3.1/index.html#running-llama-3.1-locally",
    "href": "posts/2024-07-27-llama3.1/index.html#running-llama-3.1-locally",
    "title": "Playing with Llama 3.1 in R",
    "section": "Running Llama 3.1 Locally",
    "text": "Running Llama 3.1 Locally\nFirst, youâ€™ll need to install Ollama, which you can download from here.\nNext, open your terminal and run:\nollama run llama3.1:8b\nThis command will pull and run the smallest Llama 3.1 model, which operates at a reasonable speed even on a MacBook Air. To exit, type /bye.\nYou can also directly provide a prompt in the terminal:\nollama run llama3.1:8b \"Tomorrow is a...\""
  },
  {
    "objectID": "posts/2024-07-27-llama3.1/index.html#using-llama-3.1-from-r",
    "href": "posts/2024-07-27-llama3.1/index.html#using-llama-3.1-from-r",
    "title": "Playing with Llama 3.1 in R",
    "section": "Using Llama 3.1 from R",
    "text": "Using Llama 3.1 from R\nHause Lin has created a lovely R wrapper for Ollama, allowing you to use Llama 3.1 within your R scripts. To install the wrapper, use:\ndevtools::install_github(\"hauselin/ollamar\")\nNow, you can use it as follows:\nlibrary(ollamar)\n\ngenerate(\"llama3.1\", \"Tomorrow is a...\", output = \"text\")"
  },
  {
    "objectID": "posts/2024-07-27-llama3.1/index.html#applications",
    "href": "posts/2024-07-27-llama3.1/index.html#applications",
    "title": "Playing with Llama 3.1 in R",
    "section": "Applications",
    "text": "Applications\nRunning this locally without privacy concerns opens up a so many possibilities. For example, if you want to get a short summary of all the README files in your Git folder, you can do something like this:\nlibrary(fs)\nlibrary(tidyverse)\nlibrary(ollamar)\n\nfiles &lt;-\n  fs::dir_ls(\"~/git\", recurse = TRUE, glob = \"*.md\") |&gt;\n  head(4)\n\nsummarize_md &lt;- function(file) {\n  generate(\n    \"llama3.1\",\n    paste(\n      \"Summarize in 3 bullet points, \",\n      \"use a descriptive title, \",\n      \"avoid sentences like 'this is a summary...', or 'Here are the 3 bullet points...'.\",\n      paste(readLines(file), collapse = \"\\n\")\n    ),\n    output = \"text\"\n  )\n}\n\nans &lt;-\n  tibble(file = files) |&gt;\n  mutate(summary = map_chr(file, summarize_md))\n\nans\n#&gt; # A tibble: 5 Ã— 2\n#&gt;   file                                                   summary\n#&gt;   &lt;fs::path&gt;                                             &lt;chr&gt;\n#&gt; 1 /Users/christophsax/git/adminr/201909_slides/README.md \"**Autumn Meetup Highlâ€¦\n#&gt; 2 /Users/christophsax/git/adminr/202103_slides/README.md \"**Key Takeaways from â€¦\n#&gt; 3 /Users/christophsax/git/adminr/202205_slides/README.md \"**Spring Meetup 2022 â€¦\n#&gt; 4 /Users/christophsax/git/adminr/202212_slides/README.md \"**Key Takeaways from â€¦\nWith this setup, you can quickly generate summaries for README files, or any other text documents, directly within R. Happy experimenting!"
  },
  {
    "objectID": "posts/2021-10-07-old-texlive/index.html",
    "href": "posts/2021-10-07-old-texlive/index.html",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "",
    "text": "Rendering PDFs with rmarkdown requires a working LaTeX installation, such as tinytex. Occasionally, existing workflows break with the newest version of LaTeX. This post describes how to run an older LaTeX version for just a little while.\nRendering PDFs with rmarkdown requires a working LaTeX installation. The excellent tinytex package helps installing a portable variant of the TeXlive distribution with minimal fuss, on any major operating system. It is really as simple as:"
  },
  {
    "objectID": "posts/2021-10-07-old-texlive/index.html#breaking-changes-in-latex",
    "href": "posts/2021-10-07-old-texlive/index.html#breaking-changes-in-latex",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "Breaking changes in LaTeX?",
    "text": "Breaking changes in LaTeX?\nDespite its age, LaTeX is still a lively maintained and evolving system, with the consequence that some documents that worked on an older version of LaTeX may no longer work with the most recent stack. One such example is the tabu package that breaks for some use cases with TeXlive 2021.\nOn a clean tinytex installation, rendering an .rmd document with the following code starts with installing missing packages:\nknitr::kable(mtcars, booktabs = TRUE, longtable = TRUE) |&gt;\n  kableExtra::kable_styling(full_width = TRUE)\ntlmgr: package repository https://mirror.foobar.to/CTAN/systems/texlive/tlnet (not verified: gpg unavailable)\n[1/1, ??:??/??:??] install: multirow [3k]\nrunning mktexlsr ...\ndone running mktexlsr.\ntlmgr: package log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr.log\ntlmgr: command log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr-commands.log\ntlmgr: package repository https://mirror.foobar.to/CTAN/systems/texlive/tlnet (not verified: gpg unavailable)\n[1/1, ??:??/??:??] install: wrapfig [10k]\nrunning mktexlsr ...\n...\nYes, missing packages are installed on the fly! A full LaTeX distribution has several GB in size. The lazy package installation is a cool feature of the tinytex distribution that allows starting quickly with a minimal installation, without downloading and unpacking the whole thing.\nHowever, compilation gives the following error message:\ntlmgr: package log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr.log\ntlmgr: command log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr-commands.log\n! Dimension too large.\n\\LT@max@sel #1#2-&gt;{\\ifdim #2=\\wd \\tw@\n                                      #1\\else \\number \\c@LT@chunks \\fi }{\\th...\nl.317 \\end{longtabu}\n\nError: LaTeX failed to compile test.tex. See https://yihui.org/tinytex/r/#debugging for debugging tips. See test.log for more info."
  },
  {
    "objectID": "posts/2021-10-07-old-texlive/index.html#what-if-we-switch-to-the-previous-version",
    "href": "posts/2021-10-07-old-texlive/index.html#what-if-we-switch-to-the-previous-version",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "What if we switch to the previous version?",
    "text": "What if we switch to the previous version?\nâ€œBut it used to work yesterday!â€ Fine, letâ€™s install the last version that is still based on TeXlive 2020:\ntinytex::install_tinytex(version = \"2021.03\")\nUnfortunately, this breaks the package downloader:\nA new version of TeX Live has been released. If you need to install or update any LaTeX packages, you have to upgrade TinyTeX with tinytex::reinstall_tinytex(). If it fails to upgrade, you might be using a default random CTAN mirror that has not been fully synced to the main CTAN repository, and you need to wait for a few more days or use a CTAN mirror that is known to be up-to-date (see the \"repository\" argument on the help page ?tinytex::install_tinytex).\n\ntlmgr: Local TeX Live (2020) is older than remote repository (2021).\nCross release updates are only supported with\n  update-tlmgr-latest(.sh/.exe) --update\nSee https://tug.org/texlive/upgrade.html for details.\n! LaTeX Error: File `multirow.sty' not found.\n...\nAnd weâ€™re told to upgrade TeXlive, which isnâ€™t helpful in our particular use case."
  },
  {
    "objectID": "posts/2021-10-07-old-texlive/index.html#can-we-make-the-previous-version-work",
    "href": "posts/2021-10-07-old-texlive/index.html#can-we-make-the-previous-version-work",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "Can we make the previous version work?",
    "text": "Can we make the previous version work?\nYes. The solution is to teach tinytex to make do with a historic snapshot of the TeXlive distribution:\ntinytex::tlmgr(\"option repository https://ftp.tu-chemnitz.de/pub/tug/historic/systems/texlive/2020/tlnet-final\")\ntlmgr: setting default package repository to https://ftp.tu-chemnitz.de/pub/tug/historic/systems/texlive/2020/tlnet-final\ntlmgr: updating ~/Library/TinyTeX/tlpkg/texlive.tlpdb\nAfter that, automatic package downloads work again, with a prominent message reminding you that youâ€™re running on a frozen snapshot:\nknitr::kable(mtcars, booktabs = TRUE, longtable = TRUE) |&gt;\n  kableExtra::kable_styling(full_width = TRUE)\nTeX Live 2020 is frozen and will no longer\nbe routinely updated. This happens in preparation for a new release.\n\nIf you're willing to help with pretesting the new release, and we hope\nyou are, (when pretests are available), please read\nhttps://tug.org/texlive/pretest.html.\n\nOtherwise, just wait, and the new release will be ready in due time.\nTeX Live 2020 is frozen and will no longer\nbe routinely updated. This happens in preparation for a new release.\n\nIf you're willing to help with pretesting the new release, and we hope\nyou are, (when pretests are available), please read\nhttps://tug.org/texlive/pretest.html.\n\nOtherwise, just wait, and the new release will be ready in due time.\ntlmgr: package repository https://ftp.tu-chemnitz.de/pub/tug/historic/systems/texlive/2020/tlnet-final (not verified: gpg unavailable)\n[1/1, ??:??/??:??] install: multirow [3k]\nrunning mktexlsr ...\ndone running mktexlsr.\ntlmgr: package log updated: /Users/kirill/Library/TinyTeX/texmf-var/web2c/tlmgr.log\nTeX Live 2020 is frozen and will no longer\nbe routinely updated. This happens in preparation for a new release.\n...\nThe document is now rendered without errors.\nThe tinytex maintainers have confirmed that old bundles remain available for download. Of course the correct solution is to avoid weakly maintained dependencies in your code, and to replace them by better solutions. In reality, this is not always feasible, and breakages may occur without notice.\nHappy freezing!"
  },
  {
    "objectID": "posts/2021-10-07-old-texlive/index.html#details",
    "href": "posts/2021-10-07-old-texlive/index.html#details",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "Details",
    "text": "Details\nOn the TeXlive homepage, the â€œHow to acquire TeX Live: other methodsâ€ link has a section â€œPast releasesâ€. The tlmgr command is shown right there. The â€œHistoric archiveâ€ page contains a list of historic mirrors, use a mirror close to you.\nI originally started my search at the â€œHistoric archiveâ€ page. It took some time to find the correct command, it is also documented in the tlmgr manual.\nThe tinytex package provides the tlmgr_repo() function as a shortcut to set the repository, but it seems to fail currently for this use case. Thanks Christophe Dervieux and Florian Kohrt for the hints!"
  },
  {
    "objectID": "posts/2021-03-09-seasonal-1.8/index.html",
    "href": "posts/2021-03-09-seasonal-1.8/index.html",
    "title": "Seasonal Adjustment of Multiple Series",
    "section": "",
    "text": "seasonal is an easy-to-use and full-featured R-interface to X-13ARIMA-SEATS, the seasonal adjustment software developed by the United States Census Bureau. The latest CRAN version of seasonal makes it much easier to adjust multiple time series.\n\nPhoto by MeriÃ§ DaÄŸlÄ±\n\nseasonal depends on the x13binary package to access pre-built binaries of X-13ARIMA-SEATS on all platforms and does not require any manual installation. To install both packages:\ninstall.packages(\"seasonal\")\nseas is the core function of the seasonal package. By default, seas calls the automatic procedures of X-13ARIMA-SEATS to perform a seasonal adjustment that works well in most circumstances:\nseas(AirPassengers)\nFor a more detailed introduction, read our article in the Journal of Statistical Software.\nMultiple Series Adjusmtent\nIn the latest CRAN version 1.8, it is now possible to seasonally adjust multiple series in a single call to seas(). This is done by using the built-in batch mode of X-13. It removes the need for loops or lapply() in such cases and finally brings one missing feature of X-13 to seasonal â€“ the composite spec.\nMultiple adjustments can be performed by supplying multiple time series as an \"mts\" object:\n\nlibrary(seasonal)\nm &lt;- seas(cbind(fdeaths, mdeaths), x11 = \"\")\nfinal(m)\n\n          fdeaths  mdeaths\nJan 1974 614.1235 1598.740\nFeb 1974 542.3500 1492.127\nMar 1974 613.5029 1443.238\nApr 1974 591.5725 1694.643\nMay 1974 607.4970 1696.021\nJun 1974 543.8415 1558.886\nJul 1974 597.0745 1663.176\nAug 1974 587.0533 1623.498\nSep 1974 588.2693 1741.394\nOct 1974 735.6666 1735.516\nNov 1974 602.0218 1665.590\nDec 1974 496.3985 1394.097\nJan 1975 564.0055 1560.605\nFeb 1975 591.0320 1708.763\nMar 1975 585.7739 1652.994\nApr 1975 581.5294 1671.265\nMay 1975 537.8055 1588.605\nJun 1975 584.3284 1600.979\nJul 1975 566.4872 1541.099\nAug 1975 617.1197 1623.445\nSep 1975 516.4781 1521.497\nOct 1975 559.0481 1577.200\nNov 1975 561.6315 1602.659\nDec 1975 580.9778 1569.692\nJan 1976 519.8106 1477.855\nFeb 1976 882.3725 2180.616\nMar 1976 674.5057 1744.114\nApr 1976 467.4502 1366.628\nMay 1976 509.7854 1344.809\nJun 1976 553.5233 1434.662\nJul 1976 503.2795 1447.952\nAug 1976 494.2373 1383.932\nSep 1976 529.1840 1453.496\nOct 1976 570.4128 1435.912\nNov 1976 590.4285 1540.551\nDec 1976 587.0971 1572.631\nJan 1977 583.2427 1607.153\nFeb 1977 498.9514 1287.403\nMar 1977 500.4632 1306.324\nApr 1977 569.2076 1685.581\nMay 1977 565.6470 1405.231\nJun 1977 509.3196 1432.968\nJul 1977 548.4062 1414.216\nAug 1977 523.6985 1444.945\nSep 1977 563.3014 1402.720\nOct 1977 495.6653 1427.458\nNov 1977 453.9859 1307.828\nDec 1977 502.3045 1268.618\nJan 1978 535.2658 1415.724\nFeb 1978 633.7605 1790.002\nMar 1978 559.2936 1469.883\nApr 1978 485.5062 1343.715\nMay 1978 590.4080 1509.166\nJun 1978 574.4467 1464.288\nJul 1978 571.2263 1428.398\nAug 1978 542.3579 1424.622\nSep 1978 551.2099 1422.428\nOct 1978 557.6905 1399.404\nNov 1978 479.8979 1199.762\nDec 1978 550.4253 1397.023\nJan 1979 548.9834 1557.853\nFeb 1979 576.9922 1425.717\nMar 1979 549.3468 1393.788\nApr 1979 546.4590 1449.784\nMay 1979 525.9881 1359.843\nJun 1979 550.2481 1330.113\nJul 1979 533.7558 1373.156\nAug 1979 566.6884 1381.653\nSep 1979 552.6500 1377.175\nOct 1979 533.9571 1337.640\nNov 1979 557.9707 1414.101\nDec 1979 475.5049 1038.506\n\n\nThis will perform two seasonal adjustments, one for fdeaths and one for mdeaths. X-13 spec-argument combinations can be applied in the usual way, such as x11 = \"\". Note that if entered that way, they will apply to both series. The vignette on multiple adjustments describes how to specify options for individual series.\nBackend\nX-13 ships with a batch mode that allows multiple adjustments in a single call to X-13. This is now the default in seasonal (multimode = \"x13\"). Alternatively, X-13 can be called for each series (multimode = \"R\"). The results should be usually the same, but switching to multimode = \"R\" may be useful for debugging:\n\nseas(cbind(fdeaths, mdeaths), multimode = \"x13\")\n\n$fdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"x13\")\n\nCoefficients:\n      Constant      AO1976.Feb  MA-Seasonal-12  \n      -0.01578         0.43345         0.63119  \n\n\n$mdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"x13\")\n\nCoefficients:\n       AO1976.Feb         LS1976.Apr         AO1977.Apr         AO1978.Feb  \n           0.3319            -0.1330             0.1957             0.2305  \n       AO1979.Dec  MA-Nonseasonal-01     MA-Seasonal-12  \n          -0.3149            -0.3854             0.6120  \n\n\n$call\nseas(x = cbind(fdeaths, mdeaths), multimode = \"x13\")\n\nattr(,\"class\")\n[1] \"seas_multi\" \"list\"      \n\nseas(cbind(fdeaths, mdeaths), multimode = \"R\")\n\n$fdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"R\")\n\nCoefficients:\n      Constant      AO1976.Feb  MA-Seasonal-12  \n      -0.01578         0.43345         0.63119  \n\n\n$mdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"R\")\n\nCoefficients:\n       AO1976.Feb         LS1976.Apr         AO1977.Apr         AO1978.Feb  \n           0.3319            -0.1330             0.1957             0.2305  \n       AO1979.Dec  MA-Nonseasonal-01     MA-Seasonal-12  \n          -0.3149            -0.3854             0.6120  \n\n\n$call\nseas(x = cbind(fdeaths, mdeaths), multimode = \"R\")\n\nattr(,\"class\")\n[1] \"seas_multi\" \"list\"      \n\n\nIn general, multimode = \"x13\" is faster. The following comparison on a MacBook Pro shows a modest speed gain, but bigger differences have been observed on other systems:\nmany &lt;- rep(list(fdeaths), 100)\nsystem.time(seas(many, multimode = \"x13\"))\n#   user  system elapsed\n#  9.415   0.653  10.079\nsystem.time(seas(many, multimode = \"R\"))\n#   user  system elapsed\n# 11.130   1.039  12.324\ncomposite spec\nSupport for the X-13 batch mode makes it finally possible to use the composite spec â€“ the one feature of X-13 that was missing in seasonal. Sometimes, one has to decide whether seasonal adjustment should be performed on a granular level or on an aggregated level. The composite spec helps you to analyze the problem and to compare the direct and the indirect adjustments.\nThe composite argument is a list with an X-13 specification that is applied on the aggregated series. Specification works identically for other series in seas(), including the application of the defaults. If you provide an empty list, the usual defaults of seas() are used. A minimal composite call looks like this:\n\nseas(\n  cbind(mdeaths, fdeaths),\n  composite = list(),\n  series.comptype = \"add\"\n)\n\n$mdeaths\n\nCall:\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nCoefficients:\n       AO1976.Feb         LS1976.Apr         AO1977.Apr         AO1978.Feb  \n           0.3319            -0.1330             0.1957             0.2305  \n       AO1979.Dec  MA-Nonseasonal-01     MA-Seasonal-12  \n          -0.3149            -0.3854             0.6120  \n\n\n$fdeaths\n\nCall:\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nCoefficients:\n      Constant      AO1976.Feb  MA-Seasonal-12  \n      -0.01578         0.43345         0.63119  \n\n\n$composite\n\nCall:\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nCoefficients:\n         Constant         AO1976.Feb  MA-Nonseasonal-01     MA-Seasonal-12  \n         -0.03133            0.31247           -0.43509            0.99937  \n\n\n$call\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nattr(,\"class\")\n[1] \"seas_multi\" \"list\"      \n\n\nYou can verify that the composite refers to the total of mdeaths and fdeaths by running:\n\nseas(ldeaths)\n\n\nCall:\nseas(x = ldeaths)\n\nCoefficients:\n         Constant         AO1976.Feb  MA-Nonseasonal-01     MA-Seasonal-12  \n         -0.03133            0.31247           -0.43509            0.99937  \n\n\nwhere ldeaths is the sum of mdeaths and fdeaths.\nAcknowledgement\nMany thanks to Severin ThÃ¶ni and Matthias Bannert, for demonstrating the benefits of the X-13 batch mode. Also to the ETH KOF, for partially funding this development."
  },
  {
    "objectID": "posts/2021-07-09-certified-partner-anniversary/index.html",
    "href": "posts/2021-07-09-certified-partner-anniversary/index.html",
    "title": "Celebrating one-year anniversary as RStudio Full Service Certified Partner",
    "section": "",
    "text": "cynkra celebrates its first anniversary as an RStudio Full Service Certified Partner! Every day, we help our clients set up professional IT infrastructures using RStudio products and license compositions suited to their individual needs. In the context of our anniversary, we would like to discuss the RStudio suite of products and our approach of combining RStudioâ€™s products with our Managed Workbench Solution to make RStudio Professional products even more powerful.\n\n\n\nPhoto by Adi Goldstein\n\n\n\nPhilosophy and lineup\nWe believe that good tools combined with smart settings can make data science teams enjoy their work more, resulting in higher productivity levels. We, therefore, work to provide our clients with customized RStudio product solutions to create a data science environment that would support their daily work most efficiently and effectively.\nOur team members, who have all been using practically all RStudio products daily, are testimony to our claims.\n\n\nFinding the right setup\nBesides finding the optimal combination of different software provided by RStudio, we customize the RStudio products to include smart defaults and individualized configurations such as local package repositories or preconfigured database setups. We use a full-fledged configured and centralized managed RStudio Workbench installation that simplifies administrative tasks for system admins and users. In addition, we ensure constant product and security updates, taking away all the maintenance burden and leaving users with a happy place for their data science tasks. We also train the team on how to work with RStudio Products.\nThe transition from the local RStudio Desktop usage is seamless since RStudio Workbench and friends provide all features of the free version.\nThere is no lock-in; everything that works on the free RStudio Desktop version will also work on RStudio Workbench and the other way round â€“ even if you decide to go back to the free version at some point. Of course, once clients have experienced the added benefits of RStudio Workbench, they rarely want to go back.\nOur lineup consists of the following RStudio products: RStudio Workbench (previously RStudio Server Pro), RStudio Connect, RStudio Package Manager, Shiny Server, and RStudio Team.\n\n\nSmart defaults and optimized configurations\nRStudio Workbench, RStudio Connect, and RStudio Package Manager can work together very well if configured correctly. We configure the settings as needed to enable users to experience the full power of the RStudio product suite. Our product saves you from having to go through the entire admin guide where there is always the potential to miss something important. The power (and complexity) of RStudio products resides in their configuration. We have experience with almost all possible configuration scenarios, including configuring RStudio products in highly secured enterprise environments.\n\n\nDockerized environment\nOur products come in a containerized environment, making them agnostic to any underlying operating system. Your users will have access to an Ubuntu LTS system (20.04 at the time of writing) which provides stable support for linking R packages against required system libraries.\nUsing a containerized environment also simplifies update tasks (for the client and us) due to the independence of the underlying operating system. All of this also holds if the underlying operating system of your company is subject to change in the future â€“ and if this happens, the RStudio environment will stay the same.\nAnother common pain point for local RStudio users is the use of LaTeX and pandoc when compiling PDF reports. By relying on the stable Ubuntu LTS environment as the base layer we can provide a stable LaTeX/pandoc environment that takes away almost all troubles for users in this area.\n\n\nPerformance\nCentralized, server-based installations can be very efficient both in speed and battery consumption. It applies to all RStudio products but in particular to RStudio Workbench and RStudio Package Manager.\nThe following GIF might give you an idea of speed improvement (we did the test using a relatively new MacBook Pro 2020). When we deployed the cloud-based RStudio Workbench on the MacBook, the machine performed much faster. The difference in speed will be even more noticeable in an older machine.\n\n\n\nRStudio Desktop vs.Â RStudio Workbench speed comparison\n\n\nBattery usage becomes critical when working from home or on the go. Using RStudio Workbench instead of RStudio Desktop will save battery life because all computation is done on a remote machine. RStudio Desktop, in particular, is quite energy-hungry; we have experienced battery life improvements of 30% and more in our daily work when using the cloud-based RStudio Workbench instead of RStudio Desktop.\n\n\nCustom real-world examples\nOne of the key strengths of our offering is individualization. To provide more details on this, we put together some configurations we implemented recently to give an idea of what individualized setups can look like in practice.\n\n\n\n\n\n\nSetupÂ 1\n\n\nSetupÂ 2\n\n\nSetupÂ 3\n\n\nSetupÂ 4\n\n\n\n\n\n\nRStudio Workbench\n\n\nâœ“\n\n\nâœ“\n\n\n\n\nâœ“\n\n\n\n\nRStudio Connect\n\n\nâœ“\n\n\nâœ“\n\n\nâœ“\n\n\nâœ“\n\n\n\n\nRStudio Package Manager\n\n\nâœ“\n\n\nâœ“\n\n\n\n\n\n\n\n\nRStudio Server Open Source\n\n\nâœ“\n\n\n\n\nâœ“\n\n\n\n\n\n\nShiny Server\n\n\n\n\n\n\n\n\nâœ“\n\n\n\n\nUser Provisioning\n\n\nActive Directory\n\n\nLDAP\n\n\nLocal Users\n\n\nActive Directory\n\n\n\n\nSSO Type\n\n\nPAM\n\n\nOIDC\n\n\n\n\nSAML\n\n\n\n\nDB Drivers\n\n\nRStudio Professional Drivers\n\n\nMS SQL\n\n\nOracle\n\n\nIBM DB2\n\n\n\n\nAcronym dictionary for Table:\n\nSSO: Single-Sign-On\nOIDC: OpenID Connect\nPAM: Pluggable Authentication Modules\n\nOften, in larger organizations, the system that is already in use determines the authentication settings. We have experience configuring RStudio Products with various authentication solutions.\n\n\nStandalone licenses and R training\nBesides the complete package that comes with our Managed Workbench offering, we are happy to provide our customers with standalone RStudio licenses. The advantage for Swiss-based customers is that they get a bill in Swiss Francs including VAT, and avoid currency conversion issues with USD when ordering from RStudio directly.\nIn addition to setting up R-related infrastructure, we also provide R training of any kind so that your team can make full use of the available software stack. You can visit our consulting offering on our website for more information."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog posts",
    "section": "",
    "text": "Playing with AI Agents in R\n\n\n6 min\n\n\n\nR\n\n\nLLM\n\n\n\n\n\n\n\nChristoph Sax\n\n\nFeb 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseasonal 1.10: R-interface to X-13ARIMA-SEATS\n\n\n2 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nOct 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing blockr: a no-code dashboard builder for R\n\n\n14 min\n\n\n\nR\n\n\n\n\n\n\n\nDavid Granjon\n\n\nSep 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2024 road and para-cycling road world championships: preliminaRy analysis\n\n\n19 min\n\n\n\nsport\n\n\nR\n\n\n\n\n\n\n\nDavid Granjon\n\n\nSep 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with Llama 3.1 in R\n\n\n3 min\n\n\n\nLLM\n\n\n\n\n\n\n\nChristoph Sax\n\n\nJul 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshinyMobile 2.0.0: a preview\n\n\n66 min\n\n\n\nshiny\n\n\n\n\n\n\n\nVeerle van Leemput and David Granjon\n\n\nMay 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEFS vs.Â NFS for RStudio on Kubernetes (AWS): Configuration and considerations\n\n\n18 min\n\n\n\nDevOps\n\n\nPosit\n\n\nfile system\n\n\nperformance\n\n\n\n\n\n\n\nPatrick Schratz\n\n\nSep 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccessing Googleâ€™s API via OAuth2\n\n\n9 min\n\n\n\napi\n\n\ngoogle\n\n\noauth2\n\n\n\n\n\n\n\nPatrick Schratz\n\n\nMay 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseasonal 1.9: Accessing composite output\n\n\n2 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nApr 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Season of Docs with R: useR! Information Board\n\n\n6 min\n\n\n\nR\n\n\ngsod\n\n\ndashboard\n\n\n\n\n\n\n\nBen Ubah\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning old versions of TeXlive with tinytex\n\n\n5 min\n\n\n\nR\n\n\n\n\n\n\n\nKirill MÃ¼ller\n\n\nOct 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox 0.3.1: extended functionality\n\n\n5 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nSep 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCelebrating one-year anniversary as RStudio Full Service Certified Partner\n\n\n8 min\n\n\n\ncertified partner\n\n\nPosit\n\n\nDocker\n\n\n\n\n\n\n\nCosima Meyer, Patrick Schratz\n\n\nJul 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeprecating a pkgdown site served via GitHub Pages\n\n\n4 min\n\n\n\nR\n\n\nGitHub\n\n\n\n\n\n\n\nPatrick Schratz, Kirill MÃ¼ller\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\ngfortran support for R on macOS\n\n\n5 min\n\n\n\nR\n\n\nmacOS\n\n\n\n\n\n\n\nPatrick Schratz\n\n\nMar 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonal Adjustment of Multiple Series\n\n\n3 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nMar 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic build matrix in GitHub Actions\n\n\n8 min\n\n\n\nR\n\n\nGitHub\n\n\n\n\n\n\n\nKirill MÃ¼ller\n\n\nDec 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up a load-balanced Jitsi Meet instance\n\n\n4 min\n\n\n\nDevOps\n\n\n\n\n\n\n\nPatrick Schratz\n\n\nNov 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaintaining multiple identities with Git\n\n\n2 min\n\n\n\nGit\n\n\n\n\n\n\n\nKirill MÃ¼ller\n\n\nAug 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelational data models in R\n\n\n12 min\n\n\n\nR\n\n\ndm\n\n\ndatabases\n\n\n\n\n\n\n\nAngel Dâ€™az, Kirill MÃ¼ller\n\n\nApr 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntempdisagg: converting quarterly time series to daily\n\n\n4 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox 0.2: supporting additional time series classes\n\n\n4 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nAug 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing dm: easy juggling of tables and relations\n\n\n7 min\n\n\n\nR\n\n\ndm\n\n\ndatabases\n\n\n\n\n\n\n\nBalthasar Sager\n\n\nJul 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox 0.1: class-agnostic time series\n\n\n3 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nApr 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTime series of the world, unite!\n\n\n3 min\n\n\n\nR\n\n\ntime-series\n\n\n\n\n\n\n\nChristoph Sax\n\n\nMay 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nDone â€œEstablishing DBIâ€!?\n\n\n15 min\n\n\n\nR\n\n\ndatabases\n\n\n\n\n\n\n\nKirill MÃ¼ller\n\n\nMay 1, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-09-16-blockr/index.html",
    "href": "posts/2024-09-16-blockr/index.html",
    "title": "Introducing blockr: a no-code dashboard builder for R",
    "section": "",
    "text": "Since 2023, BristolMyersSquibb, the Y company and cynkra have teamed up to develop a novel no-code solution for R.\nCodelibrary(blockr)\n\n\nAttaching package: 'blockr'\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nCodelibrary(pracma)"
  },
  {
    "objectID": "posts/2024-09-16-blockr/index.html#introduction",
    "href": "posts/2024-09-16-blockr/index.html#introduction",
    "title": "Introducing blockr: a no-code dashboard builder for R",
    "section": "Introduction",
    "text": "Introduction\nblockr is an R package designed to democratize data analysis by providing a flexible, intuitive, and code-free approach to building data pipelines. It has 2 main user targets:\n\nOn the one hand, it empowers non technical users to create insightful data workflows using pre-built blocks that can be easily connected, all without writing a single line of code.\nOn the other hand, it provides developers with a set of tools to seamlessly create new blocks, thereby enhancing the entire framework and fostering collaboration within organizations teams.\n\nblockr is data agnostic, meaning it can work with any kind of dataset, that is pharmaceutical data or sport analytics data. It builds on top of shiny to ensure real time feedback to any data change. Finally, it allows to export code to create reproducible data analysis."
  },
  {
    "objectID": "posts/2024-09-16-blockr/index.html#getting-started",
    "href": "posts/2024-09-16-blockr/index.html#getting-started",
    "title": "Introducing blockr: a no-code dashboard builder for R",
    "section": "Getting started",
    "text": "Getting started\nAs a simple user\nAs a simple user, youâ€™re not expected to write any single line of code to use blockr. You can use the below kitchen sink to get started. This example is based on the palmer penguins data and running a single stack with 3 blocks: the first block to select the data, another one to create the plot and then add the points to it.\nblockr has a its own validation system. For instance, using the below example, you can try to press return on the first block select box (penguins is the selected default). Youâ€™ll notice an immediate feedback message. A global message is displayed in the block upper middle part: â€œ1 error(s) found in this blockâ€. You get more detailed mesages next to the faulty input(s): â€œselected value(s) not among provided choicesâ€. You can repeat the same experience with the last plot layer block, by emptying the color and shape select inputs. Error messages can accumulate.\nYou can dynamically add blocks to a current stack, that gathers a set of related blocks. You can think a stack as a data analysis recipe as in cooking, where blocks are instructions. To add a new block, you can click on the + icon on the stack top right corner. This opens a sidebar on the left side, where one may search for blocks that are compatible with the current state of the pipeline. With an empty stack, only entry point blocks are suggested, so you can import data. Then, after clicking on the block, the suggestion list changes so you can, for instance, filter data or select only a subset of columns, and much more.\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nwebr::install(\"blockr\", repos = c(\"https://bristolmyerssquibb.github.io/webr-repos\", \"https://repo.r-wasm.org\"))\n\nlibrary(blockr)\nlibrary(palmerpenguins)\nlibrary(ggplot2)\n\nnew_ggplot_block &lt;- function(col_x = character(), col_y = character(), ...) {\n\n  data_cols &lt;- function(data) colnames(data)\n\n  new_block(\n    fields = list(\n      x = new_select_field(col_x, data_cols, type = \"name\"),\n      y = new_select_field(col_y, data_cols, type = \"name\")\n    ),\n    expr = quote(\n      ggplot(mapping = aes(x = .(x), y = .(y)))\n    ),\n    class = c(\"ggplot_block\", \"plot_block\"),\n    ...\n  )\n}\n\nnew_geompoint_block &lt;- function(color = character(), shape = character(), ...) {\n\n  data_cols &lt;- function(data) colnames(data$data)\n\n  new_block(\n    fields = list(\n      color = new_select_field(color, data_cols, type = \"name\"),\n      shape = new_select_field(shape, data_cols, type = \"name\")\n    ),\n    expr = quote(\n      geom_point(aes(color = .(color), shape = .(shape)), size = 2)\n    ),\n    class = c(\"geompoint_block\", \"plot_layer_block\", \"plot_block\"),\n    ...\n  )\n}\n\nstack &lt;- new_stack(\n  data_block = new_dataset_block(\"penguins\", \"palmerpenguins\"),\n  plot_block = new_ggplot_block(\"flipper_length_mm\", \"body_mass_g\"),\n  layer_block = new_geompoint_block(\"species\", \"species\")\n)\nserve_stack(stack)\n\nToward more complex analysis\nLetâ€™s consider this dataset, which contains 120 years of olympics athletes data until Rio in 2016. In the below kitchen sink, we first add an upload block:\n\nDownload the dataset file locally.\nCLick on Add stack.\nClick on the stack + button and search for browser, then select the new_filesbrowser_block.\nUncollapse the stack by click on the top right arrow icon. This makes the upload block file input visible.\nClick on File select and select the downloaded file at step 1 (athlete_events.csv).\nAs we obtain a csv file, we must parse it with a new_csv_block. Repeat step 3 to add the new_csv_block. The table is 271116 rows and 15 columns.\nAdd a new_filter_block and select Sex as column and then F in the values input. We leave the comparison to == and click on the Run button. Notice we now have 74522 rows.\nAdd a new_mutate_block with the following expression: birth_year = Year - Age (this gives us an approximate birth year). Click on submit.\n\nFrom now on, we leave the first stack as is and will reuse it in other stacks. We want to display the average height distribution for female athletes. Letâ€™s do it below.\n\nCreate a new stack by clicking on Add stack.\nAdd it a new_result_block. This allows to import the data from the first stack (and potentially any stack from the dashboard). If you donâ€™t see any data, select another stack name from the dropdown menu.\nAdd a new_ggplot_block, leave x as default function and select Height as variable in the columns input.\nAdd a new_geomhistogram_block. Now we have our distribution plot.\n\nAlternatively, you could remove the 2 plot blocks and add a new_summarize_block using mean as function and Height as column (result: 168 cm).\nIn the following, we create a look-up table to be able to retrieve the athlete names based on their ID.\n\nCreate a new stack.\nAdd a result block to import data from the very first stack.\nAdd a new_select_block and only select ID, Name, birth_year, Team and Sport as columns.\n\nOur goal is now to find which athlete did 2 or more different sports.\n\nCreate a new stack.\nAdd a result block to import data from the very first stack.\nAdd a new_filter_block , select Medal as column, != as comparison operator and leave the value empty. Click on run, which will only get athletes with medals.\nAdd a new_group_by_block, grouping by ID (as some athletes have the same name).\nAdd a new_summarize_block by choising the function n_distinct applied on the Sport columns.\nAdd a new_filter_block , select N_DISTINCT as column, &gt;= as comparison operator and set the value to 2. Click on run. This gives us the athletes that are doing 2 sports or more.\nAdd a new_join_block. Select left_join as join function, select the third stack (lookup table) as join table and ID as column.\nAdd a new_arrange_block for the birth_year column.\n\nAs a conclusion, Hjrdis Viktoria Tpel (1904) was the first recorded athlete to compete in 2 different sports, swimming and diving for Sweden. Lauryn Chenet Williams (1984) is the latest for US with Athletics and Bobsleigh. Itâ€™s actually quite amazing to see people competing in two quite unrelated sports like swimming and handbain the case of Roswitha Krause.\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nwebr::install(\"blockr\", repos = c(\"https://bristolmyerssquibb.github.io/webr-repos\", \"https://repo.r-wasm.org\"))\nwebr::install(\"blockr.ggplot2\", repos = c(\"https://bristolmyerssquibb.github.io/webr-repos\", \"https://repo.r-wasm.org\"))\n\nlibrary(blockr)\nlibrary(blockr.ggplot2)\n\noptions(shiny.maxRequestSize = 100*1024^2)\ndo.call(set_workspace, args = list(title = \"My workspace\"))\nserve_workspace(clear = FALSE)\n\nAs an end-user, you are not supposed to write code. As such, if you think anything is missing, you can open an issue here, or ask any developer you are working with to create new blocks. This leads us to the second part of this blog post â€¦ How to use blockr as a developers?\nAs a developer\nHow to install it:\npak::pak(\"BristolMyersSquibb/blockr\")\nblockr canâ€™t provide any single data manipulation or visualization block. Thatâ€™s the reason why we made it easily extensible. You can get an introduction to blockr for developers here.\nIn the following, we create an ordinary differential equations solver block using the pracma package. We choose the Lorenz attractor. With R, equations may be written as:\nlorenz &lt;- function(t, y, parms) {\n  c(\n    X = parms[1] * y[1] + y[2] * y[3],\n    Y = parms[2] * (y[2] - y[3]),\n    Z = -y[1] * y[2] + parms[3] * y[2] - y[3]\n  )\n}\nwhere t is the time, y a vector of solutions and params the various parameters. If you are familiar with deSolve, equations are defined with similar functions. For this blog post, we selected pracma as deSolve does not run in shinylive, so you could not see the embedded demonstration.\nAdd interactivity with the fields\n\nWe want to add interactivity on the 3 different parameters. Hence, we create our new block function with 3 fields inside a list. Since the expected values are numbers, we leverage the new_numeric_field. Parameters are only explicitly shown for the first field:\nnew_ode_block &lt;- function(...) {\n  fields &lt;- list(\n    a = new_numeric_field(value = -8 / 3, min = -10, max = 20),\n    b = new_numeric_field(-10, -50, 100),\n    c = new_numeric_field(28, 1, 100)\n  )\n  # TBD\n  # ...\n}\nAs you may imagine, these fields are subsequently translated into shiny inputs, that is numericInput in our example. If you face a situation where you need to implement a custom field, not included in blockr, you can read this vignette.\nCreate the block expression\nAs next step, we instantiate our block with the new_block blockr constructor:\nnew_block(\n  fields = fields,\n  expr = quote(&lt;EXPR&gt;),\n  ...,\n  class = &lt;CLASSES&gt;,\n  submit = FALSE\n)\nA block is composed of fields, a quoted expression which involved fields (to delay the evaluation), somes classes which control the block behavior, and extra parameters passed with .... Finally, submit allows to delay the block evaluation by requiring the user to click on a submit button (FALSE by default). This prevents from triggering unwanted intensive computations.\nIn our example, the expression calls the ode45 function. Notice the usage of substitute to inject the lorenz function within the expression. This is necessary since lorenz is defined outside of the expression, and using quote would fail. Fields are invoked with .(field_name), a rather strange notation, required by bquote to process the expression. It is not mandory to understand this technical underlying detail, but this standard must be respected. You may also notice that some parameters like the initial conditions y0 or time values are hardcoded. We leave the reader to transform them into fields, as an exercise:\nnew_block(\n  fields = fields,\n  expr = substitute(\n    as.data.frame(\n      ode45(\n        fun,\n        y0 = c(X = 1, Y = 1, Z = 1),\n        t0 = 0,\n        tfinal = 100,\n        parms = c(.(a), .(b), .(c))\n      )\n    ),\n    list(fun = lorenz)\n  )\n  # TBD\n)\nAdd the right classes\nWe give our block 2 classes, namely ode_block and data_block:\n\nCodenew_ode_block &lt;- function(...) {\n  fields &lt;- list(\n    a = new_numeric_field(-8 / 3, -10, 20),\n    b = new_numeric_field(-10, -50, 100),\n    c = new_numeric_field(28, 1, 100)\n  )\n\n  new_block(\n    fields = fields,\n    expr = substitute(\n      as.data.frame(\n        ode45(\n          fun,\n          y0 = c(X = 1, Y = 1, Z = 1),\n          t0 = 0,\n          tfinal = 100,\n          parms = c(.(a), .(b), .(c))\n        )\n      ),\n      list(fun = lorenz)\n    ),\n    ...,\n    class = c(\"ode_block\", \"data_block\")\n  )\n}\n\n\nAs explained earlier, they are required to control the block behavior, as blockr is build with S3. For instance, data_block have a specific evaluation method, to calculate the expression:\nevaluate_block.data_block &lt;- function (x, ...) \n{\n  stopifnot(...length() == 0L)\n  eval(generate_code(x), new.env())\n}\nwhere generate_code processes the block code. Data blocks are considered as entry point blocks, as opposed to transformation blocks, that operate on data. Therefore, you may easily understand that the evaluation method for a transform block requires to pass the data from the previous block with %&gt;%:\nevaluate_block.block &lt;- function (x, data, ...) \n{\n  stopifnot(...length() == 0L)\n  eval(substitute(data %&gt;% expr, list(expr = generate_code(x))), list(data = data))\n}\nIf you want to build a plot block and plot layers blocks, you would have to design a specific evaluate method, that accounts for the + operator required by ggplot2. To learn more about how to create a plot block, you can read this article.\nDemo\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| components: [viewer, editor]\n#| column: screen-inset-shaded\n#| viewerHeight: 800\nwebr::install(\"blockr\", repos = c(\"https://bristolmyerssquibb.github.io/webr-repos\", \"https://repo.r-wasm.org\"))\nwebr::install(\"blockr.ggplot2\", repos = c(\"https://bristolmyerssquibb.github.io/webr-repos\", \"https://repo.r-wasm.org\"))\n\nlibrary(blockr)\nlibrary(pracma)\nlibrary(blockr.ggplot2)\n\nlorenz &lt;- function(t, y, parms) {\n  c(\n    X = parms[1] * y[1] + y[2] * y[3],\n    Y = parms[2] * (y[2] - y[3]),\n    Z = -y[1] * y[2] + parms[3] * y[2] - y[3]\n  )\n}\n\nnew_ode_block &lt;- function(...) {\n  fields &lt;- list(\n    a = new_numeric_field(-8 / 3, -10, 20),\n    b = new_numeric_field(-10, -50, 100),\n    c = new_numeric_field(28, 1, 100)\n  )\n\n  new_block(\n    fields = fields,\n    expr = substitute(\n      as.data.frame(\n        ode45(\n          fun,\n          y0 = c(X = 1, Y = 1, Z = 1),\n          t0 = 0,\n          tfinal = 100,\n          parms = c(.(a), .(b), .(c))\n        )\n      ),\n      list(fun = lorenz)\n    ),\n    ...,\n    class = c(\"ode_block\", \"data_block\")\n  )\n}\n\nstack &lt;- new_stack(\n  new_ode_block,\n  new_ggplot_block(\n    func = c(\"x\", \"y\"),\n    default_columns = c(\"y.1\", \"y.2\")\n  ),\n  new_geompoint_block\n)\nserve_stack(stack)\n\nPackaging new blocks: the registry\nIn the above example, we define the block on the fly. However, an other outstanding feature of blockr is the registry, which you can see as a blocks supermarket. From the R side, the registry is an environment that can be extended by developers who bring their own blocks packages:\n\n\n\n\n\n\n\nregister\n\n\nRegistry\n\n\nunregister\n\n\nSelect block\n\n\nName: select block\n\n\nDescription: select columns in a table\n\n\nClasses: select_block, tranform_block\n\n\nInput: data.frame\n\n\nOutput: data.frame\n\n\nPackage: blockr\n\n\nFilter block\n\n\nâ€˜â€™\n\n\nyour_block_package\n\n\nNew block 1\n\n\nNew block 2\n\n\n\n\n\n\n\nTo get an overview of all available blocks within the blockr core package, we call get_registry:\n\nCodeget_registry()\n\n                 ctor                                  description  category\n1       arrange_block                              Arrange columns transform\n2           csv_block                           Read a csv dataset    parser\n3       dataset_block              Choose a dataset from a package      data\n4  filesbrowser_block       Select files on the server file system      data\n5        filter_block                       filter rows in a table transform\n6      group_by_block                             Group by columns transform\n7          head_block               Select n first rows of dataset transform\n8          join_block                              Join 2 datasets transform\n9          json_block                          Read a json dataset    parser\n10       mutate_block                                 Mutate block transform\n11          rds_block                           Read a rds dataset    parser\n12       result_block Shows result of another stack as data source      data\n13       select_block                    select columns in a table transform\n14    summarize_block                        summarize data groups transform\n15       upload_block                   Upload files from location      data\n16          xpt_block                           Read a xpt dataset    parser\n                                            classes      input     output\n1             arrange_block, transform_block, block data.frame data.frame\n2   csv_block, parser_block, transform_block, block     string data.frame\n3                  dataset_block, data_block, block       &lt;NA&gt; data.frame\n4             filesbrowser_block, data_block, block       &lt;NA&gt;     string\n5              filter_block, transform_block, block data.frame data.frame\n6            group_by_block, transform_block, block data.frame data.frame\n7                head_block, transform_block, block data.frame data.frame\n8                join_block, transform_block, block data.frame data.frame\n9  json_block, parser_block, transform_block, block     string data.frame\n10             mutate_block, transform_block, block data.frame data.frame\n11  rds_block, parser_block, transform_block, block     string data.frame\n12                  result_block, data_block, block       &lt;NA&gt; data.frame\n13             select_block, transform_block, block data.frame data.frame\n14          summarize_block, transform_block, block data.frame data.frame\n15                  upload_block, data_block, block       &lt;NA&gt;     string\n16  xpt_block, parser_block, transform_block, block     string data.frame\n   package\n1   blockr\n2   blockr\n3   blockr\n4   blockr\n5   blockr\n6   blockr\n7   blockr\n8   blockr\n9   blockr\n10  blockr\n11  blockr\n12  blockr\n13  blockr\n14  blockr\n15  blockr\n16  blockr\n\n\nThis function returns a dataframe containing information about blocks such as their constructors, like new_ode_block, the description, the category (data, transform, plot â€¦ this is user defined), classes, accepted input, returned output and package.\nTo register a block we call register_block (or register_blocks for multiple blocks):\n\nCoderegister_my_blocks &lt;- function() {\n  register_block(\n    constructor = new_ode_block,\n    name = \"ode block\",\n    description = \"Computed the Lorent attractor solutions\",\n    classes = c(\"ode_block\", \"data_block\"),\n    input = NA_character_,\n    output = \"data.frame\",\n    package = \"&lt;YOUR_PACKAGE&gt;\",\n    category = \"data\"\n  )\n  # You can register any other blocks here ...\n}\n\n\nwhere &lt;YOUR_PACKAGE&gt; must be replaced by your real package name.\nWithin a zzz.R script, you can ensure to register any block when the package loads with a hook:\n.onLoad &lt;- function(libname, pkgname) {\n  register_my_blocks()\n  invisible(NULL)\n}\nAfter the registration, you can check whether the registry is updated, by looking at the ode block:\n\nCoderegister_my_blocks()\nreg &lt;- get_registry()\nreg[reg$package == \"&lt;YOUR_PACKAGE&gt;\", ]\n\n        ctor                             description category\n11 ode_block Computed the Lorent attractor solutions     data\n                        classes input     output        package\n11 ode_block, data_block, block  &lt;NA&gt; data.frame &lt;YOUR_PACKAGE&gt;"
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html",
    "href": "posts/2020-12-23-dynamic-gha/index.html",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "",
    "text": "I wanted to try out the new fromJSON() that allows dynamic build matrices in GitHub Actions for quite some time now. Today was the day.\nGitHub Actions allows automating build and deployment processes (CI/CD), tightly integrated with GitHub. A build matrix is a way to define very similar workflows that differ only by configuration parameters. Usually, a build matrix is defined directly in the .yaml files together with the workflows. This blog post shows how to define these build matrices dynamically so that the â€œsource of truthâ€ for the matrix definition is outside the .yaml file.\nThe configuration for a workflow is a YAML file that has a context and expression syntax with very few basic functions. Two very powerful functions are toJSON() and fromJSON():\nThe basic setup comprises of two jobs: one that creates the workflow definition as JSON and stores it as output, and another dependent job that injects this output via fromJSON() into its matrix definition. A third job is defined for testing if outputs are passed correctly between jobs.\nThe original blog post contains a somewhat brief description. This blog post gives a walkthrough of how I converted a static to a dynamic build matrix in the DBItest project."
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#original-matrix",
    "href": "posts/2020-12-23-dynamic-gha/index.html#original-matrix",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Original matrix",
    "text": "Original matrix\nIn DBItest, we test the compatibility of new or updated tests with backend packages. Each backend is run in a build matrix, which is defined as follows:\njobs:\n  backend:\n    strategy:\n      fail-fast: false\n      matrix:\n        package:\n          - duckdb\n          - RSQLite\n          - RMariaDB\n          - RPostgres\n          - RKazam\nThe relevant backends are defined in the Makefile, we want to get the list from there so that we can use a single source of truth.\nThis is a very simple build matrix, ideally suited for first experiments. The techniques shown here are applicable to build matrices of any complexity and size."
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#derive-and-verify-json",
    "href": "posts/2020-12-23-dynamic-gha/index.html#derive-and-verify-json",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Derive and verify JSON",
    "text": "Derive and verify JSON\nOur goal is to create the package: section from the above matrix in JSON format. To derive the JSON format, I use the sed stream editor, my beloved hammer that I use whenever I see a text transformation task in the shell:\necho '{ \"package\" : ['\n## { \"package\" : [\nsed -n \"/^REVDEP *:= */ { s///; p }\" revdep-dev/Makefile | sed 's/ /, /g' | xargs -n 1 echo | sed -r 's/^([^,]*)(,?)$/\"\\1\"\\2/'\n## \"RMariaDB\",\n## \"RSQLite\",\n## \"RPostgres\",\n## \"RKazam\",\n## \"duckdb\"\necho \"]}\"\n## ]}\nThis is not pretty, but still valid JSON when put together. We can prettify with jq ., later we will use jq -c . to condense to a single line.\n(\n  echo '{ \"package\" : ['\n  sed -n \"/^REVDEP *:= */ { s///; p }\" revdep-dev/Makefile | sed 's/ /, /g' | xargs -n 1 echo | sed -r 's/^([^,]*)(,?)$/\"\\1\"\\2/'\n  echo \"]}\"\n) | jq .\n{\n  \"package\": [\n    \"RMariaDB\",\n    \"RSQLite\",\n    \"RPostgres\",\n    \"RKazam\",\n    \"duckdb\"\n  ]\n}\nWe verify the YAML version by piping to json2yaml which can be installed with npm install json2yaml:\n---\n  package:\n    - \"RMariaDB\"\n    - \"RSQLite\"\n    - \"RPostgres\"\n    - \"RKazam\"\n    - \"duckdb\"\nThese tools are preinstalled on the workers. This avoids time-consuming installation procedures in this first job that needs to be run before the main jobs can even start.1"
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#define-job",
    "href": "posts/2020-12-23-dynamic-gha/index.html#define-job",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Define job",
    "text": "Define job\nOnce we have derived the JSON, weâ€™re ready to define a job that creates the matrix. This must be done in the same workflow file where the matrix is defined, ideally before the main job. The job runs on ubuntu-latest, and also must clone the repository. In the bash snippet, the $matrix variable contains the JSON. It is shown and pretty-printed before it is provided as output via echo ::set-output ....\njobs:\n  matrix:\n    runs-on: ubuntu-latest\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - uses: actions/checkout@v2\n\n      - id: set-matrix\n        run: |\n          matrix=$((\n            echo '{ \"package\" : ['\n            sed -n \"/^REVDEP *:= */ { s///; p }\" revdep-dev/Makefile | sed 's/ /, /g' | xargs -n 1 echo | sed -r 's/^([^,]*)(,?)$/\"\\1\"\\2/'\n            echo \" ]}\"\n          ) | jq -c .)\n          echo $matrix\n          echo $matrix | jq .\n          echo \"::set-output name=matrix::$matrix\"\n\n  backend:\n    # Original workflow\n    # ..."
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#verify-output",
    "href": "posts/2020-12-23-dynamic-gha/index.html#verify-output",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Verify output",
    "text": "Verify output\nBefore plugging in the generated JSON into our build job, we add another check job to verify if the generated JSON is transported correctly across job boundaries. The needs: matrix declares that the job must wait before the first matrix job succeeds. The jobâ€™s output is queried via ${{ needs.matrix.outputs.matrix }}, the quotes ensure that bash processes this correctly. We install and use json2yaml to double-check what the YAML snippet looks like.\njobs:\n  matrix:\n    # job defined above\n\n  check-matrix:\n    runs-on: ubuntu-latest\n    needs: matrix\n    steps:\n      - name: Install json2yaml\n        run: |\n          sudo npm install -g json2yaml\n\n      - name: Check matrix definition\n        run: |\n          matrix='${{ needs.matrix.outputs.matrix }}'\n          echo $matrix\n          echo $matrix | jq .\n          echo $matrix | json2yaml\n\n  backend:\n    # Original workflow\n    # ..."
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#use-output",
    "href": "posts/2020-12-23-dynamic-gha/index.html#use-output",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Use output",
    "text": "Use output\nFinally, weâ€™re ready to use the generated JSON as a build matrix. The workflow now uses matrix: ${{fromJson(needs.matrix.outputs.matrix)}} instead of the hard-coded matrix:\njobs:\n  matrix:\n    # see above\n\n  check-matrix:\n    # see above\n\n  backend:\n    needs: matrix\n\n    strategy:\n      fail-fast: false\n      matrix: ${{fromJson(needs.matrix.outputs.matrix)}}\n\n    # rest unchanged\nThis gives a workflow as shown in the image below.\n\n\n\nFinal workflow with dynamic build matrix"
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#next-steps",
    "href": "posts/2020-12-23-dynamic-gha/index.html#next-steps",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Next steps",
    "text": "Next steps\nFor R packages, I see two use case where dynamic matrices can be useful:\n\nTesting if package checks pass if one suggested package is not installed. Ideally, we remove suggested packages one by one and run in parallel.\nTesting reverse dependencies. For some packages we may hit the limit of 256 jobs per workflow run. Allocating downstream packages to workers, minimizing the number of packages to be installed on each worker, sounds like an interesting optimization problem.\n\nWhat are your use cases for dynamic build matrices? Drop us a line at mail@cynkra.com!"
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#caveats",
    "href": "posts/2020-12-23-dynamic-gha/index.html#caveats",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Caveats",
    "text": "Caveats\nEven with this simple build matrix, it took more time than I would have hoped to get the bits and pieces right. Quoting is hard. Setting up the check-matrix job really saves time, I wish I had done this from the start.\nBoth fromJson() and fromJSON() appear to work. The internal functions from the expression syntax seem to be case-insensitive throughout.\nFor older versions, jq needs to be called as jq . to act as a pretty-printer. For newer versions this can be omitted.\nToday I also learned that workflows can be temporarily disabled. This is useful in situations where you experiment with a workflow and want to avoid running other workflows for every test."
  },
  {
    "objectID": "posts/2020-12-23-dynamic-gha/index.html#footnotes",
    "href": "posts/2020-12-23-dynamic-gha/index.html#footnotes",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can use any tool or ecosystem you are familiar with to come up with the JSON definition. To avoid long installation times, use a specific image for your step via uses: docker://... or implement a container action, also possible in the same repository.â†©ï¸"
  },
  {
    "objectID": "posts/2021-09-18-tsbox-03/index.html",
    "href": "posts/2021-09-18-tsbox-03/index.html",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "",
    "text": "The tsbox package provides a set of tools that are agnostic towards existing time series classes. The tools also allow you to handle time series as plain data frames, thus making it easy to deal with time series in a dplyr or data.table workflow.\nVersion 0.3.1 is now on CRAN and provides several bugfixes and extensions (see here for the full change log). A detailed overview of the package functionality is given in the documentation page (or in an older blog-post)."
  },
  {
    "objectID": "posts/2021-09-18-tsbox-03/index.html#new-and-extended-functionality",
    "href": "posts/2021-09-18-tsbox-03/index.html#new-and-extended-functionality",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "New and extended functionality",
    "text": "New and extended functionality\n\nts_frequency(): changes the frequency of a time series. It is now possible to aggregate any time series to years, quarters, months, weeks, days, hours, minutes or seconds. For low- to high-frequency conversion, the tempdisagg package can now convert low frequency to high frequency and has support for ts-boxable objects. E.g.:\nlibrary(tsbox)\nx &lt;- ts_tbl(EuStockMarkets)\nx\n#&gt; # A tibble: 7,440 Ã— 3\n#&gt;   id    time                 value\n#&gt;   &lt;chr&gt; &lt;dttm&gt;               &lt;dbl&gt;\n#&gt; 1 DAX   1991-07-01 03:18:27  1629.\n#&gt; 2 DAX   1991-07-02 13:01:32  1614.\n#&gt; 3 DAX   1991-07-03 22:44:38  1607.\n#&gt; 4 DAX   1991-07-05 08:27:43  1621.\n#&gt; 5 DAX   1991-07-06 18:10:48  1618.\n#&gt; # â€¦ with 7,435 more rows\n\nts_frequency(x, \"week\")\n#&gt; # A tibble: 1,492 Ã— 3\n#&gt;   id    time        value\n#&gt;   &lt;chr&gt; &lt;date&gt;      &lt;dbl&gt;\n#&gt; 1 DAX   1991-06-30  1618.\n#&gt; 2 DAX   1991-07-07  1633.\n#&gt; 3 DAX   1991-07-14  1632.\n#&gt; 4 DAX   1991-07-21  1620.\n#&gt; 5 DAX   1991-07-28  1616.\n#&gt; # â€¦ with 1,487 more rows\nts_index(): returns an indexed series, with a value of 1 at the base period. This base period can now be specified more flexibly. E.g., the average of a year can defined as 1 (which is a common use case).\nts_na_interpolation(): A new function that wraps imputeTS::na_interpolation() from the imputeTS package and allows the imputation of missing values for any time series object.\nts_first_of_period(): A new function that replaces the date or time value by the first of the period. This is useful because tsbox usually relies on timestamps being the first of a period. The following monthly series has an offset of 14 days. ts_first_of_period() changes the timestamp to the first date of each month:\nx &lt;- ts_lag(ts_tbl(mdeaths), \"14 days\")\nx\n#&gt; # A tibble: 72 Ã— 2\n#&gt;   time       value\n#&gt;   &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 1974-01-15  2134\n#&gt; 2 1974-02-15  1863\n#&gt; 3 1974-03-15  1877\n#&gt; 4 1974-04-15  1877\n#&gt; 5 1974-05-15  1492\n#&gt; # â€¦ with 67 more rows\n\nts_first_of_period(x)\n#&gt; # A tibble: 72 Ã— 2\n#&gt;   time       value\n#&gt;   &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 1974-01-01  2134\n#&gt; 2 1974-02-01  1863\n#&gt; 3 1974-03-01  1877\n#&gt; 4 1974-04-01  1877\n#&gt; 5 1974-05-01  1492\n#&gt; # â€¦ with 67 more rows"
  },
  {
    "objectID": "posts/2021-09-18-tsbox-03/index.html#convert-everything-to-everything",
    "href": "posts/2021-09-18-tsbox-03/index.html#convert-everything-to-everything",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "Convert everything to everything",
    "text": "Convert everything to everything\ntsbox is built around a set of converters, which convert time series stored as ts, xts, data.frame, data.table, tibble, zoo, tsibble, tibbletime, tis, irts or timeSeries to each other:\nlibrary(tsbox)\nx.ts &lt;- ts_c(fdeaths, mdeaths)\nx.xts &lt;- ts_xts(x.ts)\nx.df &lt;- ts_df(x.xts)\nx.dt &lt;- ts_dt(x.df)\nx.tbl &lt;- ts_tbl(x.dt)\nx.zoo &lt;- ts_zoo(x.tbl)\nx.tsibble &lt;- ts_tsibble(x.zoo)\nx.tibbletime &lt;- ts_tibbletime(x.tsibble)\nx.timeSeries &lt;- ts_timeSeries(x.tibbletime)\nx.irts &lt;- ts_irts(x.tibbletime)\nx.tis &lt;- ts_tis(x.irts)\nall.equal(ts_ts(x.tis), x.ts)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "posts/2021-09-18-tsbox-03/index.html#use-same-functions-for-time-series-classes",
    "href": "posts/2021-09-18-tsbox-03/index.html#use-same-functions-for-time-series-classes",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "Use same functions for time series classes",
    "text": "Use same functions for time series classes\nBecause this works reliably, it is easy to define a toolkit that works for all classes. So, whether we want to smooth, scale, differentiate, chain, forecast, regularize, impute or seasonally adjust a time series, we can use the same commands to whatever time series class at hand:\nts_trend(x.ts)   # estimate a trend line\nts_pc(x.xts)     # calculate percentage change rates (period on period)\nts_pcy(x.df)     # calculate percentage change rates (year on year)\nts_lag(x.dt)     # lagged series\nThere are many more. Because they all start with ts_, you can use auto-complete to see whatâ€™s around. Most conveniently, there is a time series plot function that works for all classes and frequencies:\nts_plot(\n  `Airline Passengers` = AirPassengers,\n  `Lynx trappings` = ts_tis(lynx),\n  `Deaths from Lung Diseases` = ts_xts(fdeaths),\n  title = \"Airlines, trappings, and deaths\",\n  subtitle = \"Monthly passengers, annual trappings, monthly deaths\"\n)\n\n\n\ntime series plot"
  },
  {
    "objectID": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html",
    "href": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html",
    "title": "Accessing Googleâ€™s API via OAuth2",
    "section": "",
    "text": "At cynkra we recently aimed to automate more parts of our internal toolstack. One tool is Google Workspace. Google Workspace offers a comprehensive REST API which can be used for automation purposes.\nWhen interacting with an API, authentication is usually required. This is commonly done by adding a header to the request which includes an access token along with the actual API request. For many APIs these headers can be simple single access token of type â€œBearerâ€. These tokens often have no expiration date and infinite scopes, meaning they can be used for any kind of request against the respective API endpoints. The risk with these tokens is that they are quite powerful and an attacker can somewhat easily get infinite access to your account, both in terms of scopes and time.\nTherefore, many services recently started to favor the use of OAuth2 in a multi-stage authentication concept. The process can be broken down as follows:\nThis multi-stage process can be automated by using automation tools like Ansible or similar. Yet the tricky part is usually the authentication against the OAuth2 app. Traditionally OAuth2 apps aim for GUI-based interaction, i.e., someone clicking a button to authorize the request. However, when aiming for automation via an API, this is not feasible. Instead the OAuth2 app should return the access token as code to further continue the automated workflow. There are plenty of Stackoverflow questions about this topic with many upvotes:"
  },
  {
    "objectID": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-oob-deprecation",
    "href": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-oob-deprecation",
    "title": "Accessing Googleâ€™s API via OAuth2",
    "section": "The â€œOOBâ€ deprecation",
    "text": "The â€œOOBâ€ deprecation\nFor many years there was a workaround by using redirect_uri=urn:ietf:wg:oauth:2.0:oob combined with response_type=code which was widely shared across the web and Youtube. Yet in Feburary 2022 Google finally blocked this approach as it is considered unsafe and more secure methods should be used."
  },
  {
    "objectID": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-jwt-authentication-approach",
    "href": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-jwt-authentication-approach",
    "title": "Accessing Googleâ€™s API via OAuth2",
    "section": "The â€œJWTâ€ authentication approach",
    "text": "The â€œJWTâ€ authentication approach\nHence a new approach is needed to authenticate against Google OAuth2 apps programmatically. One of these is the use of JSON Web Tokens (JWT). These are different to Bearer tokens in the way that they must be signed and encrypted using a domain-wide access token and am specific algorithm which the OAuth2 apps expects (for future decoding purposes). The mentioned encryption is also not straightforward and usually requires the use of an additional language (e.g.Â Python, Ruby, Java) and a respective module which does the encryption. The key and its secret (in the Google case) which should be encrypted must be generated within a service account that was granted domain-wide delegation. If such an encrypted JWT is sent to the OAuth2 app, it can verify the owner and issue a short-lived token with the respective scopes of the OAuth2 app."
  },
  {
    "objectID": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#ansible-workflow-example",
    "href": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#ansible-workflow-example",
    "title": "Accessing Googleâ€™s API via OAuth2",
    "section": "Ansible workflow example",
    "text": "Ansible workflow example\nAll of the above made the process of issueing a â€œsimpleâ€ API call against the Google API quite cumbersome. This is why in the following a fully-working ansible approach is provided which uses a Ruby script for the initial JWT encryption.\nThis assumes\n\na working Ruby installation at /usr/bin/ruby\nan existing Google service account with domain wide delegation\nan OAuth2 app to which the service account has acccess to with matching scopes required for the final API call\n\nDisclaimer: The jwt.rb script below and parts of the ansible logic are taken/adapted from another blog post which I am unable to find again. Memo to myself: always instantly store the link somewhere if you find some helpful content on a websiteâ€¦\n- name: \"Google Workspace: Create JWT for Google OAuth2\"\n  command: &gt;\n    env ruby &lt;path/to/&gt;/jwt.rb --iss \"google-workspace@&lt;some service account name&gt;.iam.gserviceaccount.com\"\n    --sub \"&lt;issuer email&gt;\" --scope \"{{ google_workspace_oauth2_api_scopes | join(' ') }}\"\n    --kid \"{{ google_workspace_oauth2_key_id.value }}\"\n    --pkey \"{{ google_workspace_oauth2_private_key.value }}\"\n  args: { chdir: \"/usr/bin/\" }\n  register: jwt\nHere google_workspace_oauth2_api_scopes is a list of Google API scopes\ngoogle_workspace_oauth2_api_scopes:\n  - 'https://www.googleapis.com/auth/admin.directory.user'\n  - 'https://www.googleapis.com/auth/admin.directory.group'\n  - 'https://www.googleapis.com/auth/admin.directory.domain'\n  - 'https://www.googleapis.com/auth/admin.directory.userschema'\n  - 'https://www.googleapis.com/auth/apps.licensing'\nand google_workspace_oauth2_key_id and google_workspace_oauth2_private_key are the credentials from the respective service account used.\nThe jwt.rb file referenced in the call above looks as follows:\n#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire 'jwt'\nrequire 'optparse'\nrequire 'openssl'\n\noptions = {}\nOptionParser.new do |opts|\n  opts.banner = 'Usage: jwt.rb [options]'\n\n  opts.on('--iss ISS', 'Issuer') do |iss|\n    options[:iss] = iss\n  end\n  opts.on('--sub SUB', 'Subject') do |sub|\n    options[:sub] = sub\n  end\n  opts.on('--scope SCOPE', 'API Scopes') do |scope|\n    options[:scope] = scope\n  end\n  opts.on('--kid KID', 'Key id') do |kid|\n    options[:kid] = kid\n  end\n  opts.on('--pkey PKEY', 'Key') do |pkey|\n    options[:pkey] = pkey\n  end\nend.parse!\n\niat = Time.now.to_i\nexp = iat + 900 # token is 900s valid\n\npayload = { iss: options[:iss].to_s,\n            sub: options[:sub].to_s,\n            scope: options[:scope].to_s,\n            aud: 'https://oauth2.googleapis.com/token',\n            kid: options[:kid].to_s,\n            exp: exp,\n            iat: iat }\n\npkey = options[:pkey].to_s.gsub('\\n', \"\\n\")\npriv_key = OpenSSL::PKey::RSA.new(pkey)\n\ntoken = JWT.encode(payload, priv_key, 'RS256')\n\nputs token\nThe important part is happening at the bottom: JWT.encode encodes the payload of the POST request, which consists of the API key from the service account. Specifically, the secret of the respective key pair (named priv_key here) is used to encrypt the payload.\nNext, this JWT needs to be passed to the https://oauth2.googleapis.com/token endpoint to ask for a Bearer access token by using the following payload in the body:\n- name: \"Google Workspace: Get access token from Google oauth2\"\n  uri:\n    url: \"https://oauth2.googleapis.com/token\"\n    method: POST\n    body: \"grant_type={{ google_workspace_oauth2_grant_type }}&assertion={{ jwt.stdout }}\"\n    return_content: true\n  register: token\nHere, google_workspace_oauth2_grant_type needs to be \"urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Ajwt-bearer\". This tells the endpoint that we are handing over a JWT token and want to get a Bearer token back.\nFinally, this (short-lived) Bearer token can be used to issue the desired API call, e.g.Â creating a new user:\n- name: \"Google Workspace: Create user\"\n  uri:\n    method: POST\n    url: https://admin.googleapis.com/admin/directory/v1/users\n    headers:\n      authorization: \"Bearer {{ token.json.access_token }}\"\n    body_format: json\n    body: '{\n           \"primaryEmail\": \"{{ username }}@email.com\",\n           \"password\": \"{{ user_password }}\",\n           \"name\": {\n             \"givenName\": \"{{ first_name }}\",\n             \"familyName\": \"{{ last_name }}\"\n           },\n           \"isAdmin\": \"{{ admin }}\"\n         }'"
  },
  {
    "objectID": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#summary",
    "href": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#summary",
    "title": "Accessing Googleâ€™s API via OAuth2",
    "section": "Summary",
    "text": "Summary\nThe OAuth2-API-Auth process to authenticate against the Google API is quite cumbersome and quite a few little things can go wrong. As for all other methods, it is not possible to say how long this method will stay functional. JWTs are a quite promising concept and it is likely that they will be around for quite some time as they are considered pretty save. The biggest challenge is usually to puzzle all bits together and find the correct documentation resource for the respective provider. Once it works, thereâ€™s almost no overhead when using tools like Ansible to automate the process.\nIt should be noted that the approach is quite generic: for some providers you might need to change the encoding algorithm when creating the JWT (e.g.Â for Zoom it needs to be HS256) but other than that you should be able to reuse the jwt.rb script."
  },
  {
    "objectID": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#full-ansible-script",
    "href": "posts/2022-05-14-accessing-google-api-via-oauth2/index.html#full-ansible-script",
    "title": "Accessing Googleâ€™s API via OAuth2",
    "section": "Full Ansible script",
    "text": "Full Ansible script\n\n- name: \"Google Workspace: Create JWT for Google OAuth2\"\n  command: &gt;\n    env ruby &lt;path/to/&gt;/jwt.rb --iss \"google-workspace@&lt;some service account name&gt;.iam.gserviceaccount.com\"\n    --sub \"&lt;issuer email&gt;\" --scope \"{{ google_workspace_oauth2_api_scopes | join(' ') }}\"\n    --kid \"{{ google_workspace_oauth2_key_id.value }}\"\n    --pkey \"{{ google_workspace_oauth2_private_key.value }}\"\n  args: { chdir: \"/usr/bin/\" }\n  register: jwt\n\n- name: \"Google Workspace: Get access token from Google oauth2\"\n  uri:\n    url: \"https://oauth2.googleapis.com/token\"\n    method: POST\n    body: \"grant_type={{ google_workspace_oauth2_grant_type }}&assertion={{ jwt.stdout }}\"\n    return_content: true\n  register: token\n\n- name: \"Google Workspace: Create user\"\n  uri:\n    method: POST\n    url: https://admin.googleapis.com/admin/directory/v1/users\n    headers:\n      authorization: \"Bearer {{ token.json.access_token }}\"\n    body_format: json\n    body: '{\n           \"primaryEmail\": \"{{ username }}@email.com\",\n           \"password\": \"{{ user_password }}\",\n           \"name\": {\n             \"givenName\": \"{{ first_name }}\",\n             \"familyName\": \"{{ last_name }}\"\n           },\n           \"isAdmin\": \"{{ admin }}\"\n         }'"
  },
  {
    "objectID": "posts/2024-10-21-seasonal-1.10/index.html",
    "href": "posts/2024-10-21-seasonal-1.10/index.html",
    "title": "seasonal 1.10: R-interface to X-13ARIMA-SEATS",
    "section": "",
    "text": "We are happy to announce that the latest CRAN version of seasonal fixes several bugs and makes it easier to read specialized output from X-13ARIMA-SEATS. See here for a complete list of changes. In addition, the accompanying seasonalview package has been updated and finally gets rid of some annoying warning messages."
  },
  {
    "objectID": "posts/2024-10-21-seasonal-1.10/index.html#what-is-seasonal-adjustment",
    "href": "posts/2024-10-21-seasonal-1.10/index.html#what-is-seasonal-adjustment",
    "title": "seasonal 1.10: R-interface to X-13ARIMA-SEATS",
    "section": "What is seasonal adjustment?",
    "text": "What is seasonal adjustment?\nTime series data often display recurring seasonal patterns throughout the year. For instance, unemployment rates in the United States typically rise from January to March and again in June and July. By applying seasonal adjustment, analysts can identify and remove these predictable annual patterns, allowing for clearer interpretation of fundamental changes in the data. The R package seasonal provides a powerful and user-friendly way to perform this task in R, leveraging the X-13ARIMA-SEATS procedure developed by the U.S. Census Bureau."
  },
  {
    "objectID": "posts/2024-10-21-seasonal-1.10/index.html#getting-started-in-r",
    "href": "posts/2024-10-21-seasonal-1.10/index.html#getting-started-in-r",
    "title": "seasonal 1.10: R-interface to X-13ARIMA-SEATS",
    "section": "Getting started in R",
    "text": "Getting started in R\nTo get started, use the seas function on any time series:\n\nlibrary(seasonal)\nm &lt;- seas(AirPassengers)\nsummary(m)\n\n\nCall:\nseas(x = AirPassengers)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)    \nWeekday           -0.0029497  0.0005232  -5.638 1.72e-08 ***\nEaster[1]          0.0177674  0.0071580   2.482   0.0131 *  \nAO1951.May         0.1001558  0.0204387   4.900 9.57e-07 ***\nMA-Nonseasonal-01  0.1156204  0.0858588   1.347   0.1781    \nMA-Seasonal-12     0.4973600  0.0774677   6.420 1.36e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSEATS adj.  ARIMA: (0 1 1)(0 1 1)  Obs.: 144  Transform: log\nAICc: 947.3, BIC: 963.9  QS (no seasonality in final):    0  \nBox-Ljung (no autocorr.): 26.65   Shapiro (normality): 0.9908  \n\n\nBy default, seas calls the automatic procedures of X-13ARIMA-SEATS to perform a seasonal adjustment that works well in most circumstances.\nTo extract the final series, use the final() function:\n\nfinal(m)  \n\nTo plot the results, use the plot() function:\n\nplot(m)"
  },
  {
    "objectID": "posts/2024-10-21-seasonal-1.10/index.html#input-and-output",
    "href": "posts/2024-10-21-seasonal-1.10/index.html#input-and-output",
    "title": "seasonal 1.10: R-interface to X-13ARIMA-SEATS",
    "section": "Input and output",
    "text": "Input and output\nIn seasonal, it is possible to use the complete syntax of X-13ARIMA-SEATS. The X-13ARIMA-SEATS syntax uses specs and arguments, with each spec optionally containing some arguments. For example, in order to set the â€˜variablesâ€™ argument of the â€˜regressionâ€™ spec equal to td and ao1999.jan, the input to seas can be specified like this:\n\nm &lt;- seas(AirPassengers, regression.variables = c(\"td\", \"ao1955.jan\"))\n\nseasonal has a flexible mechanism to read data from X-13ARIMA-SEATS. With the series function, it is possible to extract any output that can be generated by X-13ARIMA-SEATS. For example, the following command returns the forecasts of the ARIMA model as a time series:\n\nm &lt;- seas(AirPassengers)\nseries(m, \"forecast.forecasts\")"
  },
  {
    "objectID": "posts/2024-10-21-seasonal-1.10/index.html#graphical-user-interface",
    "href": "posts/2024-10-21-seasonal-1.10/index.html#graphical-user-interface",
    "title": "seasonal 1.10: R-interface to X-13ARIMA-SEATS",
    "section": "Graphical user interface",
    "text": "Graphical user interface\nThe seasonalview package provides a graphical user interface to explore the results of the seasonal adjustment. Use the view() function to open it:\n\nview(m)\n\n\n\nSeasonalview GUI\n\nThis interface allows for interactive exploration and adjustment of seasonal models, making it easier to fine-tune your seasonal adjustments and visualize the results."
  },
  {
    "objectID": "posts/2024-10-21-seasonal-1.10/index.html#more-information",
    "href": "posts/2024-10-21-seasonal-1.10/index.html#more-information",
    "title": "seasonal 1.10: R-interface to X-13ARIMA-SEATS",
    "section": "More information",
    "text": "More information\nFor a more detailed introduction, check our article in the Journal of Statistical Software or visit the package website. The package website also allows you to upload your own data and explore the results interactively.\nYou can report bugs, ask questions, or contribute to the development on our GitHub repository. Thanks for using seasonal!"
  },
  {
    "objectID": "posts/2021-03-16-gfortran-macos/index.html",
    "href": "posts/2021-03-16-gfortran-macos/index.html",
    "title": "gfortran support for R on macOS",
    "section": "",
    "text": "For a long time, gfortran support on macOS could be achieved by installing the homebrew cask gfortran via brew cask install gfortran.\nAs of 2021, both the brew cask command and the cask gfortran are deprecated. Users who have installed this cask already, will not notice since things will continue to work as normal. Only new users who want to install gfortran this way, will get the message that the cask is â€œnot availableâ€. The cask was removed in December 2020 and merged into the gcc formula (which can be installed via brew install gcc). Now, one could go to https://github.com/fxcoudert/gfortran-for-macOS/releases and manually install the respective .dmg file. However, this is not a long-term approach, and usually, one would like to do this via brew, the most popular package manager for macOS.\nUnfortunately, this change did not result in a smooth experience for R users who want to compile packages from source that require a functional gfortran compiler. This requirement does not occur very often, as most users install R package binaries on macOS. These do not require a working gfortran installation.\nHowever, in some cases, when calling install.packages(), a working gfortran installation is needed. And if type = \"source\" is used, it needs to be there.\nThe issue after the integration of gfortran into the gcc formula is that the official R binary installer for macOS expects the gfortran installation at /usr/local/gfortran. This was fulfilled by the old gfortran cask but is not by the new gcc integration. Hence, trying to install the â€œclusterâ€ package via install.packages(\"cluster\", type = \"source\") will fail and gfortran will not be found:\nThere was a discussion about these changes in the homebrew PR, but the comments that highlighted potential issues seem to have gone unnoticed. Also, some workarounds posted in the thread do not work."
  },
  {
    "objectID": "posts/2021-03-16-gfortran-macos/index.html#so-how-does-one-now-install-gfortran-on-macos-these-days",
    "href": "posts/2021-03-16-gfortran-macos/index.html#so-how-does-one-now-install-gfortran-on-macos-these-days",
    "title": "gfortran support for R on macOS",
    "section": "So how does one now install gfortran on macOS these days?",
    "text": "So how does one now install gfortran on macOS these days?\nIt is likely that one will not need the workaround presented below in the future since it will probably be fixed in the R installer at some point (hopefully). In the meantime, the following helps:\n\nCreate a file ~/.R/Makevars (if it does not exist yet)\nAdd the following to ~/.R/Makevars\nFC      = usr/local/opt/gcc/bin/gfortran\nF77     = /usr/local/opt/gcc/bin/gfortran\nFLIBS   = -L/usr/local/opt/gcc/lib\nRestart R\nTest the changes by calling install.packages(\"cluster\", type = \"source\")\n\nThe output should look like this\n* installing *source* package â€˜clusterâ€™ ...\n** package â€˜clusterâ€™ successfully unpacked and MD5 sums checked\n** using staged installation\n** libs\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c clara.c -o clara.o\n/usr/local/opt/gcc/bin/gfortran -fno-optimize-sibling-calls  -fPIC  -Wall -g -O2  -c daisy.f -o daisy.o\n/usr/local/opt/gcc/bin/gfortran -fno-optimize-sibling-calls  -fPIC  -Wall -g -O2  -c dysta.f -o dysta.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c fanny.c -o fanny.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c init.c -o init.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c mona.c -o mona.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c pam.c -o pam.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c sildist.c -o sildist.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c spannel.c -o spannel.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c twins.c -o twins.o\nclang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o cluster.so clara.o daisy.o dysta.o fanny.o init.o mona.o pam.o sildist.o spannel.o twins.o -L/usr/local/opt/gcc/lib -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation\nld: warning: object file (daisy.o) was built for newer macOS version (11.2) than being linked (11.0)\nld: warning: object file (dysta.o) was built for newer macOS version (11.2) than being linked (11.0)\ninstalling to /Users/pjs/Library/R/4.0/library/00LOCK-cluster/00new/cluster/libs\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (cluster)\nCaution: After using this approach for a few days, I have seen issues with certain packages (e.g.Â hsdar). It is unclear if the issues trace back to the packages or the new way of using gfortran. You might want to re-think using this proposed approach and eventually manually install the linked standalone gfortran binary shown earlier in this post."
  },
  {
    "objectID": "posts/2021-03-16-gfortran-macos/index.html#notes",
    "href": "posts/2021-03-16-gfortran-macos/index.html#notes",
    "title": "gfortran support for R on macOS",
    "section": "Notes",
    "text": "Notes\n\nI am not sure about the ld: warning: object file (dysta.o) was built for newer macOS version (11.2) than being linked (11.0) warning, but it does not seem to have a practical impact.\nThis approach was tested with R 4.0.4, macOS 11.2.3 in March 2021.\nIf you still have the old gfortran cask installed, you may want to switch to the new approach as the cask is no longer being updated. Hence, you will run a very outdated gfortran at some point without noticing. You can remove the old cask with brew remove â€“ cask gfortran."
  },
  {
    "objectID": "posts/2020-11-02-jitsi-load-balanced/index.html",
    "href": "posts/2020-11-02-jitsi-load-balanced/index.html",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "",
    "text": "Jitsi Meet is a self-hosted Free and Open-Source Software (FOSS) video conferencing solution. During the recent COVID-19 pandemic, the project became quite popular, and many companies decided to host their own Jitsi instance.\nThere are many different ways to install and run Jitsi on a machine. A popular choice in the DevOps space is to use Docker via docker-compose, which was the method used in our scenario.\nAt cynkra, while we have been running our own Jitsi instance quite happily for some months, there was a slightly challenging task coming up: hosting a virtual meeting for approximately 100 participants."
  },
  {
    "objectID": "posts/2020-11-02-jitsi-load-balanced/index.html#the-challenge",
    "href": "posts/2020-11-02-jitsi-load-balanced/index.html#the-challenge",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "The Challenge",
    "text": "The Challenge\ncynkra actively supports the local Zurich R User Group. For one of their recent meetings, about 100 people RSVPâ€™ed.\nWhen browsing the load capabilities of a single Jitsi instance, we found that the stock setup begins to experience some challenges at around 35 people and fails at around 70 people. The limiting factor appears to be the â€œvideobridgeâ€. One solution is to add a second videobridge to the Jitsi instance. Jitsi can then distribute the load and should be able to host more than 100 people in a meeting.\nThe best approach is to deploy the second videobridge on a new instance to avoid running into CPU limitations on the main machine. While there is a guide in the Jitsi Wiki and a video that explains how to do it, many people still struggle (1, 2) to get this set up successfully.\nHence, we thought it would be valuable to take another, hopefully simple and understandable stab at explaining this task to the community."
  },
  {
    "objectID": "posts/2020-11-02-jitsi-load-balanced/index.html#load-balancing-jitsi-meet",
    "href": "posts/2020-11-02-jitsi-load-balanced/index.html#load-balancing-jitsi-meet",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "Load-balancing Jitsi Meet",
    "text": "Load-balancing Jitsi Meet\nIn the following, we will denote the main machine on which Jitsi runs, as MAIN. The second machine, which will only host a standalone videobridge, will be named BRIDGE.\n\nThe first step is to create a working installation on MAIN, following the official docker guide from the Jitsi developers. There is no need to use Docker. An installation on the host system will also work.\nAt this point, we assume that you already have installed Jitsi with SSL support at a fictitious domain.\nTo be able to connect to the XMPP server (managed by prosody) on MAIN from BRIDGE (details in point 4 below), port 5222 needs to be exported to the public. This requires adding\nports:\n  - \"5222:5222\"\nto the prosody section in docker-compose.yml and ensuring that the port is opened in the firewall (ufw allow 5222).\nOn BRIDGE, start with the same .env and docker-compose.yml as MAIN.\nIn docker-compose.yml, remove all services besides jvb. The videobridge will later connect to all services on MAIN.\nMake sure that JVB_AUTH_USER and JVB_AUTH_PASSWORD in .env are the same as on MAIN, otherwise the authentication will fail.\nOn BRIDGE in .env change XMPP_SERVER=xmpp.&lt;DOMAIN&gt; to XMPP_SERVER=&lt;DOMAIN&gt;.\nRun docker-compose up and observe what happens. The videobridge should successfully connect to &lt;DOMAIN&gt;. On MAIN, in docker logs jitsi_jicofo_1, an entry should appear denoting that a new videobridge was successfully connected.\nIt looks like\nJicofo 2020-10-23 19:01:52.173 INFO: [29] org.jitsi.jicofo.bridge.BridgeSelector.log() Added new videobridge: Bridge[jid=jvbbrewery@internal-muc.&lt;DOMAIN&gt;/d789de303e9b, relayId=null, region=null, stress=0.00]\nIf you have another videobridge running on MAIN, you should see that the identifier of the new videobridge (here d789de303e9b) is different to your main videobridge identifier. On BRIDGE, the logs should show something like\nINFO: Joined MUC: jvbbrewery@internal-muc.&lt;DOMAIN&gt;\nINFO: Performed a successful health check in PT0S. Sticky failure: false\n\nTo test that the external videobridge is active, one can disable the main videobridge (docker stop jitsi_jvb_1) and try to enable the camera in a new meeting."
  },
  {
    "objectID": "posts/2020-11-02-jitsi-load-balanced/index.html#troubleshooting-and-tips",
    "href": "posts/2020-11-02-jitsi-load-balanced/index.html#troubleshooting-and-tips",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "Troubleshooting and Tips",
    "text": "Troubleshooting and Tips\n\nIf you see something like SASLError using SCRAM-SHA-1: not-authorized, this indicates that the JVB_AUTH_PASSWORD and/or JVB_AUTH_USER on BRIDGE are incorrect.\nIf you change something in .env of MAIN, you need to delete all config folders before running docker-compose up again. Otherwise changes wonâ€™t be picked up even when force destroying the containers.\nDo not run gen-passwords.sh multiple times as JVB_AUTH_PASSWORD and BRIDGE will not be able to connect anymore.\nUnrelated to the content above: if you want to create a user manually for your instance, the following command might be helpful:\ndocker exec jitsi_prosody_1 prosodyctl --config /config/prosody.cfg.lua register &lt;USER&gt; &lt;DOMAIN&gt; \"&lt;PASSWORD&gt;\""
  },
  {
    "objectID": "posts/2018-05-15-tsbox/index.html",
    "href": "posts/2018-05-15-tsbox/index.html",
    "title": "Time series of the world, unite!",
    "section": "",
    "text": "The R ecosystem knows a ridiculous number of time series classes. So, I decided to create a new universal standard that finally covers everyoneâ€™s use caseâ€¦\nOk, just kidding!\ntsbox, just realeased on CRAN, provides a set of tools that are agnostic towards existing time series classes. It is built around a set of converters, which convert time series stored as ts, xts, data.frame, data.table, tibble, zoo, tsibble or timeSeries to each other.\nTo install the stable version from CRAN:\ninstall.packages(\"tsbox\")\nTo get an idea how easy it is to switch from one class to another, consider this:\nlibrary(tsbox)\nx.ts &lt;- ts_c(mdeaths, fdeaths)\nx.xts &lt;- ts_xts(x.ts)\nx.df &lt;- ts_df(x.xts)\nx.tbl &lt;- ts_tbl(x.df)\nx.dt &lt;- ts_tbl(x.tbl)\nx.zoo &lt;- ts_zoo(x.dt)\nx.tsibble &lt;- ts_tsibble(x.zoo)\nx.timeSeries &lt;- ts_timeSeries(x.tsibble)\nWe jump form good old ts objects toxts, store our time series in various data frames and convert them to some highly specialized time series formats.\n\ntsbox is class-agnostic\nBecause these converters work nicely, we can use them to make functions class-agnostic. If a class-agnostic function works for one class, it works for all:\nts_scale(x.ts)\nts_scale(x.xts)\nts_scale(x.df)\nts_scale(x.dt)\nts_scale(x.tbl)\nts_scale normalizes one or multiple series, by subtracting the mean and dividing by the standard deviation. It works like a â€˜genericâ€™ function: You can apply it on any time series object, and it will return an object of the same class as its input.\nSo, whether we want to smooth, scale, differentiate, chain-link, forecast, regularize, or seasonally adjust a series, we can use the same commands to whatever time series at hand. tsbox offers a comprehensive toolkit for the basics of time series manipulation. Here are some additional operations:\nts_pc(x.ts)                 # percentage change rates\nts_forecast(x.xts)          # forecast, by exponential smoothing\nts_seas(x.df)               # seasonal adjustment, by X-13\nts_frequency(x.dt, \"year\")  # convert to annual frequency\nts_span(x.tbl, \"-1 year\")   # limit time span to final year\n\n\ntsbox is frequency-agnostic\nThere are many more. Because they all start with ts_, you can use auto-complete to see whatâ€™s around. Most conveniently, there is a time series plot function that works for all classes and frequencies:\nts_plot(\n  `Airline Passengers` = AirPassengers,\n  `Lynx trappings` = ts_df(lynx),\n  `Deaths from Lung Diseases` = ts_xts(fdeaths),\n  title = \"Airlines, trappings, and deaths\",\n  subtitle = \"Monthly passengers, annual trappings, monthly deaths\"\n)\n\n\n\nwww.dataseries.org\n\n\nThere is also a version that uses ggplot2 and has the same syntax.\n\n\nTime series in data frames\nYou may have wondered why we treated data frames as a time series class. The spread of dplyr and data.table has given data frames a boost and made them one of the most popular data structures in R. So, storing time series in a data frame is an obvious consequence. And even if you donâ€™t intend to keep time series in data frames, this is still the format in which you import and export your data. tsbox makes it easy to switch from data frames to time series and back.\n\n\nMake existing functions class-agnostic\ntsbox includes tools to make existing functions class-agnostic. To do so, the ts_ function can be used to wrap any function that works with time series. For a function that works on \"ts\" objects, this is as simple as that:\nts_rowsums &lt;- ts_(rowSums)\nts_rowsums(ts_c(mdeaths, fdeaths))\nNote that ts_ returns a function, which can be used with or without a name.\nIn case you are wondering, tsbox uses data.table as a backend and makes use of its incredibly efficient reshaping facilities, its joins, and rolling joins. And thanks to anytime, tsbox will be able to recognize almost any date format without manual intervention.\nSo, if youâ€™ve been struggling with Râ€™s time series class, we hope that you found this blog helpful.\nWebsite: www.tsbox.help"
  },
  {
    "objectID": "posts/2020-08-25-git-multiple-identities/index.html",
    "href": "posts/2020-08-25-git-multiple-identities/index.html",
    "title": "Maintaining multiple identities with Git",
    "section": "",
    "text": "When committing to a Git repository related to my consulting work, I must use my company e-mail address, kirill@cynkra.com. Not so much for my open-source work â€“ for this, I prefer to use other e-mail addresses, like krlmlr+r@mailbox.org . (For example, Travis CI sends notification e-mails to the committerâ€™s e-mail address, and I have set up filtering for that other address.)\n\n\n\nPhoto by Carson Arias\n\n\n\nHaving to configure the e-mail address for each repository separately eventually gets annoying. Instead, Iâ€™d rather have all repos within a specific subdirectory use particular e-mail address.\nAll my Git repos live in ~/git. Subdirectories R and cynkra, contain R packages and repos related to consulting, respectively. To achieve the desired setup, I edit my ~/.gitconfig with the following entry:\n[includeIf \"gitdir:git/**\"]\n    path = git/.gitconfig\nThis ensures that all repos in the git directory use the git/.gitconfig file in addition to the main configuration. That file contains the following:\n[includeIf \"gitdir:R/**\"]\n    path = R/.gitconfig\n[includeIf \"gitdir:cynkra/**\"]\n    path = cynkra/.gitconfig\nFinally, in ~/git/R/.gitconfig and ~/git/cynkra/.gitconfig, I configure the e-mail addresses I want to use for all repos pertaining to R and cynkra, respectively.\n[user]\n    email = ...\nI verify the setup with git config -l | grep user. Indeed, cynkra repos use the cynkra e-mail address. VoilÃ !\nThe above approach requires a recent-ish version of git- version 2.14 or later should suffice. Read more about conditional includes."
  },
  {
    "objectID": "posts/2022-04-19-seasonal-1.9/index.html",
    "href": "posts/2022-04-19-seasonal-1.9/index.html",
    "title": "seasonal 1.9: Accessing composite output",
    "section": "",
    "text": "seasonal is an easy-to-use and full-featured R interface to X-13ARIMA-SEATS, the seasonal adjustment software developed by the United States Census Bureau. The latest CRAN version of seasonal fixes several bugs and makes it easier to access output from multiple objects. See here for a complete list of changes.\n\nPhoto by Aaron Burden\n\nseas() is the core function of the seasonal package. By default, seas() calls the automatic procedures of X-13ARIMA-SEATS to perform a seasonal adjustment that works well in most circumstances:\nlibrary(seasonal)\nseas(AirPassengers)\nFor a more detailed introduction, read our article in the Journal of Statistical Software.\nMultiple series adjustment\nThe previous version has introduced the adjustment of multiple series in a single call to seas(). This has removed the need for loops or lapply() in such cases and finally brought the composite spec to seasonal.\nAs Brian Monsell pointed out, this was not enough to access the output from the composite spec. The latest CRAN version fixes this problem.\nMultiple adjustments can be performed by supplying multiple time series as an \"mts\" object:\n\nlibrary(seasonal)\nm0 &lt;- seas(cbind(fdeaths, mdeaths), x11 = \"\")\nfinal(m0)\n\n          fdeaths  mdeaths\nJan 1974 614.1235 1598.740\nFeb 1974 542.3500 1492.127\nMar 1974 613.5029 1443.238\nApr 1974 591.5725 1694.643\nMay 1974 607.4970 1696.021\nJun 1974 543.8415 1558.886\nJul 1974 597.0745 1663.176\nAug 1974 587.0533 1623.498\nSep 1974 588.2693 1741.394\nOct 1974 735.6666 1735.516\nNov 1974 602.0218 1665.590\nDec 1974 496.3985 1394.097\nJan 1975 564.0055 1560.605\nFeb 1975 591.0320 1708.763\nMar 1975 585.7739 1652.994\nApr 1975 581.5294 1671.265\nMay 1975 537.8055 1588.605\nJun 1975 584.3284 1600.979\nJul 1975 566.4872 1541.099\nAug 1975 617.1197 1623.445\nSep 1975 516.4781 1521.497\nOct 1975 559.0481 1577.200\nNov 1975 561.6315 1602.659\nDec 1975 580.9778 1569.692\nJan 1976 519.8106 1477.855\nFeb 1976 882.3725 2180.616\nMar 1976 674.5057 1744.114\nApr 1976 467.4502 1366.628\nMay 1976 509.7854 1344.809\nJun 1976 553.5233 1434.662\nJul 1976 503.2795 1447.952\nAug 1976 494.2373 1383.932\nSep 1976 529.1840 1453.496\nOct 1976 570.4128 1435.912\nNov 1976 590.4285 1540.551\nDec 1976 587.0971 1572.631\nJan 1977 583.2427 1607.153\nFeb 1977 498.9514 1287.403\nMar 1977 500.4632 1306.324\nApr 1977 569.2076 1685.581\nMay 1977 565.6470 1405.231\nJun 1977 509.3196 1432.968\nJul 1977 548.4062 1414.216\nAug 1977 523.6985 1444.945\nSep 1977 563.3014 1402.720\nOct 1977 495.6653 1427.458\nNov 1977 453.9859 1307.828\nDec 1977 502.3045 1268.618\nJan 1978 535.2658 1415.724\nFeb 1978 633.7605 1790.002\nMar 1978 559.2936 1469.883\nApr 1978 485.5062 1343.715\nMay 1978 590.4080 1509.166\nJun 1978 574.4467 1464.288\nJul 1978 571.2263 1428.398\nAug 1978 542.3579 1424.622\nSep 1978 551.2099 1422.428\nOct 1978 557.6905 1399.404\nNov 1978 479.8979 1199.762\nDec 1978 550.4253 1397.023\nJan 1979 548.9834 1557.853\nFeb 1979 576.9922 1425.717\nMar 1979 549.3468 1393.788\nApr 1979 546.4590 1449.784\nMay 1979 525.9881 1359.843\nJun 1979 550.2481 1330.113\nJul 1979 533.7558 1373.156\nAug 1979 566.6884 1381.653\nSep 1979 552.6500 1377.175\nOct 1979 533.9571 1337.640\nNov 1979 557.9707 1414.101\nDec 1979 475.5049 1038.506\n\n\nThis performs two seasonal adjustments, one for fdeaths and one for mdeaths. The vignette on multiple adjustments describes how to specify options for individual series.\nAccessing composite output\nThe composite argument is a list with an X-13 specification applied to the aggregated series:\n\nm1 &lt;- seas(\n  cbind(mdeaths, fdeaths),\n  composite = list(),\n  series.comptype = \"add\"\n)\n\nWith version 1.9 can now use out() to access the output of the composite spec:\nout(m1)\nWe can also use series(), e.g., to access the final, indirectly adjusted series via the composite spec (see ?series for all available series):\n\nseries(m1, \"composite.indseasadj\")\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1974 2172.614 2053.613 2057.679 2284.821 2260.974 2105.191 2240.895 2185.517\n1975 2098.251 2298.581 2213.878 2256.802 2111.628 2181.738 2098.883 2219.083\n1976 1969.128 3078.359 2373.028 1846.802 1851.167 1983.231 1943.369 1872.025\n1977 2132.860 1807.832 1795.898 2262.793 1957.817 1940.262 1949.784 1953.665\n1978 1908.154 2431.050 2007.252 1830.715 2077.654 2033.120 1987.875 1956.487\n1979 2061.627 1997.557 1925.365 1984.834 1885.778 1882.208 1903.203 1937.474\n          Sep      Oct      Nov      Dec\n1974 2296.345 2395.347 2291.835 2013.749\n1975 2024.988 2100.543 2181.495 2234.826\n1976 1967.742 1973.567 2151.597 2199.389\n1977 1946.642 1894.848 1807.119 1808.051\n1978 1959.824 1928.748 1724.336 1966.847\n1979 1925.618 1846.713 1991.679 1517.027"
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html",
    "href": "posts/2018-05-01-dbi-2/index.html",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "",
    "text": "The â€œEstablishing DBIâ€ project, funded by the R consortium, started about a year ago. It includes the completion of two new backends, RPostgres and RMariaDB, and quite a few interface extensions and specifications.\nLearn more about DBI, Râ€™s database interface, on https://r-dbi.org.\nThis blog post showcases only the visible changes, a substantial amount of work went into extending the DBI specification and making the three open-source database backends compliant to it. After describing the release of the two new backends RMariaDB and RPostgres, Iâ€™ll be discussing the following improvements:\nI conclude with an outlook on things left to do."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#release-of-rpostgres-and-rmariadb",
    "href": "posts/2018-05-01-dbi-2/index.html#release-of-rpostgres-and-rmariadb",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Release of RPostgres and RMariaDB",
    "text": "Release of RPostgres and RMariaDB\nThe DBI specification has been formulated in the preceding R consortium project, â€œImproving DBIâ€. It is both an automated test suite and a human-readable description of behavior, implemented in the DBItest package. For this project, I extended this specification and could also use it to implement RPostgres and RMariaDB: for once, test-driven development was pure pleasure because the tests were already there!\nI took over maintenance of the RPostgres and RMariaDB packages, which are complete rewrites of the RPostgreSQL and RMySQL packages, respectively. These packages use C++ (with Rcpp) as the glue between R and the native database libraries. A reimplementation and release under a different name has made it much easier to fully conform to the DBI specification: only listing temporary tables and casting to blob or character is not supported by RMariaDB (due to a limitation of the DBMS), all other parts of the specification are fully covered.\nProjects that use RPostgreSQL or RMySQL can continue to do so, or switch to the new backends at their own pace (which likely requires some changes to the code). For new projects I recommend RPostgres or RMariaDB to take advantage of the thorougly tested codebases and of the consistency across backends."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#schema-support",
    "href": "posts/2018-05-01-dbi-2/index.html#schema-support",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Schema support",
    "text": "Schema support\nConsistent access of tables in database schemas was planned for the â€œImproving DBIâ€ project already, but I have implemented it only recently. It felt safer to see how the interface works on three backends, as opposed to implementing it for just RSQLite and then perhaps having to adapt it.\nThe new Id() function constructs identifiers. All arguments must be named, yet DBI doesnâ€™t specify the argument names because DBMS have an inconsistent notion of namespaces. The objects returned by Id() are â€œdumbâ€, they gain meaning only when used in methods such as dbQuoteIdentifier() or dbWriteTable().\nFor listing database objects in schemas, the new dbListObjects() generic can be used. It returns a data frame that contains identifiers (like those created by the Id() function) and a flag that indicates if the identifier is complete (i.e., pointing to a table or view) or a prefix. Incomplete identifiers can be passed to dbListObjects() again, which allows traversing the tree of database objects.\nThe following example assumes a schema my_schema. A table named my_table is created in this schema, objects are listed, and the table is read again.\nlibrary(RPostgres)\npg_conn &lt;- dbConnect(Postgres())\n\ntable_name &lt;- Id(schema = \"my_schema\", table = \"my_table\")\ntable_name\n\n## &lt;Id&gt; schema = my_schema, table = my_table\n\ndata &lt;- data.frame(a = 1:3, b = letters[1:3])\ndbWriteTable(pg_conn, table_name, data)\n\ndbListObjects(pg_conn)\n\n##                               table is_prefix\n## 1    &lt;Id&gt; table = geography_columns     FALSE\n## 2     &lt;Id&gt; table = geometry_columns     FALSE\n## 3      &lt;Id&gt; table = spatial_ref_sys     FALSE\n## 4       &lt;Id&gt; table = raster_columns     FALSE\n## 5     &lt;Id&gt; table = raster_overviews     FALSE\n## 6             &lt;Id&gt; table = topology     FALSE\n## 7                &lt;Id&gt; table = layer     FALSE\n## 8                 &lt;Id&gt; table = temp     FALSE\n## 9            &lt;Id&gt; schema = topology      TRUE\n## 10          &lt;Id&gt; schema = my_schema      TRUE\n## 11 &lt;Id&gt; schema = information_schema      TRUE\n## 12         &lt;Id&gt; schema = pg_catalog      TRUE\n## 13             &lt;Id&gt; schema = public      TRUE\n\ndbListObjects(\n  pg_conn,\n  prefix = Id(schema = \"my_schema\")\n)\n\n##                                       table is_prefix\n## 1 &lt;Id&gt; schema = my_schema, table = my_table     FALSE\n\ndbReadTable(pg_conn, table_name)\n\n##   a b\n## 1 1 a\n## 2 2 b\n## 3 3 c\nIn addition to dbReadTable() and dbWriteTable(), also dbExistsTable() and dbRemoveTable() and the new dbCreateTable() and dbAppendTable() (see below) support an Id() object as table name. The dbQuoteIdentifier() method converts these objects to SQL strings. Some operations (e.g.Â checking if a table exists) require the inverse, the new dbUnquoteIdentifier() generic takes care of converting valid SQL identifiers to (a list of) Id() objects:\nquoted &lt;- dbQuoteIdentifier(pg_conn, table_name)\nquoted\n\n## &lt;SQL&gt; \"my_schema\".\"my_table\"\n\ndbUnquoteIdentifier(pg_conn, quoted)\n\n## [[1]]\n## &lt;Id&gt; schema = my_schema, table = my_table\nThe new methods work consistently across backends - only RSQLite is currently restricted to the default schema. (Schemas in RSQLite are created by attaching another database, this use case seemed rather exotic but can be supported with the new infrastructure.)"
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#quoting-literal-values",
    "href": "posts/2018-05-01-dbi-2/index.html#quoting-literal-values",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Quoting literal values",
    "text": "Quoting literal values\nWhen working on the database backends, it has become apparent that quoting strings and identifiers isnâ€™t quite enough. Now there is a way to quote arbitrary values, that is, convert them to a string that can be pasted into an SQL query:\nlibrary(RSQLite)\nsqlite_conn &lt;- dbConnect(SQLite())\n\nlibrary(RMariaDB)\nmariadb_conn &lt;- dbConnect(MariaDB(), dbname = \"test\")\n\ndbQuoteLiteral(sqlite_conn, 1.5)\n\n## &lt;SQL&gt; 1.5\n\ndbQuoteLiteral(mariadb_conn, 1.5)\n\n## &lt;SQL&gt; 1.5\n\ndbQuoteLiteral(pg_conn, 1.5)\n\n## &lt;SQL&gt; 1.5::float8\n\ndbQuoteLiteral(mariadb_conn, Sys.time())\n\n## &lt;SQL&gt; '20180501204025'\n\ndbQuoteLiteral(pg_conn, Sys.time())\n\n## &lt;SQL&gt; '2018-05-01 22:40:25'::timestamp\nThe default implementation works for ANSI SQL compliant DBMS, the method for RPostgres takes advantage of the :: casting operator as seen in the examples."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#more-fine-grained-creation-of-tables",
    "href": "posts/2018-05-01-dbi-2/index.html#more-fine-grained-creation-of-tables",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "More fine-grained creation of tables",
    "text": "More fine-grained creation of tables\nDBI supports storing data frames as tables in the database via dbWriteTable(). This operation consists of multiple steps:\n\nChecking if a table of this name exists, if yes:\n\nIf overwrite = TRUE, removing the table\nIf not, throwing an error\n\nCreating the table with the correct field structure\nPreparing the data for writing\nWriting the data\n\nTo reduce complexity and allow for more options without cluttering the argument list of dbWriteTable(), DBI now provides generics for the individual steps:\n\nThe existing dbRemoveTable() generic has been extended with temporary and fail_if_missing arguments. Setting temporary = TRUE makes sure that only temporaries are removed. By default, trying to remove a table that doesnâ€™t exist fails, setting fail_if_missing = FALSE changes this behavior to a silent success.\nThe new dbCreateTable() generic accepts a data frame or a character vector of DBMS data types and creates a table in the database. It builds upon the existing sqlCreateTable() generic and also supports the temporary argument. If a table by that name already exists, an error is raised.\nThe new dbAppendTable() generic uses a prepared statement (created via sqlAppendTableTemplate()) to efficiently insert rows into the database. This avoids the internal overhead of converting values to SQL literals.\n\nThe following example shows the creation and population of a table with the new methods.\ntable_name\n\n## &lt;Id&gt; schema = my_schema, table = my_table\n\ndbRemoveTable(pg_conn, table_name, fail_if_missing = FALSE)\n\ndbCreateTable(pg_conn, table_name, c(a = \"int8\", b = \"float8\"))\n\ndbAppendTable(pg_conn, table_name, data.frame(a = 1:3, b = 1:3))\n\n## [1] 3\n\nstr(dbReadTable(pg_conn, table_name))\n\n## 'data.frame':    3 obs. of  2 variables:\n##  $ a:integer64 1 2 3\n##  $ b: num  1 2 3\nThe dbWriteTable() methods in the three backends have been adapted to use the new methods."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#support-for-64-bit-integers",
    "href": "posts/2018-05-01-dbi-2/index.html#support-for-64-bit-integers",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Support for 64-bit integers",
    "text": "Support for 64-bit integers\nAs seen in the previous example, 64-bit integers can be read from the database. The three backends RSQLite, RPostgres and RMariaDB now also support writing 64-bit integers via the bit64 package:\ndata &lt;- data.frame(a = bit64::as.integer64(4:6), b = 4:6)\ndbAppendTable(pg_conn, table_name, data)\n\n## [1] 3\n\nstr(dbReadTable(pg_conn, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a:integer64 1 2 3 4 5 6\n##  $ b: num  1 2 3 4 5 6\nBecause R still lacks support for native 64-bit integers, the bit64 package feels like the best compromise: the returned values can be computed on, or coerced to integer, numeric or even character depending on the application. In some cases, it may be useful to always coerce. This is where the new bigint argument to dbConnect() helps:\npg_conn_int &lt;- dbConnect(Postgres(), bigint = \"integer\")\nstr(dbReadTable(pg_conn_int, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a: int  1 2 3 4 5 6\n##  $ b: num  1 2 3 4 5 6\n\npg_conn_num &lt;- dbConnect(Postgres(), bigint = \"numeric\")\nstr(dbReadTable(pg_conn_num, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a: num  1 2 3 4 5 6\n##  $ b: num  1 2 3 4 5 6\n\npg_conn_chr &lt;- dbConnect(Postgres(), bigint = \"character\")\nstr(dbReadTable(pg_conn_chr, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a: chr  \"1\" \"2\" \"3\" \"4\" ...\n##  $ b: num  1 2 3 4 5 6\nThe bigint argument works consistently across the three backends RSQLite, RPostgres and RMariaDB, the DBI specification contains a test for and a description of the requirements."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#geometry-columns",
    "href": "posts/2018-05-01-dbi-2/index.html#geometry-columns",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Geometry columns",
    "text": "Geometry columns\nPostgreSQL has support for user-defined data types, this is used e.g.Â by PostGIS to store spatial data. Before, user-defined data types were returned as character values, with a warning. Thanks to a contribution by Etienne B. Racine:\n\nthe warnings are gone,\nthe user-defined data type is now stored in an attribute of the column in the data frame,\ndetails on columns with user-defined data types are available in dbColumnInfo().\n\n\ndbCreateTable(\n  pg_conn,\n  \"geom_test\",\n  c(id = \"int4\", geom = \"geometry(Point, 4326)\")\n)\n\ndata &lt;- data.frame(\n  id = 1,\n  geom = \"SRID=4326;POINT(-71.060316 48.432044)\",\n  stringsAsFactors = FALSE\n)\ndbAppendTable(pg_conn, \"geom_test\", data)\n\n## [1] 1\n\nstr(dbReadTable(pg_conn, \"geom_test\"))\n\n## 'data.frame':    1 obs. of  2 variables:\n##  $ id  : int 1\n##  $ geom:Class 'pq_geometry'  chr \"0101000020E61000003CDBA337DCC351C06D37C1374D374840\"\n\nres &lt;- dbSendQuery(pg_conn, \"SELECT * FROM geom_test\")\ndbColumnInfo(res)\n\n##   name      type   .oid .known .typname\n## 1   id   integer     23   TRUE     int4\n## 2 geom character 101529  FALSE geometry\n\ndbClearResult(res)\nSpecial support for geometry columns is currently available only in RPostgres."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#duplicate-column-names",
    "href": "posts/2018-05-01-dbi-2/index.html#duplicate-column-names",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Duplicate column names",
    "text": "Duplicate column names\nThe specification has been extended to disallow duplicate, empty, or NA column names. The deduplication used by our three backends is similar to that used by tibble::set_tidy_names(), but the DBI specification does not require any particular deduplication mechanism. Syntactic names arenâ€™t required either:\ndbGetQuery(sqlite_conn, \"SELECT 1, 2, 3\")\n\n##   1 2 3\n## 1 1 2 3\n\ndbGetQuery(sqlite_conn, \"SELECT 1 AS a, 2 AS a, 3 AS `a..2`\")\n\n##   a a..2 a..3\n## 1 1    2    3\n\ndbGetQuery(mariadb_conn, \"SELECT 1, 2, 3\")\n\n##   1 2 3\n## 1 1 2 3\n\ndbGetQuery(mariadb_conn, \"SELECT 1 AS a, 2 AS a, 3 AS `a..2`\")\n\n##   a a..2 a..3\n## 1 1    2    3\n\ndbGetQuery(pg_conn, \"SELECT 1, 2, 3\")\n\n##   ?column? ?column?..2 ?column?..3\n## 1        1           2           3\n\ndbGetQuery(pg_conn, 'SELECT 1 AS a, 2 AS a, 3 AS \"a..2\"')\n\n##   a a..2 a..3\n## 1 1    2    3"
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#helpers",
    "href": "posts/2018-05-01-dbi-2/index.html#helpers",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Helpers",
    "text": "Helpers\nTwo little helper generics have been added.\nThe new dbIsReadOnly() generic (contributed by Anh Le) should return TRUE for a read-only connection. This is not part of the specification yet.\nThe dbCanConnect() tests a set of connection parameters. The default implementation simply connects and then disconnects upon success. For DBMS that can provide more efficient methods of checking connectivity, a lighter-weight implementation of this method may give a better experience.\nNone of the three backends currently provide specialized implementations for these generics."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#code-reuse",
    "href": "posts/2018-05-01-dbi-2/index.html#code-reuse",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Code reuse",
    "text": "Code reuse\nI have made some efforts to extract common C++ classes for assembling data frames and prepare them for reuse. The C++ source code for the three backends contains files prefixed with Db, these are almost identical across the backends. The planned packaging into the RKazam package had to yield to higher-priority features described above.\nThe situation in the R code is similar: I have found myself copy-pasting code from one backend into another because I didnâ€™t feel itâ€™s ready (or standardized enough) to be included in the DBI package.\nFor both use cases, a code reuse strategy based on copying/updating template files or reconciling files may be more robust than the traditional importing mechanisms offered by R."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#outlook",
    "href": "posts/2018-05-01-dbi-2/index.html#outlook",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Outlook",
    "text": "Outlook\nThe upcoming CRAN release of DBI, DBItest and the three backends RSQLite, RMariaDB, and RPostgres are important milestones. Stability is important when more and more users and projects use the new backends. Nevertheless, I see quite a few potential improvements that so far were out of scope of the â€œImproving DBIâ€ and â€œEstablishing DBIâ€ projects:\n\nSupport running the test suite locally, to validate adherence to DBI for a particular installation.\nConsistent fast data import.\nConsistent query placeholders (currently $1 for RPostgres and ? for many other backends).\nSupport for arbitrary data types via hooks.\nAssistance with installation problems on specific architectures, or connectivity problems with certain databases, or other specific issues.\nRework the internal architecture of DBItest to simplify locating test failures.\nImprove the https://r-dbi.org website.\nNon-blocking queries.\n\nI have submitted another proposal to the R Consortium, hoping to receive support with these and other issues."
  },
  {
    "objectID": "posts/2018-05-01-dbi-2/index.html#acknowledgments",
    "href": "posts/2018-05-01-dbi-2/index.html#acknowledgments",
    "title": "Done â€œEstablishing DBIâ€!?",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nIâ€™d like to thank the R Consortium for their generous financial support. Many thanks to the numerous contributors who helped make the past two projects a success.\nThis post appeared previously on https://r-dbi.org."
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html",
    "href": "posts/2025-02-07-r-agents/index.html",
    "title": "Playing with AI Agents in R",
    "section": "",
    "text": "Itâ€™s local LLM time! ğŸ¥³ What an adventure it has been since I first started exploring local LLMs. With the introduction of various new Llama models, we now have impressive small and large models that run seamlessly on consumer hardware. With deepseek R1, we have access to remarkably good MIT-licensed reasoning models that rival the top models in the industry.\nAnd now, weâ€™ve got a fully-specced M4 Mac Mini for our office, which runs all of this like a charm.\nIn this post, I will explore how we can use R to run agent models through Ollama."
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#intro",
    "href": "posts/2025-02-07-r-agents/index.html#intro",
    "title": "Playing with AI Agents in R",
    "section": "",
    "text": "Itâ€™s local LLM time! ğŸ¥³ What an adventure it has been since I first started exploring local LLMs. With the introduction of various new Llama models, we now have impressive small and large models that run seamlessly on consumer hardware. With deepseek R1, we have access to remarkably good MIT-licensed reasoning models that rival the top models in the industry.\nAnd now, weâ€™ve got a fully-specced M4 Mac Mini for our office, which runs all of this like a charm.\nIn this post, I will explore how we can use R to run agent models through Ollama."
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#what-is-an-agent",
    "href": "posts/2025-02-07-r-agents/index.html#what-is-an-agent",
    "title": "Playing with AI Agents in R",
    "section": "What is an agent?",
    "text": "What is an agent?\nUnlike normal chats, agents can use tools to do tasks. Tools are pieces of software that can do anything, and it is up to the user to construct the agentic workflow they want. With R, we have a powerful language to configure these workflows and tell the tools what to do.\nA typical agentic workflow looks like this:\n\nYou tell an LLM what you want in plain English and the tools it has available\nThe LLM figures out what tools it needs to use\nThe agent uses the tools to get the job done\nIterate, improve or continue with other tasks\nLet the LLM explain what it did"
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#r-and-ollama",
    "href": "posts/2025-02-07-r-agents/index.html#r-and-ollama",
    "title": "Playing with AI Agents in R",
    "section": "R and Ollama",
    "text": "R and Ollama\nFor the following experiments with agents, Iâ€™m using plain R and Ollama (which you have to install first, but itâ€™s straightforward.)\nI know that there is the R package ellmer, and you can do many things that I am doing here with it. While ellmer is a great package, I prefer to work directly with the Ollama API. In my opinion, this gives us more transparency into the agent mechanics.\nMy experiments with agents in R can be found in the ragent repository on GitHub. To install the package, use:\n\nremotes::install_github(\"cynkra/ragent\")\n\nI will use diagrams to show the conversation flow and the tools that are used."
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#a-simple-chat",
    "href": "posts/2025-02-07-r-agents/index.html#a-simple-chat",
    "title": "Playing with AI Agents in R",
    "section": "A simple chat",
    "text": "A simple chat\nGetting started is straightforward. In llm_chat(), Iâ€™ve used the httr2 package to wrap the Ollama API. You can use the function to perform normal chats:\n\nlibrary(ragent)\nllm_chat(\"What is the capital of France?\")\n#&gt; Paris is the capital of France.\n\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    participant User\n    participant LLM\n\n    User-&gt;&gt;LLM: Question\n    Note over LLM: No tools needed\n    LLM--&gt;&gt;User: Direct response\n\n\n\n\n\n\nNo surprise here: The user asks a question (1) and the LLM answers (2). By default, I am using the llama3.2:3b model, because itâ€™s fast and good enough for this demo. But I can change it to any other, more powerful model that Ollama supports. E.g,:\n\nllm_chat(\"What is the capital of France?\", model = \"deepseek-r1:14b\")"
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#structured-output",
    "href": "posts/2025-02-07-r-agents/index.html#structured-output",
    "title": "Playing with AI Agents in R",
    "section": "Structured output",
    "text": "Structured output\nThe ollama API supports structured output. We can use this to tell the LLM to return a specific format. For example:\n\nllm_chat(\"What is the capital of France?\", format = list(\n  type = \"object\",\n  properties = list(\n    country = list(type = \"string\"),\n    capital = list(type = \"string\")\n  ),\n  required = c(\"country\", \"capital\")\n))\n#&gt; $country\n#&gt; [1] \"France\"\n#&gt;\n#&gt; $capital\n#&gt; [1] \"Paris\"\n\nThis structured response makes it easy to process the output programmatically in R, as it will be automatically converted to a list with named elements. It is often a good idea to add a system prompt to guide the LLM to the desired output."
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#a-agent-that-can-calculate",
    "href": "posts/2025-02-07-r-agents/index.html#a-agent-that-can-calculate",
    "title": "Playing with AI Agents in R",
    "section": "A agent that can calculate",
    "text": "A agent that can calculate\nThis is now a sweet setup for tinkering. We can use R to work with this output to do more complex things.\nNote that LLMs are not really good at calculating. The small Llama model that I am using here messes this up:\n\nllm_chat(\"What is 2.111^2.111\")\n#&gt; [1] \"To calculate this, we can use the formula for squaring a binomial:\\n\\n(a + b)^2 = a^2 + 2ab + b^2\\n\\nIn this case, a = 2 and b = 1.111.\\n\\nSo, \\n\\n(2 + 1.111)^2 = \\n(3.111)^2 = 9.377226\\n2*3.111 = 6.222442\\n6.222442 + 9.377226 = 15.600668\\n\\nTherefore, 2.111^2.111 â‰ˆ 15.60\"\n\nBut we can tell the LLM to use a calculator tool to do the calculation in R. The result looks like this:\n\nagent_calculator(\"What is 2.111^2.111\")\n#&gt; **Calculation Result:** 2.111^2.111 = 4.84166414903285\n\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    participant User\n    participant Agent\n    participant Calculator\n\n    User-&gt;&gt;Agent: Math question\n    Note over Agent: Analyzes question&lt;br/&gt;Decides to use calculator\n    Agent-&gt;&gt;Calculator: Structured calculation request\n    Calculator--&gt;&gt;Agent: Numerical result\n    Note over Agent: Formats response\n    Agent--&gt;&gt;User: Explained calculation\n\n\n\n\n\n\nMuch better! This is an agent that:\n\nAnalyzes the question\nDecides to use the calculator tool\nUses the calculator tool to calculate the result in R\nReturns the response to the user"
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#a-smarter-rag",
    "href": "posts/2025-02-07-r-agents/index.html#a-smarter-rag",
    "title": "Playing with AI Agents in R",
    "section": "A smarter RAG",
    "text": "A smarter RAG\nA simple RAG (Retrieval Augmented Generation) enhances LLM responses by first searching a knowledge base for relevant information. But our improved RAG goes a step further: after the initial search, the LLM formulates additional questions to gather more context, leading to more comprehensive answers.\nIn our smarter RAG, we build an agent that:\n\nSearch the knowledge base for relevant passages (2)\nAsk the LLM to provide additional questions to search for (5)\nSearch the knowledge base for the additional questions (7)\nUse the context to answer the question (10)\n\n\n\n\n\n\nsequenceDiagram\n    autonumber\n    participant User\n    participant Agent\n    participant RAG\n    participant Docs\n\n    User-&gt;&gt;Agent: Question about docs\n    Note over Agent: Analyzes question\n    Agent-&gt;&gt;RAG: Initial search\n    RAG-&gt;&gt;Docs: Search in directory\n    Docs--&gt;&gt;RAG: Initial passages\n    RAG--&gt;&gt;Agent: Initial context\n    Note over Agent: Formulates follow-up&lt;br/&gt;questions for context\n    Agent-&gt;&gt;RAG: Additional searches\n    RAG-&gt;&gt;Docs: Search with new questions\n    Docs--&gt;&gt;RAG: Additional passages\n    RAG--&gt;&gt;Agent: Combined context\n    Note over Agent: Synthesizes all&lt;br/&gt;information\n    Agent--&gt;&gt;User: Complete response&lt;br/&gt;with citations\n\n\n\n\n\n\n\nagent_rag(\"What is ggplot2 used for?\", dir = \"docs/\")\n#&gt; Based on the documentation:\n#&gt; ggplot2 is a core package of the tidyverse used for visualization.\n#&gt; [Source: tidyverse.md]\n\nThis is just the beginning. We can build much more complex agents. For example, we could create a general agent that combines both the calculator and RAG capabilities. This agent would analyze each question and automatically choose the right approach - either searching the knowledge base or performing calculations."
  },
  {
    "objectID": "posts/2025-02-07-r-agents/index.html#conclusion",
    "href": "posts/2025-02-07-r-agents/index.html#conclusion",
    "title": "Playing with AI Agents in R",
    "section": "Conclusion",
    "text": "Conclusion\nR is a powerful language for building AI agents. Its functional nature makes tools feel like Lego bricks that we can combine. The way R handles lists and data frames fits nicely with the JSON returned from the API, and the rich R ecosystem provides everything we need to build powerful agents. But what I found most useful is Râ€™s interactive workflow that allows me to watch and debug the workings of the LLMs and the tools step by step."
  }
]