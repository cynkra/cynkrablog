[
  {
    "objectID": "blog/posts/2021-07-09-certified-partner-anniversary/index.html",
    "href": "blog/posts/2021-07-09-certified-partner-anniversary/index.html",
    "title": "Celebrating one-year anniversary as RStudio Full Service Certified Partner",
    "section": "",
    "text": "cynkra celebrates its first anniversary as an RStudio Full Service Certified Partner! Every day, we help our clients set up professional IT infrastructures using RStudio products and license compositions suited to their individual needs. In the context of our anniversary, we would like to discuss the RStudio suite of products and our approach of combining RStudio’s products with our Managed Workbench Solution to make RStudio Professional products even more powerful.\n\n\n\nPhoto by Adi Goldstein\n\n\n\nPhilosophy and lineup\nWe believe that good tools combined with smart settings can make data science teams enjoy their work more, resulting in higher productivity levels. We, therefore, work to provide our clients with customized RStudio product solutions to create a data science environment that would support their daily work most efficiently and effectively.\nOur team members, who have all been using practically all RStudio products daily, are testimony to our claims.\n\n\nFinding the right setup\nBesides finding the optimal combination of different software provided by RStudio, we customize the RStudio products to include smart defaults and individualized configurations such as local package repositories or preconfigured database setups. We use a full-fledged configured and centralized managed RStudio Workbench installation that simplifies administrative tasks for system admins and users. In addition, we ensure constant product and security updates, taking away all the maintenance burden and leaving users with a happy place for their data science tasks. We also train the team on how to work with RStudio Products.\nThe transition from the local RStudio Desktop usage is seamless since RStudio Workbench and friends provide all features of the free version.\nThere is no lock-in; everything that works on the free RStudio Desktop version will also work on RStudio Workbench and the other way round – even if you decide to go back to the free version at some point. Of course, once clients have experienced the added benefits of RStudio Workbench, they rarely want to go back.\nOur lineup consists of the following RStudio products: RStudio Workbench (previously RStudio Server Pro), RStudio Connect, RStudio Package Manager, Shiny Server, and RStudio Team.\n\n\nSmart defaults and optimized configurations\nRStudio Workbench, RStudio Connect, and RStudio Package Manager can work together very well if configured correctly. We configure the settings as needed to enable users to experience the full power of the RStudio product suite. Our product saves you from having to go through the entire admin guide where there is always the potential to miss something important. The power (and complexity) of RStudio products resides in their configuration. We have experience with almost all possible configuration scenarios, including configuring RStudio products in highly secured enterprise environments.\n\n\nDockerized environment\nOur products come in a containerized environment, making them agnostic to any underlying operating system. Your users will have access to an Ubuntu LTS system (20.04 at the time of writing) which provides stable support for linking R packages against required system libraries.\nUsing a containerized environment also simplifies update tasks (for the client and us) due to the independence of the underlying operating system. All of this also holds if the underlying operating system of your company is subject to change in the future – and if this happens, the RStudio environment will stay the same.\nAnother common pain point for local RStudio users is the use of LaTeX and pandoc when compiling PDF reports. By relying on the stable Ubuntu LTS environment as the base layer we can provide a stable LaTeX/pandoc environment that takes away almost all troubles for users in this area.\n\n\nPerformance\nCentralized, server-based installations can be very efficient both in speed and battery consumption. It applies to all RStudio products but in particular to RStudio Workbench and RStudio Package Manager.\nThe following GIF might give you an idea of speed improvement (we did the test using a relatively new MacBook Pro 2020). When we deployed the cloud-based RStudio Workbench on the MacBook, the machine performed much faster. The difference in speed will be even more noticeable in an older machine.\n\n\n\nRStudio Desktop vs. RStudio Workbench speed comparison\n\n\nBattery usage becomes critical when working from home or on the go. Using RStudio Workbench instead of RStudio Desktop will save battery life because all computation is done on a remote machine. RStudio Desktop, in particular, is quite energy-hungry; we have experienced battery life improvements of 30% and more in our daily work when using the cloud-based RStudio Workbench instead of RStudio Desktop.\n\n\nCustom real-world examples\nOne of the key strengths of our offering is individualization. To provide more details on this, we put together some configurations we implemented recently to give an idea of what individualized setups can look like in practice.\n\n\n\n\n\n\n\nSetup 1\n\n\nSetup 2\n\n\nSetup 3\n\n\nSetup 4\n\n\n\n\n\n\nRStudio Workbench\n\n\n✓\n\n\n✓\n\n\n\n\n✓\n\n\n\n\nRStudio Connect\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n✓\n\n\n\n\nRStudio Package Manager\n\n\n✓\n\n\n✓\n\n\n\n\n\n\n\n\nRStudio Server Open Source\n\n\n✓\n\n\n\n\n✓\n\n\n\n\n\n\nShiny Server\n\n\n\n\n\n\n\n\n✓\n\n\n\n\nUser Provisioning\n\n\nActive Directory\n\n\nLDAP\n\n\nLocal Users\n\n\nActive Directory\n\n\n\n\nSSO Type\n\n\nPAM\n\n\nOIDC\n\n\n\n\nSAML\n\n\n\n\nDB Drivers\n\n\nRStudio Professional Drivers\n\n\nMS SQL\n\n\nOracle\n\n\nIBM DB2\n\n\n\n\n\nAcronym dictionary for Table:\n\nSSO: Single-Sign-On\nOIDC: OpenID Connect\nPAM: Pluggable Authentication Modules\n\nOften, in larger organizations, the system that is already in use determines the authentication settings. We have experience configuring RStudio Products with various authentication solutions.\n\n\nStandalone licenses and R training\nBesides the complete package that comes with our Managed Workbench offering, we are happy to provide our customers with standalone RStudio licenses. The advantage for Swiss-based customers is that they get a bill in Swiss Francs including VAT, and avoid currency conversion issues with USD when ordering from RStudio directly.\nIn addition to setting up R-related infrastructure, we also provide R training of any kind so that your team can make full use of the available software stack. You can visit our consulting offering on our website for more information."
  },
  {
    "objectID": "blog/posts/2019-08-12-tsbox-02/index.html",
    "href": "blog/posts/2019-08-12-tsbox-02/index.html",
    "title": "tsbox 0.2: supporting additional time series classes",
    "section": "",
    "text": "The tsbox package makes life with time series in R easier. It is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic and allow the user to combine time series of multiple non-standard and irregular frequencies. A detailed overview of the package functionality is given in the documentation page (or in a previous blog-post).\nVersion 0.2 is now on CRAN and provides a larger number of bug fixes. Non-standard column names are now handled correctly, and non-standard column orders are treated consistently."
  },
  {
    "objectID": "blog/posts/2019-08-12-tsbox-02/index.html#new-classes",
    "href": "blog/posts/2019-08-12-tsbox-02/index.html#new-classes",
    "title": "tsbox 0.2: supporting additional time series classes",
    "section": "New Classes",
    "text": "New Classes\nThere are two more time series classes supported: tis time series, from the tis package, and irts time series, from the tseries package.\nTo create an object of these classes, it is sufficient to use the appropriate converter.\nE.g., for tis time series:\nlibrary(tsbox)\nts_tis(fdeaths)\n##       Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec\n## 1974  901  689  827  677  522  406  441  393  387  582  578  666\n## 1975  830  752  785  664  467  438  421  412  343  440  531  771\n## 1976  767 1141  896  532  447  420  376  330  357  445  546  764\n## 1977  862  660  663  643  502  392  411  348  387  385  411  638\n## 1978  796  853  737  546  530  446  431  362  387  430  425  679\n## 1979  821  785  727  612  478  429  405  379  393  411  487  574\n## class: tis\nOr for irts time series:\nhead(ts_irts(fdeaths))\n## 1974-01-01 00:00:00 GMT 901\n## 1974-02-01 00:00:00 GMT 689\nConversion works from all classes to all classes, and we can easily convert these objects to any other time series class, or a data frame:\nx.tis &lt;- ts_tis(fdeaths)\nhead(ts_df(x.tis))\n##         time value\n## 1 1974-01-01   901\n## 2 1974-02-01   689\n## 3 1974-03-01   827\n## 4 1974-04-01   677\n## 5 1974-05-01   522\n## 6 1974-06-01   406"
  },
  {
    "objectID": "blog/posts/2019-08-12-tsbox-02/index.html#class-agnostic-functions",
    "href": "blog/posts/2019-08-12-tsbox-02/index.html#class-agnostic-functions",
    "title": "tsbox 0.2: supporting additional time series classes",
    "section": "Class-agnostic functions",
    "text": "Class-agnostic functions\nBecause coercion works reliably and is well tested, we can use it to make functions class-agnostic. If a class-agnostic function works for one class, it works for all:\nts_pc(ts_tis(fdeaths))\nts_pc(ts_irts(fdeaths))\nts_pc(ts_df(fdeaths))\nts_pc(fdeaths)\nts_pc calculates percentage change rates towards the previous period. It works like a ‘generic’ function: You can apply it on any time series object, and it will return an object of the same class as its input.\nSo, whether we want to smooth, scale, differentiate, chain-link, forecast, regularize, or seasonally adjust a series, we can use the same commands to all time series classes. tsbox offers a comprehensive toolkit for the basics of time series manipulation. Here are some additional examples:\nts_pcy(fdeaths)                # p.c., compared do same period of prev. year\nts_forecast(fdeaths)           # forecast, by exponential smoothing\nts_seas(fdeaths)               # seasonal adjustment, by X-13\nts_frequency(fdeaths, \"year\")  # convert to annual frequency\nts_span(fdeaths, \"-1 year\")    # limit time span to final year\nThere are many more. Because they all start with ts_, you can use auto-complete to see what’s around. Most conveniently, there is a time series plot function that works for all classes and frequencies:\nts_plot(\n  `Airline Passengers` = AirPassengers,\n  `Lynx trappings` = ts_tis(lynx),\n  `Deaths from Lung Diseases` = ts_xts(fdeaths),\n  title = \"Airlines, trappings, and deaths\",\n  subtitle = \"Monthly passengers, annual trappings, monthly deaths\"\n)\n\n\n\ntime series plot"
  },
  {
    "objectID": "blog/posts/2020-09-15-devops/index.html",
    "href": "blog/posts/2020-09-15-devops/index.html",
    "title": "DevOps Expert (f/m/d, 60-100%)",
    "section": "",
    "text": "We are on the lookout for a DevOps System Engineer. You are familiar with Linux, Docker, Git, CI/CD, and Ansible. You will work on interesting projects around the R ecosystem and have a lot of freedom. While you will be required to work on-site at times, remote work is also possible. If you are interested in data science and want to learn more about R, you will enjoy working in this position.\n\n\n\nPhoto by James Pond\n\n\n\nYour role\n\nResponsibility for enhancing and maintaining our hosted RStudio service products and our internal infrastructure\nCommunicating with IT departments of our clients\nSetting up and maintaining infrastructure as code\n\n\n\nWhat we look for\n\nVery good knowledge of Linux, Docker, Git, CI/CD, and Ansible.\n60%-100% commitment\nAbility and desire to learn and improve on the job\nVery good command of written and spoken German\nGood working knowledge of written and spoken English\nAn interest in R and data science, in general, is a plus.\n\n\n\nWhat we offer\n\nAn open-source, friendly environment that encouranges community contribution\nInteresting projects around consulting and open-source software development\nOffices in Zurich: at Stauffacher and near ETH Hönggerberg\nFlexible working hours with the possibility to work from home\n\n\n\nHow to apply\nPlease submit your application via mail@cynkra.com. Get in touch with us if you have further questions.\n\n\nWho we are\ncynkra is a Zurich-based data consulting company with a strong focus on R. We use R and the tidyverse in the vast majority of our projects. We are an RStudio Full Certified Partner.\nWe support businesses and organizations by helping them pick the right tools, implementing solutions, training, and code review. We are enthusiastic about open-source software and contribute to a large number of R packages. Learn more at www.cynkra.com."
  },
  {
    "objectID": "blog/posts/2019-04-10-tsbox-01/index.html",
    "href": "blog/posts/2019-04-10-tsbox-01/index.html",
    "title": "tsbox 0.1: class-agnostic time series",
    "section": "",
    "text": "The R ecosystem knows a vast number of time series classes: ts, xts, zoo, tsibble, tibbletime or timeSeries. The plethora of standards causes confusion. As different packages rely on different classes, it is hard to use them in the same analysis. tsbox provides a set of tools that make it easy to switch between these classes. It also allows the user to treat time series as plain data frames, facilitating the use with tools that assume rectangular data.\n\n\n\n\n\ncomic by xkcd\n\n\n\n\nThe tsbox package is built around a set of functions that convert time series of different classes to each other. They are frequency-agnostic and allow the user to combine multiple non-standard and irregular frequencies. Because coercion works reliably, it is easy to write functions that work identically for all classes. So whether we want to smooth, scale, differentiate, chain-link, forecast, regularize, or seasonally adjust a time series, we can use the same tsbox-command for any time series class.\nThis blog gives a short overview of the changes introduced in 0.1. A detailed overview of the package functionality is given in the documentation page (or in a previous blog-post).\n\nKeeping explicit missing values\nVersion 0.1, now on CRAN, brings many bug fixes and improvements. A substantial change involves the treatment of NA values in data frames. Previously, all NAs in data frames were treated as implicit and were only made explicit by a call to ts_regular.\nThis has changed now. If you convert a ts object to a data frame, all NA values will be preserved. To replicate previous behavior, apply the ts_na_omit function:\nlibrary(tsbox)\nx.ts &lt;- ts_c(mdeaths, austres)\nx.ts\nts_df(x.ts)\nts_na_omit(ts_df(x.ts))\n\n\nts_span extends outside of series span\nThis lays the groundwork for ts_span to be extensible. With extend = TRUE, ts_span extends a regular series with NA values, up to the specified limits, similar to base window. Like all functions in tsbox, this is frequency-agnostic. For example, in the following, the monthly series mdeaths is extended by monthly NA values, while the quarterly series austres is extended by quarterly NA values.\nx.df &lt;- ts_df(ts_c(mdeaths, austres))\nts_span(x.df, end = \"1999-12-01\", extend = TRUE)\n\n\nts_default standardizes column names in a data frame\nIn rectangular data structures, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. By default, tsbox detects a value, a time and zero, one or several id columns. Alternatively, the time column and the value column can be explicitly named time and value. If explicit names are used, the column order will be ignored.\nWhile automatic column name detection is useful in interactive mode, it produces unnecessary overhead in longer workflows. The helper function ts_default detects and renames the time and the value column so that auto-detection will be turned off in subsequent steps (note that the names of the id columns are not affected):\nx.df &lt;- ts_df(ts_c(mdeaths, austres))\nnames(x.df) &lt;- c(\"a fancy id name\", \"date\", \"count\")\nts_plot(x.df)  # tsbox is fine with that\nts_default(x.df)\n\n\nts_summary summarizes time series\nts_summary provides a frequency agnostic summary of a ts-boxable object:\nts_summary(ts_c(mdeaths, austres))\n#&gt;        id obs    diff freq      start        end\n#&gt; 1 mdeaths  72 1 month   12 1974-01-01 1979-12-01\n#&gt; 2 austres  89 3 month    4 1971-04-01 1993-04-01\nts_summary returns a plain data frame that can be used for any purpose. It is also recommended for the extraction of various time series properties, such as start, freq or id:\nts_summary(austres)$id\n#&gt; [1] \"austres\"\nts_summary(austres)$start\n#&gt; [1] \"1971-04-01\"\n\n\nAnd a cheat sheet!\nFinally, we fabricated a tsbox cheat sheet that summarizes most functionality. Print and enjoy working with time series."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "",
    "text": "“Google Season of Docs (GSoD) provides support for open source projects to improve their documentation and gives professional technical writers an opportunity to gain experience in open source.” (Source: Program website)\nThe program makes it possible for technical writers to work closely with an open-source community they may or may not have been engaged with, to solve real problems with high-quality documentation.\nIn the end, an awareness of open source, of documentation and technical writing is raised, while participating open source organizations benefit from an improvement in their documentation. The R Project participated in GSoD as an open-source organization for the first time this year after several years of participating in Google Summer of Code (GSoC), another open-source program focused on coding."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#the-user-conference",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#the-user-conference",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "The useR! conference",
    "text": "The useR! conference\nuseR! is the main annual conference for the R user and developer community that is organized by a community of volunteers and supported by the R Foundation. It is organized by a different team of community organizers each year and has been held since 2004. The useR! conference program consists of both invited and user-contributed presentations in addition to tutorials and other social events.\nWith so much historical information about useR! scattered around Git repositories, useR! websites, and organizers’ hard-disks, the R Project proposed to organize useR! documentation with two outputs- an information board and a knowledgebase. The knowledgebase was proposed to take the form of an online book, inspired by examples such as the satRdays knowledgebase. The information board was proposed as a dashboard to interactively browse historical information. These two projects were carried on concurrently over a span of 6 months (May - November, 2021) during which my primary responsibility as a technical writer was to curate historical useR! conference data and develop the information board with this data."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#the-information-board",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#the-information-board",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "The Information board",
    "text": "The Information board\nWhy an information board?\nAfter participating in useR! 2021 as a part of the organizing committee, I identified several gaps within the organizational process that an information board could fill up. Organizers spend a lot of time looking for information from past useR! conference websites, past organizers, and other archives of un-structured or semi-structured data. This process repeats each year for every useR! conference and this seems to put a burden on past organizers or co-ordinators to continuously provide information to future organizers.\nTo fill in these gaps, I proposed to: - gather data in a structured format for at least the past six useR! conferences - build a dashboard using this data-set - structure things in such a way that updating the data files leads to an updated dashboard after a rebuild process\nThe final product could be found by accessing the following URL: https://bit.ly/infoboard-cynkra"
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#use-cases",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#use-cases",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Use cases",
    "text": "Use cases\nA typical useR! conference program consists of keynotes, regular talks, lightning talks, poster sessions, tutorials and social events. The organizing team for each conference would need to identify keynote speakers, select talks and tutorials, and determine which social events to offer. In addition to those, the organizing team would need to set up partnerships for the conference - sponsors (that can contribute in different ways) and partner organizations. To prospective organizers who have never organized useR! or a conference of such capacity, it is burdensome to sift through 15+ past useR! websites to search out the type of tutorials that have been offered in the past, the number of keynote presentations to offer and around what topics, or what kind of social events to host.\nIt is difficult to ascertain if a talk has been presented in a useR! in the past without access to structured and filterable historical data.\nFurthermore, with diversity and inclusion in mind, it could be difficult to determine presenters that have already presented many times before (perhaps on a similar topic) and those from under-represented groups that have only had a few chances to present talks at a useR!\nFor organizers to target sponsors who are interested in R and who may have sponsored in the past within the location of the conference is hard without data-driven assistance. For potential sponsors, it provides some insight into the scope and reach of useR! conferences.\nFor entities like the R Foundation, R Consortium and R Forwards, the information board provides an easy way to gain insights into the history of useR! while planning for the future in a global and diverse context.\nOther conferences beyond useR! could benefit from the information board as it provides organized data around people, organizations, and presentations that could be helpful in planning local and regional R events."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#tools",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#tools",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Tools",
    "text": "Tools\nI used flexdashboard for the structure and layout of the dashboard, echarts4r for charts, and reactable for interactive tables."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#technical-information",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#technical-information",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Technical information",
    "text": "Technical information\nThe source for this dashboard lives in a GitLab repository where issues or merge requests can be raised.\nAll the data are located in the data directory and a description of the years which each dataset covers.\nThe charts and tables are produced from scripts in the R directory - hopefully making it easier to reproduce them or use them in other contexts.\nThe sidebar menu, footer and JavaScript codes are saved as HTML fragments that are included via the YAML header of the index.Rmd file."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#license",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#license",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "License",
    "text": "License\nThe data is available for download under a CC BY 4.0 license, while the R code is available under a GPL-3.0 license."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#technical-writing-experience",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#technical-writing-experience",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Technical Writing experience",
    "text": "Technical Writing experience\nHaving written R articles for Open Data Science in the past, the experience gained from this GSoD project improved my technical writing and project management skills. In a collaborative sense: I received and implemented feedback several times a month from different volunteers across several timezones, while working to produce the deliverable per project-phase and covering the general scope of the project."
  },
  {
    "objectID": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#acknowledgements",
    "href": "blog/posts/2022-01-05-gsod-user-infoboard/index.html#acknowledgements",
    "title": "Google Season of Docs with R: useR! Information Board",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis project would not have been possible at this time without support from Google via the GSoD program. Much appreciation goes to the GSoD admins for the R Project - Heather Turner and Matt Bannert - who did a lot of admin work from the proposal to the final report. I also appreciate Noa Tamir, who excellently managed the week-to-week supervision of this work. I appreciate the help I received from several volunteers on this project including past organizers who provided data from their archives.\nFinally, cynkra is passionate about open-source and the R community, and this has provided an enabling environment for this project to succeed and to continue succeeding."
  },
  {
    "objectID": "blog/posts/2021-04-09-deprecating-pkgdown/index.html",
    "href": "blog/posts/2021-04-09-deprecating-pkgdown/index.html",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "",
    "text": "Websites for R packages built with {pkgdown} have become a de-facto standard over the last few years. Many R packages build their site during Continuous Integration (CI) runs, pushing the assets to the special gh-pages branch (even though now any branch can be used to deploy a website).\nSometimes, repositories are transferred to a new user/organization or the package is renamed. While GitHub takes care of redirecting repository URLs, the pkgdown URLs (https://&lt;username&gt;.github.io/&lt;rpackage&gt;) are not redirected. Since some users might have bookmarked specific URLs or the URLs appear in their browsing history, it would be great if these links do not return a 404.\nThis blog post proposes several ways to handle this gracefully:\nAll options hinge on the observation that users and organizations can create a user or organization site that will be the source for https://&lt;username&gt;.github.io/&lt;package&gt; after the renaming. The user site will also serve robots.txt that advises crawlers to avoid deprecated contents."
  },
  {
    "objectID": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#user-or-organization-site",
    "href": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#user-or-organization-site",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "User or organization site",
    "text": "User or organization site\nIn GitHub, users can create a user repository &lt;username&gt;/&lt;username&gt;.github.io. This repo will be served automatically as a web page on https://&lt;username&gt;.github.io/ . In this repo, a directory can be created which corresponds to the respective GitHub Pages site of the original repo. Example: The rpackage/ directory in the &lt;username&gt;/&lt;username&gt;.github.io repository corresponds to https://&lt;username&gt;.github.io/rpackage. If both &lt;username&gt;/&lt;rpackage&gt; and &lt;username&gt;/&lt;username&gt;.github.io/&lt;rpackage&gt; exist, the former takes precedence. This means that you can prepare everything in your user repository &lt;username&gt;/&lt;username&gt;.github.io and it will work immediately after you rename your package repository. The following has worked for https://krlmlr.github.io/fledge/, which has moved to https://cynkra.github.io/fledge/:\n\nCreate repository &lt;username&gt;/&lt;username&gt;.github.io\nIn &lt;username&gt;/&lt;username&gt;.github.io create directory &lt;rpackage&gt;\nPopulate the &lt;rpackage&gt; directory using one of the methods described below\nPush to GitHub\nRename repository\n\nAll of this works the same way for organizations. The munch package was previously located at https://cynkra.github.io/SwissCommunes/ The original pages, with a warning, are defined at cynkra/cynkra.github.io."
  },
  {
    "objectID": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#redirection",
    "href": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#redirection",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Redirection",
    "text": "Redirection\nBasic idea: set up an HTML redirect from https://&lt;username-old&gt;.github.io/&lt;package&gt; to https://&lt;username-new&gt;.github.io/&lt;package&gt;.\nTo achieve this, create an index.html in &lt;username&gt;/&lt;username&gt;.github.io/rpackage with the following contents:\n&lt;meta http-equiv=\"refresh\" content=\"0; url=&lt;url to redirect to&gt;\" /&gt;\nHowever, some redirection practices like this one are considered bad practice (“Use of meta refresh is discouraged by the World Wide Web Consortium (W3C).”)[^1]. Also, users might find it sketchy to see some redirection happening shortly after they visited a site. Last, the redirection shown above only works for the top-level domain. Level 2 or level 3 links like &lt;url&gt;/level1/level2 will not work and return a 404."
  },
  {
    "objectID": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-css",
    "href": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-css",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Deprecation via CSS",
    "text": "Deprecation via CSS\nA better way to deprecate a pkgdown/GitHub Pages site is to serve a static version of the last state before the package was moved, and add information to the user that the site has moved.\nAn easy way to achieve this is to include a little CSS snippet. The following will add a colored line before the page-header div in the pkgdown site.\n.page-header:after {\n  content: \"You are viewing an outdated page which is not going to be updated anymore. Please go to &lt;https:/new-url.com&gt; for the latest version.\";\n  font-size: 12px;\n  font-style: italic;\n  color: #f03333;\n}\n\n\n\nDeprecation information in the header via CSS\n\n\n\nPlace this code in the pkgdown/ directory of your package and it will be automatically picked up when the site is built next time:\n\nIn your package, add the CSS snippet from above to pkgdown/extra.css (CSS name can be different) in the repository/R package which should be deprecated\nCall pkgdown::build_site() one last time\nCopy the contents of docs/ to &lt;username&gt;/&lt;username&gt;.github.io/&lt;packagename&gt;\n\nUnfortunately, the :after operator does not allow hyperlinks, so the new URL will not be clickable."
  },
  {
    "objectID": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-bulk-edit",
    "href": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#deprecation-via-bulk-edit",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Deprecation via bulk edit",
    "text": "Deprecation via bulk edit\nFor the URL to be clickable, the HTML files must be edited. The find, xargs and sed utilities help to automate this.\npkgdown uses the Bootstrap framework, which has alerts that serve the purpose. They look best just before the closing &lt;/header&gt; element. The following command line adds an alert to each HTML page, advertising https://cynkra.github.io/munch as the target URL. It must be run in the rpackage directory of &lt;username&gt;/&lt;username&gt;.github.io:\nfind -name \"*.html\" |\n  xargs sed -i -r 's#(^.*[&lt;]/header[&gt;])#&lt;div class=\"alert alert-warning\" role=\"alert\"&gt;&lt;strong&gt;Warning!&lt;/strong&gt; This content has moved to &lt;a href=\"https://cynkra.github.io/munch\"&gt;https://cynkra.github.io/munch&lt;/a&gt;.&lt;/div&gt;\\n\\1#'\nThis assumes GNU sed. MacOS users will need to use gsed, or -i.bak instead of -i and deal with the leftover *.bak files.\n\n\n\nDeprecation information in the header via editing HTML\n\n\n\nAlways advertising the new root works well enough because it is very likely that the structure of the site will eventually change after the repository rename."
  },
  {
    "objectID": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#web-crawlers",
    "href": "blog/posts/2021-04-09-deprecating-pkgdown/index.html#web-crawlers",
    "title": "Deprecating a pkgdown site served via GitHub Pages",
    "section": "Web crawlers",
    "text": "Web crawlers\nIt is a good idea to make the deprecated contents invisible to web crawlers. Add a file robots.txt to the root of &lt;username&gt;/&lt;username&gt;.github.io. The following contents forbids crawling the /SwissCommunes/ directory which contains the old snapshot with pointers to the new location:\nUser-agent: *\nDisallow: /SwissCommunes/"
  },
  {
    "objectID": "blog/posts/2019-08-02-system-engineer/index.html",
    "href": "blog/posts/2019-08-02-system-engineer/index.html",
    "title": "DevOps System Engineer (40-60%)",
    "section": "",
    "text": "We are hiring a system engineer! You are familiar with Ansible, Terraform, Docker, Git, CI/CD. You will work on interesting projects around the R ecosystem and have a lot of freedom. While you will be required to work on-site at times, remote work is also possible. If you are interested in data science and want to learn more about R, you will enjoy working in this position.\n\n\n\nPhoto by Samuel Zeller\n\n\n\nYour role\n\nResponsibility for enhancing and maintaining our hosted service project and our internal infrastructure\nCommunicating with IT departments of our clients\nSetting up infrastructure as code\nHiring and supervising nearshore development, with own budget\n\n\n\nWhat we look for\n\nVery good knowledge of Ansible, Terraform, Docker, Git, CI/CD.\nAn interest in R and data science, in general, is a plus\n40%-60% commitment\nAbility and desire to learn and improve on the job\nGood working knowledge of written and spoken English\nVery good command of spoken German\n\n\n\nWhat we offer\n\nInteresting projects around consulting and open-source software development\nOffice in Zurich close to ETH Hönggerberg\nFlexible working hours\nRemote work possible\nCompetitive compensation\n\n\n\nHow to apply\nPlease submit your application via mail@cynkra.com. Get in touch with us if you have further questions.\n\n\nWho we are\ncynkra is a Zurich-based data consulting company with a strong focus on R. We use R and the tidyverse in the vast majority of our projects. We support businesses and organizations by helping them pick the right tools, implementing solutions, training, and code review. We are enthusiastic about open-source software and contribute to it, too. Learn more at www.cynkra.com."
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html",
    "href": "blog/posts/2020-04-02-dm/index.html",
    "title": "Relational data models in R",
    "section": "",
    "text": "Relational databases are powerful tools for analyzing and manipulating data. However, many modeling workflows require a great deal of time and effort to wrangle data from databases to place it into a flat data frame or table format. Only then the actual data analysis can start."
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html#why-a-relational-model",
    "href": "blog/posts/2020-04-02-dm/index.html#why-a-relational-model",
    "title": "Relational data models in R",
    "section": "Why a relational model?",
    "text": "Why a relational model?\nWith the proper tools, analysis can begin using a relational model that works directly with the database. And, if wrangling is still required, R users can leverage this powerful and proven SQL approach to data organization and manipulation.\nThe dm package takes the primary advantage of databases – relational modeling – and brings it to R. In relational databases, tables or data frames link through primary keys (PK) and foreign keys (FK). Instead of having a single, wide table to work with, data is segmented across multiple tables to eliminate or reduce redundancies. This process is called normalization. A classic example is storing unique identifiers in a large table and looking up values for these unique identifiers in a small data frame. This approach lets an analysis run without reading or writing look-up values until necessary because unique identifiers are enough for most of the runtime.\ndm 0.1.1 is available on CRAN. You can now download and install dm, from CRAN, with the following command:\ninstall.packages(\"dm\")"
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html#connect-to-a-database",
    "href": "blog/posts/2020-04-02-dm/index.html#connect-to-a-database",
    "title": "Relational data models in R",
    "section": "Connect to a database",
    "text": "Connect to a database\nWe connect to a relational dataset repository with a database server that is publicly accessible without registration. There is a financial dataset that contains loan data, along with relevant information and transactions. We chose this loan dataset because the relationships between loan, account, and transcactions tables are good representations of databases that record real-world business transactions.\nThe dataset page lists the credentials required for connecting to the database:\n\nhostname: relational.fit.cvut.cz\nport: 3306\nusername: guest\npassword: relational\ndatabase: Financial_ijs\n\nThese can be used, for example, in MySQL Workbench to download the CSV data manually. To automate and keep the data in the database for as long as possible, we can connect to the database from R through its database interface to access the tables:\nlibrary(RMariaDB)\nmy_db &lt;- dbConnect(\n  MariaDB(),\n  user = 'guest',\n  password = 'relational',\n  dbname = 'Financial_ijs',\n  host = 'relational.fit.cvut.cz'\n)\ndbListTables(my_db)\n\n## [1] \"accounts\"  \"cards\"     \"clients\"   \"disps\"     \"districts\" \"loans\"\n## [7] \"orders\"    \"tkeys\"     \"trans\"\nBy creating a dm object from the connection, we get access to all tables:\nlibrary(dm)\nmy_dm &lt;- dm_from_src(my_db)\n\nmy_dm\n\n## ── Table source ────────────────────────────────────────────────────────────────\n## src:  mysql  [guest@relational.fit.cvut.cz:NA/Financial_ijs]\n## ── Metadata ────────────────────────────────────────────────────────────────────\n## Tables: `accounts`, `cards`, `clients`, `disps`, `districts`, … (9 total)\n## Columns: 57\n## Primary keys: 0\n## Foreign keys: 0\nnames(my_dm)\n\n## [1] \"accounts\"  \"cards\"     \"clients\"   \"disps\"     \"districts\" \"loans\"\n## [7] \"orders\"    \"tkeys\"     \"trans\"\nmy_dm$accounts\n\n## # Source:   table&lt;accounts&gt; [?? x 4]\n## # Database: mysql [guest@relational.fit.cvut.cz:NA/Financial_ijs]\n##      id district_id frequency        date\n##   &lt;int&gt;       &lt;int&gt; &lt;chr&gt;            &lt;date&gt;\n## 1     1          18 POPLATEK MESICNE 1995-03-24\n## 2     2           1 POPLATEK MESICNE 1993-02-26\n## 3     3           5 POPLATEK MESICNE 1997-07-07\n## 4     4          12 POPLATEK MESICNE 1996-02-21\n## 5     5          15 POPLATEK MESICNE 1997-05-30\n## 6     6          51 POPLATEK MESICNE 1994-09-27\n## # … with more rows\nThe components of this particular dm object are lazy tables powered by dbplyr. This package translates the dplyr grammar of data manipulation into queries the database server understands. The advantage to a lazy table is that there is no data download until results are collected for printing or local processing. Below, the summary operation is computed on the database, and only the results are sent back to the R session.\nlibrary(dplyr)\nmy_dm$accounts %&gt;%\n  group_by(district_id) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup()\n\n## # Source:   lazy query [?? x 2]\n## # Database: mysql [guest@relational.fit.cvut.cz:NA/Financial_ijs]\n##   district_id n\n##         &lt;int&gt; &lt;int64&gt;\n## 1           1 554\n## 2           2  42\n## 3           3  50\n## 4           4  48\n## 5           5  65\n## 6           6  48\n## # … with more rows\nIf the data fits into your RAM, a database connection is not required to use dm. The collect() command downloads all tables for our dm object.\nmy_local_dm &lt;-\n  my_dm %&gt;%\n  collect()\n\nobject.size(my_local_dm)\n## 77922024 bytes\nmy_local_dm$accounts %&gt;%\n  group_by(district_id) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup()\n\n## # A tibble: 77 x 2\n##   district_id     n\n##         &lt;int&gt; &lt;int&gt;\n## 1           1   554\n## 2           2    42\n## 3           3    50\n## 4           4    48\n## 5           5    65\n## 6           6    48\n## # … with 71 more rows\nA dm object can also be created from individual data frames with the dm() function."
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html#define-primary-and-foreign-keys",
    "href": "blog/posts/2020-04-02-dm/index.html#define-primary-and-foreign-keys",
    "title": "Relational data models in R",
    "section": "Define primary and foreign keys",
    "text": "Define primary and foreign keys\nRelational database tables link to each other via primary and foreign keys. The model diagram provided by our test database illustrates the intended relationships.\nHowever, it turns out this is not an accurate representation of the entities and relationships within the database:\n\nTable names in our database have the plural form; in the diagram it’s singular.\nThere is a tkeys table available in the database that is not listed in the model diagram.\nThe Financial_std database is similar, but different from the one that we work with, Financial_ijs.\n\nBearing these discrepancies in mind, we can define suitable primary and foreign keys for our dm object. The documentation suggests that the loans table is the most important one. We color the target table separately with dm_color().\n# Defining PKs and FKs\nmy_dm_keys &lt;-\n  my_local_dm %&gt;%\n  dm_add_pk(districts, id) %&gt;%\n  dm_add_pk(accounts, id) %&gt;%\n  dm_add_pk(clients, id) %&gt;%\n  dm_add_pk(loans, id) %&gt;%\n  dm_add_pk(orders, id) %&gt;%\n  dm_add_pk(trans, id) %&gt;%\n  dm_add_pk(disps, id) %&gt;%\n  dm_add_pk(cards, id) %&gt;%\n  dm_add_fk(loans, account_id, accounts) %&gt;%\n  dm_add_fk(orders, account_id, accounts) %&gt;%\n  dm_add_fk(trans, account_id, accounts) %&gt;%\n  dm_add_fk(disps, account_id, accounts) %&gt;%\n  dm_add_fk(disps, client_id, clients) %&gt;%\n  dm_add_fk(accounts, district_id, districts) %&gt;%\n  dm_add_fk(cards, disp_id, disps) %&gt;%\n  dm_set_colors(green = loans)\n\n# Draw the visual model\nmy_dm_keys %&gt;%\n  dm_draw()\n\nThe discrepancies highlight the importance of being able to define primary and foreign keys. Most of the challenges in manipulating data are not syntax knowledge gaps. The syntax can always be looked up with search engines. Knowledge gaps regarding how data is organized are much more common as stumbling blocks for R users when working with distributed data.\nInsight into the structure of a database using the built-in dm_draw() function provides an instant efficiency boost. Combined with defining unique identifiers (primary keys) and how they are found by other tables (foreign keys), an R user can quickly clarify the structures with which they are working.\nTo assist with this process of defining the structure, dm comes with a built-in helper to check the referential integrity of the dataset:\nmy_dm_keys %&gt;%\n  dm_examine_constraints()\n\n## ℹ All constraints satisfied."
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html#create-a-dataset-ready-for-analysis",
    "href": "blog/posts/2020-04-02-dm/index.html#create-a-dataset-ready-for-analysis",
    "title": "Relational data models in R",
    "section": "Create a dataset ready for analysis",
    "text": "Create a dataset ready for analysis\nFor modeling, a flat table or matrix is required as input. If normalization is the process of splitting up a table to reduce redundancies, joining multiple tables together is called denormalizing.\nThe dm_squash_to_tbl() function creates a denormalized table by performing a cascading join between cards and all outgoing foreign keys. A join is the SQL term for combining some or all of the unique columns between 2 or more tables into a single table using the appropriate keys. In this case, the cards table has a foreign key to disps table, which has a foreign key to accounts, which also has a foreign key to the districts table. These foreign key relationships are then used in a cascading join within the dm_squash_to_tbl() function, without having to specify the relationships because they are already encoded within the dm object.\nmy_dm_keys %&gt;%\n  dm_squash_to_tbl(cards)\n\n## Renamed columns:\n## * type -&gt; cards$cards.type, disps$disps.type\n## * district_id -&gt; accounts$accounts.district_id, clients$clients.district_id\n\n## # A tibble: 892 x 28\n##      id disp_id cards.type issued     client_id account_id disps.type\n##   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;      &lt;date&gt;         &lt;int&gt;      &lt;int&gt; &lt;chr&gt;\n## 1     1       9 gold       1998-10-16         9          7 OWNER\n## 2     2      19 classic    1998-03-13        19         14 OWNER\n## 3     3      41 gold       1995-09-03        41         33 OWNER\n## 4     4      42 classic    1998-11-26        42         34 OWNER\n## 5     5      51 junior     1995-04-24        51         43 OWNER\n## 6     7      56 classic    1998-06-11        56         48 OWNER\n## # … with 886 more rows, and 21 more variables: accounts.district_id &lt;int&gt;,\n## #   frequency &lt;chr&gt;, date &lt;date&gt;, A2 &lt;chr&gt;, A3 &lt;chr&gt;, A4 &lt;int&gt;, A5 &lt;int&gt;,\n## #   A6 &lt;int&gt;, A7 &lt;int&gt;, A8 &lt;int&gt;, A9 &lt;int&gt;, A10 &lt;dbl&gt;, A11 &lt;int&gt;, A12 &lt;dbl&gt;,\n## #   A13 &lt;dbl&gt;, A14 &lt;int&gt;, A15 &lt;int&gt;, A16 &lt;int&gt;, birth_number &lt;chr&gt;,\n## #   clients.district_id &lt;int&gt;, tkey_id &lt;int&gt;\nWe have an analysis-ready dataset available to use!"
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html#transform-data-in-a-dm",
    "href": "blog/posts/2020-04-02-dm/index.html#transform-data-in-a-dm",
    "title": "Relational data models in R",
    "section": "Transform data in a dm",
    "text": "Transform data in a dm\nData transformation in dm is done by zooming on the table with which you would like to work. A zoomed dm supports dplyr operations on the zoomed table: simple transformations, grouped operations, joins, and more.\nmy_dm_keys %&gt;%\n  dm_zoom_to(accounts) %&gt;%\n  group_by(district_id) %&gt;%\n  summarize(n = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(districts)\n\n## # Zoomed table: accounts\n## # A tibble:     77 x 17\n##   district_id     n A2    A3        A4    A5    A6    A7    A8    A9   A10   A11\n##         &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n## 1           1   554 Hl.m… Prag… 1.20e6     0     0     0     1     1   100 12541\n## 2           2    42 Bene… cent… 8.89e4    80    26     6     2     5    47  8507\n## 3           3    50 Bero… cent… 7.52e4    55    26     4     1     5    42  8980\n## 4           4    48 Klad… cent… 1.50e5    63    29     6     2     6    67  9753\n## 5           5    65 Kolin cent… 9.56e4    65    30     4     1     6    51  9307\n## 6           6    48 Kutn… cent… 7.80e4    60    23     4     2     4    52  8546\n## # … with 71 more rows, and 5 more variables: A12 &lt;dbl&gt;, A13 &lt;dbl&gt;, A14 &lt;int&gt;,\n## #   A15 &lt;int&gt;, A16 &lt;int&gt;\nThe columns used by left_join() to consolidate tables are inferred from the primary and foreign keys already encoded within the dm object."
  },
  {
    "objectID": "blog/posts/2020-04-02-dm/index.html#reproducible-dataflows-with-dm",
    "href": "blog/posts/2020-04-02-dm/index.html#reproducible-dataflows-with-dm",
    "title": "Relational data models in R",
    "section": "Reproducible dataflows with dm",
    "text": "Reproducible dataflows with dm\nWalking through dm’s data modeling fundamentals, adding keys, and drawing the structure, will help R users better understand data from external databases or apply best practices from relational data modeling to their local data.\nYou can immediately start testing on an Rstudio cloud instance! For more examples and explanations, check out the documentation page. Install this package today!"
  },
  {
    "objectID": "blog/posts/2020-11-02-jitsi-load-balanced/index.html",
    "href": "blog/posts/2020-11-02-jitsi-load-balanced/index.html",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "",
    "text": "Jitsi Meet is a self-hosted Free and Open-Source Software (FOSS) video conferencing solution. During the recent COVID-19 pandemic, the project became quite popular, and many companies decided to host their own Jitsi instance.\nThere are many different ways to install and run Jitsi on a machine. A popular choice in the DevOps space is to use Docker via docker-compose, which was the method used in our scenario.\nAt cynkra, while we have been running our own Jitsi instance quite happily for some months, there was a slightly challenging task coming up: hosting a virtual meeting for approximately 100 participants."
  },
  {
    "objectID": "blog/posts/2020-11-02-jitsi-load-balanced/index.html#the-challenge",
    "href": "blog/posts/2020-11-02-jitsi-load-balanced/index.html#the-challenge",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "The Challenge",
    "text": "The Challenge\ncynkra actively supports the local Zurich R User Group. For one of their recent meetings, about 100 people RSVP’ed.\nWhen browsing the load capabilities of a single Jitsi instance, we found that the stock setup begins to experience some challenges at around 35 people and fails at around 70 people. The limiting factor appears to be the “videobridge”. One solution is to add a second videobridge to the Jitsi instance. Jitsi can then distribute the load and should be able to host more than 100 people in a meeting.\nThe best approach is to deploy the second videobridge on a new instance to avoid running into CPU limitations on the main machine. While there is a guide in the Jitsi Wiki and a video that explains how to do it, many people still struggle (1, 2) to get this set up successfully.\nHence, we thought it would be valuable to take another, hopefully simple and understandable stab at explaining this task to the community."
  },
  {
    "objectID": "blog/posts/2020-11-02-jitsi-load-balanced/index.html#load-balancing-jitsi-meet",
    "href": "blog/posts/2020-11-02-jitsi-load-balanced/index.html#load-balancing-jitsi-meet",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "Load-balancing Jitsi Meet",
    "text": "Load-balancing Jitsi Meet\nIn the following, we will denote the main machine on which Jitsi runs, as MAIN. The second machine, which will only host a standalone videobridge, will be named BRIDGE.\n\nThe first step is to create a working installation on MAIN, following the official docker guide from the Jitsi developers. There is no need to use Docker. An installation on the host system will also work.\nAt this point, we assume that you already have installed Jitsi with SSL support at a fictitious domain.\nTo be able to connect to the XMPP server (managed by prosody) on MAIN from BRIDGE (details in point 4 below), port 5222 needs to be exported to the public. This requires adding\nports:\n  - \"5222:5222\"\nto the prosody section in docker-compose.yml and ensuring that the port is opened in the firewall (ufw allow 5222).\nOn BRIDGE, start with the same .env and docker-compose.yml as MAIN.\nIn docker-compose.yml, remove all services besides jvb. The videobridge will later connect to all services on MAIN.\nMake sure that JVB_AUTH_USER and JVB_AUTH_PASSWORD in .env are the same as on MAIN, otherwise the authentication will fail.\nOn BRIDGE in .env change XMPP_SERVER=xmpp.&lt;DOMAIN&gt; to XMPP_SERVER=&lt;DOMAIN&gt;.\nRun docker-compose up and observe what happens. The videobridge should successfully connect to &lt;DOMAIN&gt;. On MAIN, in docker logs jitsi_jicofo_1, an entry should appear denoting that a new videobridge was successfully connected.\nIt looks like\nJicofo 2020-10-23 19:01:52.173 INFO: [29] org.jitsi.jicofo.bridge.BridgeSelector.log() Added new videobridge: Bridge[jid=jvbbrewery@internal-muc.&lt;DOMAIN&gt;/d789de303e9b, relayId=null, region=null, stress=0.00]\nIf you have another videobridge running on MAIN, you should see that the identifier of the new videobridge (here d789de303e9b) is different to your main videobridge identifier. On BRIDGE, the logs should show something like\nINFO: Joined MUC: jvbbrewery@internal-muc.&lt;DOMAIN&gt;\nINFO: Performed a successful health check in PT0S. Sticky failure: false\n\nTo test that the external videobridge is active, one can disable the main videobridge (docker stop jitsi_jvb_1) and try to enable the camera in a new meeting."
  },
  {
    "objectID": "blog/posts/2020-11-02-jitsi-load-balanced/index.html#troubleshooting-and-tips",
    "href": "blog/posts/2020-11-02-jitsi-load-balanced/index.html#troubleshooting-and-tips",
    "title": "Setting up a load-balanced Jitsi Meet instance",
    "section": "Troubleshooting and Tips",
    "text": "Troubleshooting and Tips\n\nIf you see something like SASLError using SCRAM-SHA-1: not-authorized, this indicates that the JVB_AUTH_PASSWORD and/or JVB_AUTH_USER on BRIDGE are incorrect.\nIf you change something in .env of MAIN, you need to delete all config folders before running docker-compose up again. Otherwise changes won’t be picked up even when force destroying the containers.\nDo not run gen-passwords.sh multiple times as JVB_AUTH_PASSWORD and BRIDGE will not be able to connect anymore.\nUnrelated to the content above: if you want to create a user manually for your instance, the following command might be helpful:\ndocker exec jitsi_prosody_1 prosodyctl --config /config/prosody.cfg.lua register &lt;USER&gt; &lt;DOMAIN&gt; \"&lt;PASSWORD&gt;\""
  },
  {
    "objectID": "blog/posts/2019-03-21-senior-data-scientist/index.html",
    "href": "blog/posts/2019-03-21-senior-data-scientist/index.html",
    "title": "Data Scientist/Engineer (40-100%)",
    "section": "",
    "text": "We are hiring a data scientist/engineer! You are enthusiastic about R and know a bit about Git – everything else is negotiable. You will work on interesting projects around the R ecosystem, and you will have lots of freedom, with the option to work remotely.\n\n\n\nPhoto by Scott Webb\n\n\n\n\n\nWhat we look for\n\nVery good knowledge of R\nBasic knowledge of Git\nExperience in Machine Learning, Statistics, Programming\nExperience with Databases, Teaching, shiny, tidyverse, etc., is a plus\n40%-100% commitment, can be adapted throughout the year\nAbility and desire to learn and improve on the job\nGood working knowledge of written English\nVery good command of spoken English or German\n\n\n\nWhat we offer\n\nInteresting projects around consulting and open-source software development\nNice private office in Zurich close to ETH Hönggerberg, with free coffee during office hours\nRemote work possible\nCompetitive compensation\n\n\n\nHow to apply\nPlease submit your application via mail@cynkra.com, or share a private GitHub repository with us. Get in touch with us if you have further questions.\n\n\nWho we are\ncynkra is a Zurich-based data consulting company with a strong focus on R. We use R and the tidyverse in the vast majority of our projects. We support businesses and organizations by helping them pick the right tools, implementing solutions, training, and code review. We are enthusiastic about open-source software and contribute to it, too. Learn more at www.cynkra.com."
  },
  {
    "objectID": "blog/posts/2019-07-17-dm/index.html",
    "href": "blog/posts/2019-07-17-dm/index.html",
    "title": "Introducing dm: easy juggling of tables and relations",
    "section": "",
    "text": "The dm package reduces your mental strain when working with many tables. It connects your data, while still keeping them in their original tables. You can easily perform operations on all tables, visualize how they’re connected, and integrate databases.\n\n\n\nPhoto by Tobias Fischer\n\n\n\nRather than writing out the relational structure in code while connecting the sources, dm packages them in data. This workflow prevents you from inconsistently updating tables, making copies due to redundancies and, more generally, losing track of decisions made in a data pipeline.\n\nA case study\nTo demonstrate dm’s benefits, let’s compare some workflows using data from the nycflights13 package.\nWe want to get the data to analyze arrival delays of flights of planes built more than 30 years ago. For these flights, we also want to check if they departed for destinations in the south. We use methods from the dplyr package, the grammar for data manipulation from the tidyverse.\nThis requires columns lon (longitude) from table airports, year from table planes and arr_delay (arrival delay) from flights. We can do this by performing multiple joins, one after another, as you might be used to:\nlibrary(nycflights13)\nlibrary(dplyr)\nplanes_join &lt;- planes %&gt;%\n  rename(production_year_plane = year) %&gt;%\n  left_join(flights, by = \"tailnum\") %&gt;%\n  left_join(airports, by = c(\"dest\" = \"faa\")) %&gt;%\n  select(lon, year, production_year_plane, dest)\n\nplanes_join %&gt;%\n  filter(production_year_plane &lt; 1983)\n## # A tibble: 1,432 x 4\n##      lon  year production_year_plane dest\n##    &lt;dbl&gt; &lt;int&gt;                 &lt;int&gt; &lt;chr&gt;\n##  1 -95.3  2013                  1965 IAH\n##  2 -95.3  2013                  1965 IAH\n##  3 -95.3  2013                  1965 IAH\n##  4 -95.3  2013                  1965 IAH\n##  5 -87.9  2013                  1959 ORD\n##  6 -97.0  2013                  1959 DFW\n##  7 -97.0  2013                  1959 DFW\n##  8 -97.0  2013                  1959 DFW\n##  9 -90.4  2013                  1959 STL\n## 10 -97.0  2013                  1959 DFW\n## # … with 1,422 more rows\nFair enough, you might say — to keep an overview, we can reduce the 34 columns using select(), then filter(). But what do you do once there are 30 tables, possibly from databases you’re working with? Is this still a workable solution? And how sure are we that we truly only need these three columns?\n\n\ndm\nOn your dm object, you can directly apply operations like filter() to all your tables! How so? Because relations between the objects are already registered in the dm object, you don’t need to indicate them.\nNot having to state how tables are connected each time you perform a join may make exploratory work a lot easier:\nlibrary(dm)\ncdm_nycflights13() %&gt;%\n  cdm_filter(planes, year &lt; 1983)\n## ── Table source ───────────────────────────────────────────────────────────\n## src:  &lt;package: nycflights13&gt;\n## ── Data model ─────────────────────────────────────────────────────────────\n## Data model object:\n##   5 tables:  airlines, airports, flights, planes ...\n##   53 columns\n##   3 primary keys\n##   3 references\n## ── Rows ───────────────────────────────────────────────────────────────────\n## Total: 27583\n## airlines: 4, airports: 3, flights: 1432, planes: 29, weather: 26115\nThis produces an overview of your data sources — all tables still separated. Also note: there was no need to rename() the column year, and all tables stay separate.\nWhat if you still wanted to produce a join and check where those flights departed to? Voilà:\ncdm_nycflights13() %&gt;%\n  cdm_filter(planes, year &lt; 1983) %&gt;%\n  cdm_join_tbl(flights, airports)\n## # A tibble: 1,432 x 19\n##     year month   day dep_time sched_dep_time dep_delay arr_time\n##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;\n##  1  2013     1     1      839            850       -11     1027\n##  2  2013     1     1      858            900        -2     1102\n##  3  2013     1     1     1219           1220        -1     1415\n##  4  2013     1     1     1317           1325        -8     1454\n##  5  2013     1     1     1422           1410        12     1613\n##  6  2013     1     1     1806           1810        -4     2002\n##  7  2013     1     1     1911           1910         1     2050\n##  8  2013     1     1     2030           2045       -15     2150\n##  9  2013     1     2      535            540        -5      831\n## 10  2013     1     2      945            945         0     1113\n## # … with 1,422 more rows, and 12 more variables: sched_arr_time &lt;int&gt;,\n## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,\n## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,\n## #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n\nVisualization\nYou can easily gain a visual overview using cdm_draw(). This results in pretty, customizable (using cdm_set_colors()) visualizations of how your data connect.\ncdm_nycflights13() %&gt;%\n  cdm_set_colors(\n    flights = \"blue_nb\",\n    airlines = \"orange_nb\",\n    weather = \"green_nb\",\n    airports = \"yellow_nb\",\n    planes = \"light_grey_nb\"\n  ) %&gt;%\n  cdm_draw()\n\n\n\nDatabases\ndm works for both in-memory data and databases. You can easily integrate it into your dbplyr workflow, making for new, powerful ways of analysis.\n\n\nFeedback\nThis was just a short glimpse of what dm can do — more posts will follow. Feel free to contribute to this package on Github. Funding for dm is generously provided by Energie360 and cynkra. This package is authored by Tobias Schieferdecker and Kirill Müller."
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "",
    "text": "I wanted to try out the new fromJSON() that allows dynamic build matrices in GitHub Actions for quite some time now. Today was the day.\nGitHub Actions allows automating build and deployment processes (CI/CD), tightly integrated with GitHub. A build matrix is a way to define very similar workflows that differ only by configuration parameters. Usually, a build matrix is defined directly in the .yaml files together with the workflows. This blog post shows how to define these build matrices dynamically so that the “source of truth” for the matrix definition is outside the .yaml file.\nThe configuration for a workflow is a YAML file that has a context and expression syntax with very few basic functions. Two very powerful functions are toJSON() and fromJSON():\nThe basic setup comprises of two jobs: one that creates the workflow definition as JSON and stores it as output, and another dependent job that injects this output via fromJSON() into its matrix definition. A third job is defined for testing if outputs are passed correctly between jobs.\nThe original blog post contains a somewhat brief description. This blog post gives a walkthrough of how I converted a static to a dynamic build matrix in the DBItest project."
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#original-matrix",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#original-matrix",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Original matrix",
    "text": "Original matrix\nIn DBItest, we test the compatibility of new or updated tests with backend packages. Each backend is run in a build matrix, which is defined as follows:\njobs:\n  backend:\n    strategy:\n      fail-fast: false\n      matrix:\n        package:\n          - duckdb\n          - RSQLite\n          - RMariaDB\n          - RPostgres\n          - RKazam\nThe relevant backends are defined in the Makefile, we want to get the list from there so that we can use a single source of truth.\nThis is a very simple build matrix, ideally suited for first experiments. The techniques shown here are applicable to build matrices of any complexity and size."
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#derive-and-verify-json",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#derive-and-verify-json",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Derive and verify JSON",
    "text": "Derive and verify JSON\nOur goal is to create the package: section from the above matrix in JSON format. To derive the JSON format, I use the sed stream editor, my beloved hammer that I use whenever I see a text transformation task in the shell:\necho '{ \"package\" : ['\n## { \"package\" : [\nsed -n \"/^REVDEP *:= */ { s///; p }\" revdep-dev/Makefile | sed 's/ /, /g' | xargs -n 1 echo | sed -r 's/^([^,]*)(,?)$/\"\\1\"\\2/'\n## \"RMariaDB\",\n## \"RSQLite\",\n## \"RPostgres\",\n## \"RKazam\",\n## \"duckdb\"\necho \"]}\"\n## ]}\nThis is not pretty, but still valid JSON when put together. We can prettify with jq ., later we will use jq -c . to condense to a single line.\n(\n  echo '{ \"package\" : ['\n  sed -n \"/^REVDEP *:= */ { s///; p }\" revdep-dev/Makefile | sed 's/ /, /g' | xargs -n 1 echo | sed -r 's/^([^,]*)(,?)$/\"\\1\"\\2/'\n  echo \"]}\"\n) | jq .\n{\n  \"package\": [\n    \"RMariaDB\",\n    \"RSQLite\",\n    \"RPostgres\",\n    \"RKazam\",\n    \"duckdb\"\n  ]\n}\nWe verify the YAML version by piping to json2yaml which can be installed with npm install json2yaml:\n---\n  package:\n    - \"RMariaDB\"\n    - \"RSQLite\"\n    - \"RPostgres\"\n    - \"RKazam\"\n    - \"duckdb\"\nThese tools are preinstalled on the workers. This avoids time-consuming installation procedures in this first job that needs to be run before the main jobs can even start.1"
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#define-job",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#define-job",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Define job",
    "text": "Define job\n{% raw %}\nOnce we have derived the JSON, we’re ready to define a job that creates the matrix. This must be done in the same workflow file where the matrix is defined, ideally before the main job. The job runs on ubuntu-latest, and also must clone the repository. In the bash snippet, the $matrix variable contains the JSON. It is shown and pretty-printed before it is provided as output via echo ::set-output ....\njobs:\n  matrix:\n    runs-on: ubuntu-latest\n    outputs:\n      matrix: ${{ steps.set-matrix.outputs.matrix }}\n    steps:\n      - uses: actions/checkout@v2\n\n      - id: set-matrix\n        run: |\n          matrix=$((\n            echo '{ \"package\" : ['\n            sed -n \"/^REVDEP *:= */ { s///; p }\" revdep-dev/Makefile | sed 's/ /, /g' | xargs -n 1 echo | sed -r 's/^([^,]*)(,?)$/\"\\1\"\\2/'\n            echo \" ]}\"\n          ) | jq -c .)\n          echo $matrix\n          echo $matrix | jq .\n          echo \"::set-output name=matrix::$matrix\"\n\n  backend:\n    # Original workflow\n    # ..."
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#verify-output",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#verify-output",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Verify output",
    "text": "Verify output\nBefore plugging in the generated JSON into our build job, we add another check job to verify if the generated JSON is transported correctly across job boundaries. The needs: matrix declares that the job must wait before the first matrix job succeeds. The job’s output is queried via ${{ needs.matrix.outputs.matrix }}, the quotes ensure that bash processes this correctly. We install and use json2yaml to double-check what the YAML snippet looks like.\njobs:\n  matrix:\n    # job defined above\n\n  check-matrix:\n    runs-on: ubuntu-latest\n    needs: matrix\n    steps:\n      - name: Install json2yaml\n        run: |\n          sudo npm install -g json2yaml\n\n      - name: Check matrix definition\n        run: |\n          matrix='${{ needs.matrix.outputs.matrix }}'\n          echo $matrix\n          echo $matrix | jq .\n          echo $matrix | json2yaml\n\n  backend:\n    # Original workflow\n    # ...\n{% endraw %}"
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#use-output",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#use-output",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Use output",
    "text": "Use output\n{% raw %} Finally, we’re ready to use the generated JSON as a build matrix. The workflow now uses matrix: ${{fromJson(needs.matrix.outputs.matrix)}} instead of the hard-coded matrix:\njobs:\n  matrix:\n    # see above\n\n  check-matrix:\n    # see above\n\n  backend:\n    needs: matrix\n\n    strategy:\n      fail-fast: false\n      matrix: ${{fromJson(needs.matrix.outputs.matrix)}}\n\n    # rest unchanged\n{% endraw %}\nThis gives a workflow as shown in the image below. Click on the image to view the workflow on GitHub.\n\n\n\n\nFinal workflow with dynamic build matrix"
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#next-steps",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#next-steps",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Next steps",
    "text": "Next steps\nFor R packages, I see two use case where dynamic matrices can be useful:\n\nTesting if package checks pass if one suggested package is not installed. Ideally, we remove suggested packages one by one and run in parallel.\nTesting reverse dependencies. For some packages we may hit the limit of 256 jobs per workflow run. Allocating downstream packages to workers, minimizing the number of packages to be installed on each worker, sounds like an interesting optimization problem.\n\nWhat are your use cases for dynamic build matrices? Drop us a line at mail@cynkra.com!"
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#caveats",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#caveats",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Caveats",
    "text": "Caveats\nEven with this simple build matrix, it took more time than I would have hoped to get the bits and pieces right. Quoting is hard. Setting up the check-matrix job really saves time, I wish I had done this from the start.\nBoth fromJson() and fromJSON() appear to work. The internal functions from the expression syntax seem to be case-insensitive throughout.\nFor older versions, jq needs to be called as jq . to act as a pretty-printer. For newer versions this can be omitted.\nToday I also learned that workflows can be temporarily disabled. This is useful in situations where you experiment with a workflow and want to avoid running other workflows for every test."
  },
  {
    "objectID": "blog/posts/2020-12-23-dynamic-gha/index.html#footnotes",
    "href": "blog/posts/2020-12-23-dynamic-gha/index.html#footnotes",
    "title": "Dynamic build matrix in GitHub Actions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can use any tool or ecosystem you are familiar with to come up with the JSON definition. To avoid long installation times, use a specific image for your step via uses: docker://... or implement a container action, also possible in the same repository.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog posts",
    "section": "",
    "text": "Accessing Google’s API via OAuth2\n\n\n9 min\n\n\n\n\n\n\nPatrick Schratz\n\n\nMay 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Scientist (80-100%)\n\n\n2 min\n\n\n\n\n\n\ncynkra team\n\n\nApr 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseasonal 1.9: Accessing composite output\n\n\n2 min\n\n\n\n\n\n\nChristoph Sax\n\n\nApr 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Season of Docs with R: useR! Information Board\n\n\n6 min\n\n\n\n\n\n\nBen Ubah\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning old versions of TeXlive with tinytex\n\n\n5 min\n\n\n\n\n\n\nKirill Müller\n\n\nOct 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox 0.3.1: extended functionality\n\n\n5 min\n\n\n\n\n\n\nChristoph Sax\n\n\nSep 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCelebrating one-year anniversary as RStudio Full Service Certified Partner\n\n\n8 min\n\n\n\n\n\n\nCosima Meyer, Patrick Schratz\n\n\nJul 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeprecating a pkgdown site served via GitHub Pages\n\n\n4 min\n\n\n\n\n\n\nPatrick Schratz, Kirill Müller\n\n\nApr 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngfortran support for R on macOS\n\n\n5 min\n\n\n\n\n\n\nPatrick Schratz\n\n\nMar 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeasonal Adjustment of Multiple Series\n\n\n3 min\n\n\n\n\n\n\nChristoph Sax\n\n\nMar 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDynamic build matrix in GitHub Actions\n\n\n8 min\n\n\n\n\n\n\nKirill Müller\n\n\nDec 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up a load-balanced Jitsi Meet instance\n\n\n4 min\n\n\n\n\n\n\nPatrick Schratz\n\n\nNov 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevOps Expert (f/m/d, 60-100%)\n\n\n2 min\n\n\n\n\n\n\ncynkra team\n\n\nSep 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaintaining multiple identities with Git\n\n\n2 min\n\n\n\n\n\n\nKirill Müller\n\n\nAug 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelational data models in R\n\n\n12 min\n\n\n\n\n\n\nAngel D’az, Kirill Müller\n\n\nApr 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntempdisagg: converting quarterly time series to daily\n\n\n4 min\n\n\n\n\n\n\nChristoph Sax\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox 0.2: supporting additional time series classes\n\n\n4 min\n\n\n\n\n\n\nChristoph Sax\n\n\nAug 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevOps System Engineer (40-60%)\n\n\n2 min\n\n\n\n\n\n\ncynkra team\n\n\nAug 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing dm: easy juggling of tables and relations\n\n\n7 min\n\n\n\n\n\n\nBalthasar Sager\n\n\nJul 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox 0.1: class-agnostic time series\n\n\n3 min\n\n\n\n\n\n\nChristoph Sax\n\n\nApr 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Scientist/Engineer (40-100%)\n\n\n2 min\n\n\n\n\n\n\ncynkra team\n\n\nMar 21, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime series of the world, unite!\n\n\n3 min\n\n\n\n\n\n\nChristoph Sax\n\n\nMay 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDone “Establishing DBI”!?\n\n\n15 min\n\n\n\n\n\n\nKirill Müller\n\n\nMay 1, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "R-DBI blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nAccessing Google’s API via OAuth2\n\n\n\n\n\n\n\n\n\n\n\n\n14 May 2022\n\n\nPatrick Schratz\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nData Scientist (80-100%)\n\n\n\n\n\n\n\n\n\n\n\n\n27 April 2022\n\n\ncynkra team\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nseasonal 1.9: Accessing composite output\n\n\n\n\n\n\n\n\n\n\n\n\n19 April 2022\n\n\nChristoph Sax\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nGoogle Season of Docs with R: useR! Information Board\n\n\n\n\n\n\n\n\n\n\n\n\n05 January 2022\n\n\nBen Ubah\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nRunning old versions of TeXlive with tinytex\n\n\n\n\n\n\n\n\n\n\n\n\n07 October 2021\n\n\nKirill Müller\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\ntsbox 0.3.1: extended functionality\n\n\n\n\n\n\n\n\n\n\n\n\n18 September 2021\n\n\nChristoph Sax\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nCelebrating one-year anniversary as RStudio Full Service Certified Partner\n\n\n\n\n\n\n\n\n\n\n\n\n09 July 2021\n\n\nCosima Meyer, Patrick Schratz\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nDeprecating a pkgdown site served via GitHub Pages\n\n\n\n\n\n\n\n\n\n\n\n\n09 April 2021\n\n\nPatrick Schratz, Kirill Müller\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\ngfortran support for R on macOS\n\n\n\n\n\n\n\n\n\n\n\n\n16 March 2021\n\n\nPatrick Schratz\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nSeasonal Adjustment of Multiple Series\n\n\n\n\n\n\n\n\n\n\n\n\n09 March 2021\n\n\nChristoph Sax\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nDynamic build matrix in GitHub Actions\n\n\n\n\n\n\n\n\n\n\n\n\n23 December 2020\n\n\nKirill Müller\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nSetting up a load-balanced Jitsi Meet instance\n\n\n\n\n\n\n\n\n\n\n\n\n02 November 2020\n\n\nPatrick Schratz\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nDevOps Expert (f/m/d, 60-100%)\n\n\n\n\n\n\n\n\n\n\n\n\n15 September 2020\n\n\ncynkra team\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nMaintaining multiple identities with Git\n\n\n\n\n\n\n\n\n\n\n\n\n25 August 2020\n\n\nKirill Müller\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nRelational data models in R\n\n\n\n\n\n\n\n\n\n\n\n\n02 April 2020\n\n\nAngel D’az, Kirill Müller\n\n\n12 min\n\n\n\n\n\n\n  \n\n\n\n\ntempdisagg: converting quarterly time series to daily\n\n\n\n\n\n\n\n\n\n\n\n\n09 February 2020\n\n\nChristoph Sax\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\ntsbox 0.2: supporting additional time series classes\n\n\n\n\n\n\n\n\n\n\n\n\n12 August 2019\n\n\nChristoph Sax\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nDevOps System Engineer (40-60%)\n\n\n\n\n\n\n\n\n\n\n\n\n02 August 2019\n\n\ncynkra team\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing dm: easy juggling of tables and relations\n\n\n\n\n\n\n\n\n\n\n\n\n17 July 2019\n\n\nBalthasar Sager\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\ntsbox 0.1: class-agnostic time series\n\n\n\n\n\n\n\n\n\n\n\n\n10 April 2019\n\n\nChristoph Sax\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nData Scientist/Engineer (40-100%)\n\n\n\n\n\n\n\n\n\n\n\n\n21 March 2019\n\n\ncynkra team\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nTime series of the world, unite!\n\n\n\n\n\n\n\n\n\n\n\n\n15 May 2018\n\n\nChristoph Sax\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nDone “Establishing DBI”!?\n\n\n\n\n\n\n\n\n\n\n\n\n01 May 2018\n\n\nKirill Müller\n\n\n15 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html",
    "href": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html",
    "title": "Accessing Google’s API via OAuth2",
    "section": "",
    "text": "At cynkra we recently aimed to automate more parts of our internal toolstack. One tool is Google Workspace. Google Workspace offers a comprehensive REST API which can be used for automation purposes.\nWhen interacting with an API, authentication is usually required. This is commonly done by adding a header to the request which includes an access token along with the actual API request. For many APIs these headers can be simple single access token of type “Bearer”. These tokens often have no expiration date and infinite scopes, meaning they can be used for any kind of request against the respective API endpoints. The risk with these tokens is that they are quite powerful and an attacker can somewhat easily get infinite access to your account, both in terms of scopes and time.\nTherefore, many services recently started to favor the use of OAuth2 in a multi-stage authentication concept. The process can be broken down as follows:\nThis multi-stage process can be automated by using automation tools like Ansible or similar. Yet the tricky part is usually the authentication against the OAuth2 app. Traditionally OAuth2 apps aim for GUI-based interaction, i.e., someone clicking a button to authorize the request. However, when aiming for automation via an API, this is not feasible. Instead the OAuth2 app should return the access token as code to further continue the automated workflow. There are plenty of Stackoverflow questions about this topic with many upvotes:"
  },
  {
    "objectID": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-oob-deprecation",
    "href": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-oob-deprecation",
    "title": "Accessing Google’s API via OAuth2",
    "section": "The “OOB” deprecation",
    "text": "The “OOB” deprecation\nFor many years there was a workaround by using redirect_uri=urn:ietf:wg:oauth:2.0:oob combined with response_type=code which was widely shared across the web and Youtube. Yet in Feburary 2022 Google finally blocked this approach as it is considered unsafe and more secure methods should be used."
  },
  {
    "objectID": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-jwt-authentication-approach",
    "href": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#the-jwt-authentication-approach",
    "title": "Accessing Google’s API via OAuth2",
    "section": "The “JWT” authentication approach",
    "text": "The “JWT” authentication approach\nHence a new approach is needed to authenticate against Google OAuth2 apps programmatically. One of these is the use of JSON Web Tokens (JWT). These are different to Bearer tokens in the way that they must be signed and encrypted using a domain-wide access token and am specific algorithm which the OAuth2 apps expects (for future decoding purposes). The mentioned encryption is also not straightforward and usually requires the use of an additional language (e.g. Python, Ruby, Java) and a respective module which does the encryption. The key and its secret (in the Google case) which should be encrypted must be generated within a service account that was granted domain-wide delegation. If such an encrypted JWT is sent to the OAuth2 app, it can verify the owner and issue a short-lived token with the respective scopes of the OAuth2 app."
  },
  {
    "objectID": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#ansible-workflow-example",
    "href": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#ansible-workflow-example",
    "title": "Accessing Google’s API via OAuth2",
    "section": "Ansible workflow example",
    "text": "Ansible workflow example\nAll of the above made the process of issueing a “simple” API call against the Google API quite cumbersome. This is why in the following a fully-working ansible approach is provided which uses a Ruby script for the initial JWT encryption.\nThis assumes\n\na working Ruby installation at /usr/bin/ruby\nan existing Google service account with domain wide delegation\nan OAuth2 app to which the service account has acccess to with matching scopes required for the final API call\n\nDisclaimer: The jwt.rb script below and parts of the ansible logic are taken/adapted from another blog post which I am unable to find again. Memo to myself: always instantly store the link somewhere if you find some helpful content on a website…\n- name: \"Google Workspace: Create JWT for Google OAuth2\"\n  command: &gt;\n    env ruby &lt;path/to/&gt;/jwt.rb --iss \"google-workspace@&lt;some service account name&gt;.iam.gserviceaccount.com\"\n    --sub \"&lt;issuer email&gt;\" --scope \"{% raw %}{{ google_workspace_oauth2_api_scopes | join(' ') }}{% endraw %}\"\n    --kid \"{% raw %}{{ google_workspace_oauth2_key_id.value }}{% endraw %}\"\n    --pkey \"{% raw %}{{ google_workspace_oauth2_private_key.value }}{% endraw %}\"\n  args: { chdir: \"/usr/bin/\" }\n  register: jwt\nHere google_workspace_oauth2_api_scopes is a list of Google API scopes\ngoogle_workspace_oauth2_api_scopes:\n  - 'https://www.googleapis.com/auth/admin.directory.user'\n  - 'https://www.googleapis.com/auth/admin.directory.group'\n  - 'https://www.googleapis.com/auth/admin.directory.domain'\n  - 'https://www.googleapis.com/auth/admin.directory.userschema'\n  - 'https://www.googleapis.com/auth/apps.licensing'\nand google_workspace_oauth2_key_id and google_workspace_oauth2_private_key are the credentials from the respective service account used.\nThe jwt.rb file referenced in the call above looks as follows:\n#!/usr/bin/env ruby\n# frozen_string_literal: true\n\nrequire 'jwt'\nrequire 'optparse'\nrequire 'openssl'\n\noptions = {}\nOptionParser.new do |opts|\n  opts.banner = 'Usage: jwt.rb [options]'\n\n  opts.on('--iss ISS', 'Issuer') do |iss|\n    options[:iss] = iss\n  end\n  opts.on('--sub SUB', 'Subject') do |sub|\n    options[:sub] = sub\n  end\n  opts.on('--scope SCOPE', 'API Scopes') do |scope|\n    options[:scope] = scope\n  end\n  opts.on('--kid KID', 'Key id') do |kid|\n    options[:kid] = kid\n  end\n  opts.on('--pkey PKEY', 'Key') do |pkey|\n    options[:pkey] = pkey\n  end\nend.parse!\n\niat = Time.now.to_i\nexp = iat + 900 # token is 900s valid\n\npayload = { iss: options[:iss].to_s,\n            sub: options[:sub].to_s,\n            scope: options[:scope].to_s,\n            aud: 'https://oauth2.googleapis.com/token',\n            kid: options[:kid].to_s,\n            exp: exp,\n            iat: iat }\n\npkey = options[:pkey].to_s.gsub('\\n', \"\\n\")\npriv_key = OpenSSL::PKey::RSA.new(pkey)\n\ntoken = JWT.encode(payload, priv_key, 'RS256')\n\nputs token\nThe important part is happening at the bottom: JWT.encode encodes the payload of the POST request, which consists of the API key from the service account. Specifically, the secret of the respective key pair (named priv_key here) is used to encrypt the payload.\nNext, this JWT needs to be passed to the https://oauth2.googleapis.com/token endpoint to ask for a Bearer access token by using the following payload in the body:\n- name: \"Google Workspace: Get access token from Google oauth2\"\n  uri:\n    url: \"https://oauth2.googleapis.com/token\"\n    method: POST\n    body: \"grant_type={% raw %}{{ google_workspace_oauth2_grant_type }}&assertion={{ jwt.stdout }}{% endraw %}\"\n    return_content: true\n  register: token\nHere, google_workspace_oauth2_grant_type needs to be \"urn%3Aietf%3Aparams%3Aoauth%3Agrant-type%3Ajwt-bearer\". This tells the endpoint that we are handing over a JWT token and want to get a Bearer token back.\nFinally, this (short-lived) Bearer token can be used to issue the desired API call, e.g. creating a new user:\n- name: \"Google Workspace: Create user\"\n  uri:\n    method: POST\n    url: https://admin.googleapis.com/admin/directory/v1/users\n    headers:\n      authorization: \"Bearer {% raw %}{{ token.json.access_token }}{% endraw %}\"\n    body_format: json\n    body: '{\n           \"primaryEmail\": \"{% raw %}{{ username }}{% endraw %}@email.com\",\n           \"password\": \"{% raw %}{{ user_password }}{% endraw %}\",\n           \"name\": {\n             \"givenName\": \"{% raw %}{{ first_name }}{% endraw %}\",\n             \"familyName\": \"{% raw %}{{ last_name }}{% endraw %}\"\n           },\n           \"isAdmin\": \"{% raw %}{{ admin }}{% endraw %}\"\n         }'"
  },
  {
    "objectID": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#summary",
    "href": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#summary",
    "title": "Accessing Google’s API via OAuth2",
    "section": "Summary",
    "text": "Summary\nThe OAuth2-API-Auth process to authenticate against the Google API is quite cumbersome and quite a few little things can go wrong. As for all other methods, it is not possible to say how long this method will stay functional. JWTs are a quite promising concept and it is likely that they will be around for quite some time as they are considered pretty save. The biggest challenge is usually to puzzle all bits together and find the correct documentation resource for the respective provider. Once it works, there’s almost no overhead when using tools like Ansible to automate the process.\nIt should be noted that the approach is quite generic: for some providers you might need to change the encoding algorithm when creating the JWT (e.g. for Zoom it needs to be HS256) but other than that you should be able to reuse the jwt.rb script."
  },
  {
    "objectID": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#full-ansible-script",
    "href": "blog/posts/2022-05-14-accessing-google-api-via-oauth2/index.html#full-ansible-script",
    "title": "Accessing Google’s API via OAuth2",
    "section": "Full Ansible script",
    "text": "Full Ansible script\n\n- name: \"Google Workspace: Create JWT for Google OAuth2\"\n  command: &gt;\n    env ruby &lt;path/to/&gt;/jwt.rb --iss \"google-workspace@&lt;some service account name&gt;.iam.gserviceaccount.com\"\n    --sub \"&lt;issuer email&gt;\" --scope \"{% raw %}{{ google_workspace_oauth2_api_scopes | join(' ') }}{% endraw %}\"\n    --kid \"{% raw %}{{ google_workspace_oauth2_key_id.value }}{% endraw %}\"\n    --pkey \"{% raw %}{{ google_workspace_oauth2_private_key.value }}{% endraw %}\"\n  args: { chdir: \"/usr/bin/\" }\n  register: jwt\n\n- name: \"Google Workspace: Get access token from Google oauth2\"\n  uri:\n    url: \"https://oauth2.googleapis.com/token\"\n    method: POST\n    body: \"grant_type={% raw %}{{ google_workspace_oauth2_grant_type }}{% endraw %}&assertion={% raw %}{{ jwt.stdout }}{% endraw %}\"\n    return_content: true\n  register: token\n\n- name: \"Google Workspace: Create user\"\n  uri:\n    method: POST\n    url: https://admin.googleapis.com/admin/directory/v1/users\n    headers:\n      authorization: \"Bearer {% raw %}{{ token.json.access_token }}{% endraw %}\"\n    body_format: json\n    body: '{\n           \"primaryEmail\": \"{% raw %}{{ username }}{% endraw %}@email.com\",\n           \"password\": \"{% raw %}{{ user_password }}{% endraw %}\",\n           \"name\": {\n             \"givenName\": \"{% raw %}{{ first_name }}{% endraw %}\",\n             \"familyName\": \"{% raw %}{{ last_name }}{% endraw %}\"\n           },\n           \"isAdmin\": \"{% raw %}{{ admin }}{% endraw %}\"\n         }'"
  },
  {
    "objectID": "blog/posts/2020-08-25-git-multiple-identities/index.html",
    "href": "blog/posts/2020-08-25-git-multiple-identities/index.html",
    "title": "Maintaining multiple identities with Git",
    "section": "",
    "text": "When committing to a Git repository related to my consulting work, I must use my company e-mail address, kirill@cynkra.com. Not so much for my open-source work – for this, I prefer to use other e-mail addresses, like krlmlr+r@mailbox.org . (For example, Travis CI sends notification e-mails to the committer’s e-mail address, and I have set up filtering for that other address.)\n\n\n\nPhoto by Carson Arias\n\n\n\nHaving to configure the e-mail address for each repository separately eventually gets annoying. Instead, I’d rather have all repos within a specific subdirectory use particular e-mail address.\nAll my Git repos live in ~/git. Subdirectories R and cynkra, contain R packages and repos related to consulting, respectively. To achieve the desired setup, I edit my ~/.gitconfig with the following entry:\n[includeIf \"gitdir:git/**\"]\n    path = git/.gitconfig\nThis ensures that all repos in the git directory use the git/.gitconfig file in addition to the main configuration. That file contains the following:\n[includeIf \"gitdir:R/**\"]\n    path = R/.gitconfig\n[includeIf \"gitdir:cynkra/**\"]\n    path = cynkra/.gitconfig\nFinally, in ~/git/R/.gitconfig and ~/git/cynkra/.gitconfig, I configure the e-mail addresses I want to use for all repos pertaining to R and cynkra, respectively.\n[user]\n    email = ...\nI verify the setup with git config -l | grep user. Indeed, cynkra repos use the cynkra e-mail address. Voilà!\nThe above approach requires a recent-ish version of git- version 2.14 or later should suffice. Read more about conditional includes."
  },
  {
    "objectID": "blog/posts/2022-04-27-data-scientist/index.html",
    "href": "blog/posts/2022-04-27-data-scientist/index.html",
    "title": "Data Scientist (80-100%)",
    "section": "",
    "text": "We are hiring a data scientist! You enjoy working with R and know a bit about Shiny, the tidyverse, and Git. We offer work on interesting projects around the R ecosystem, with lots of freedom and flexibility and the option to work remotely.\n\n\n\nPhoto by Scott Webb\n\n\n\n\n\nWhat we look for\n\nVery good knowledge of R, in particular Shiny and the tidyverse\nBasic knowledge of Git\nExperience with databases is a plus\n80%-100% commitment, can be adapted throughout the year\nAbility and desire to learn and improve on the job\nGood working knowledge of written English\nVery good command of spoken English or German\n\n\n\nWhat we offer\n\nInteresting projects around consulting and open-source software development\nRemote first: work wherever (and whenever) you want, some on-site presence might be required\nFocused work and asynchronous communication, keeping meetings to a minimum\nCompassionate learning-oriented work environment: we help each other grow and get better at what we do\nFull flexibility: we adjust to your needs, especially if you have family commitments\nNice offices in Zurich: Stauffacher (downtown), Zürich Affoltern (near ETH Hönggerberg)\nAttractive compensation\n\n\n\nHow to apply\nPlease submit your application via mail@cynkra.com, or share a private GitHub repository with us. Get in touch with us if you have further questions.\n\n\nWho we are\ncynkra is a Zurich-based data consulting company with a strong focus on R. We use R and the tidyverse in the vast majority of our projects. We support businesses and organizations by helping them pick the right tools, implementing solutions, training, and code review. We are enthusiastic about open-source software and contribute to it, too. Learn more at www.cynkra.com.\n\n\nYou are welcome!\nWe are an equal opportunity employer who recruits, employs, trains, compensates and promotes regardless of age, color, disability, ethnicity, family or marital status, gender identity or expression, language, national origin, physical and mental ability, political affiliation, race, religion, sexual orientation, socio-economic status, and other characteristics that make our employees unique. We are committed to fostering, cultivating and preserving a culture of diversity, equity and inclusion."
  },
  {
    "objectID": "blog/posts/2022-04-19-seasonal-1.9/index.html",
    "href": "blog/posts/2022-04-19-seasonal-1.9/index.html",
    "title": "seasonal 1.9: Accessing composite output",
    "section": "",
    "text": "seasonal is an easy-to-use and full-featured R interface to X-13ARIMA-SEATS, the seasonal adjustment software developed by the United States Census Bureau. The latest CRAN version of seasonal fixes several bugs and makes it easier to access output from multiple objects. See here for a complete list of changes.\n\n\n\nPhoto by Aaron Burden\n\n\n\nseas() is the core function of the seasonal package. By default, seas() calls the automatic procedures of X-13ARIMA-SEATS to perform a seasonal adjustment that works well in most circumstances:\nlibrary(seasonal)\nseas(AirPassengers)\nFor a more detailed introduction, read our article in the Journal of Statistical Software.\n\nMultiple series adjustment\nThe previous version has introduced the adjustment of multiple series in a single call to seas(). This has removed the need for loops or lapply() in such cases and finally brought the composite spec to seasonal.\nAs Brian Monsell pointed out, this was not enough to access the output from the composite spec. The latest CRAN version fixes this problem.\nMultiple adjustments can be performed by supplying multiple time series as an \"mts\" object:\n\nlibrary(seasonal)\nm0 &lt;- seas(cbind(fdeaths, mdeaths), x11 = \"\")\nfinal(m0)\n\n          fdeaths  mdeaths\nJan 1974 614.1235 1598.740\nFeb 1974 542.3500 1492.127\nMar 1974 613.5029 1443.238\nApr 1974 591.5725 1694.643\nMay 1974 607.4970 1696.021\nJun 1974 543.8415 1558.886\nJul 1974 597.0745 1663.176\nAug 1974 587.0533 1623.498\nSep 1974 588.2693 1741.394\nOct 1974 735.6666 1735.516\nNov 1974 602.0218 1665.590\nDec 1974 496.3985 1394.097\nJan 1975 564.0055 1560.605\nFeb 1975 591.0320 1708.763\nMar 1975 585.7739 1652.994\nApr 1975 581.5294 1671.265\nMay 1975 537.8055 1588.605\nJun 1975 584.3284 1600.979\nJul 1975 566.4872 1541.099\nAug 1975 617.1197 1623.445\nSep 1975 516.4781 1521.497\nOct 1975 559.0481 1577.200\nNov 1975 561.6315 1602.659\nDec 1975 580.9778 1569.692\nJan 1976 519.8106 1477.855\nFeb 1976 882.3725 2180.616\nMar 1976 674.5057 1744.114\nApr 1976 467.4502 1366.628\nMay 1976 509.7854 1344.809\nJun 1976 553.5233 1434.662\nJul 1976 503.2795 1447.952\nAug 1976 494.2373 1383.932\nSep 1976 529.1840 1453.496\nOct 1976 570.4128 1435.912\nNov 1976 590.4285 1540.551\nDec 1976 587.0971 1572.631\nJan 1977 583.2427 1607.153\nFeb 1977 498.9514 1287.403\nMar 1977 500.4632 1306.324\nApr 1977 569.2076 1685.581\nMay 1977 565.6470 1405.231\nJun 1977 509.3196 1432.968\nJul 1977 548.4062 1414.216\nAug 1977 523.6985 1444.945\nSep 1977 563.3014 1402.720\nOct 1977 495.6653 1427.458\nNov 1977 453.9859 1307.828\nDec 1977 502.3045 1268.618\nJan 1978 535.2658 1415.724\nFeb 1978 633.7605 1790.002\nMar 1978 559.2936 1469.883\nApr 1978 485.5062 1343.715\nMay 1978 590.4080 1509.166\nJun 1978 574.4467 1464.288\nJul 1978 571.2263 1428.398\nAug 1978 542.3579 1424.622\nSep 1978 551.2099 1422.428\nOct 1978 557.6905 1399.404\nNov 1978 479.8979 1199.762\nDec 1978 550.4253 1397.023\nJan 1979 548.9834 1557.853\nFeb 1979 576.9922 1425.717\nMar 1979 549.3468 1393.788\nApr 1979 546.4590 1449.784\nMay 1979 525.9881 1359.843\nJun 1979 550.2481 1330.113\nJul 1979 533.7558 1373.156\nAug 1979 566.6884 1381.653\nSep 1979 552.6500 1377.175\nOct 1979 533.9571 1337.640\nNov 1979 557.9707 1414.101\nDec 1979 475.5049 1038.506\n\n\nThis performs two seasonal adjustments, one for fdeaths and one for mdeaths. The vignette on multiple adjustments describes how to specify options for individual series.\n\n\nAccessing composite output\nThe composite argument is a list with an X-13 specification applied to the aggregated series:\n\nm1 &lt;- seas(\n  cbind(mdeaths, fdeaths),\n  composite = list(),\n  series.comptype = \"add\"\n)\n\nWith version 1.9 can now use out() to access the output of the composite spec:\nout(m1)\nWe can also use series(), e.g., to access the final, indirectly adjusted series via the composite spec (see ?series for all available series):\n\nseries(m1, \"composite.indseasadj\")\n\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n1974 2172.614 2053.613 2057.679 2284.821 2260.974 2105.191 2240.895 2185.517\n1975 2098.251 2298.581 2213.878 2256.802 2111.628 2181.738 2098.883 2219.083\n1976 1969.128 3078.359 2373.028 1846.802 1851.167 1983.231 1943.369 1872.025\n1977 2132.860 1807.832 1795.898 2262.793 1957.817 1940.262 1949.784 1953.665\n1978 1908.154 2431.050 2007.252 1830.715 2077.654 2033.120 1987.875 1956.487\n1979 2061.627 1997.557 1925.365 1984.834 1885.778 1882.208 1903.203 1937.474\n          Sep      Oct      Nov      Dec\n1974 2296.345 2395.347 2291.835 2013.749\n1975 2024.988 2100.543 2181.495 2234.826\n1976 1967.742 1973.567 2151.597 2199.389\n1977 1946.642 1894.848 1807.119 1808.051\n1978 1959.824 1928.748 1724.336 1966.847\n1979 1925.618 1846.713 1991.679 1517.027"
  },
  {
    "objectID": "blog/posts/2021-03-09-seasonal-1.8/index.html",
    "href": "blog/posts/2021-03-09-seasonal-1.8/index.html",
    "title": "Seasonal Adjustment of Multiple Series",
    "section": "",
    "text": "seasonal is an easy-to-use and full-featured R-interface to X-13ARIMA-SEATS, the seasonal adjustment software developed by the United States Census Bureau. The latest CRAN version of seasonal makes it much easier to adjust multiple time series.\n\n\n\nPhoto by Meriç Dağlı\n\n\n\nseasonal depends on the x13binary package to access pre-built binaries of X-13ARIMA-SEATS on all platforms and does not require any manual installation. To install both packages:\ninstall.packages(\"seasonal\")\nseas is the core function of the seasonal package. By default, seas calls the automatic procedures of X-13ARIMA-SEATS to perform a seasonal adjustment that works well in most circumstances:\nseas(AirPassengers)\nFor a more detailed introduction, read our article in the Journal of Statistical Software.\n\nMultiple Series Adjusmtent\nIn the latest CRAN version 1.8, it is now possible to seasonally adjust multiple series in a single call to seas(). This is done by using the built-in batch mode of X-13. It removes the need for loops or lapply() in such cases and finally brings one missing feature of X-13 to seasonal – the composite spec.\nMultiple adjustments can be performed by supplying multiple time series as an \"mts\" object:\n\nlibrary(seasonal)\nm &lt;- seas(cbind(fdeaths, mdeaths), x11 = \"\")\nfinal(m)\n\n          fdeaths  mdeaths\nJan 1974 614.1235 1598.740\nFeb 1974 542.3500 1492.127\nMar 1974 613.5029 1443.238\nApr 1974 591.5725 1694.643\nMay 1974 607.4970 1696.021\nJun 1974 543.8415 1558.886\nJul 1974 597.0745 1663.176\nAug 1974 587.0533 1623.498\nSep 1974 588.2693 1741.394\nOct 1974 735.6666 1735.516\nNov 1974 602.0218 1665.590\nDec 1974 496.3985 1394.097\nJan 1975 564.0055 1560.605\nFeb 1975 591.0320 1708.763\nMar 1975 585.7739 1652.994\nApr 1975 581.5294 1671.265\nMay 1975 537.8055 1588.605\nJun 1975 584.3284 1600.979\nJul 1975 566.4872 1541.099\nAug 1975 617.1197 1623.445\nSep 1975 516.4781 1521.497\nOct 1975 559.0481 1577.200\nNov 1975 561.6315 1602.659\nDec 1975 580.9778 1569.692\nJan 1976 519.8106 1477.855\nFeb 1976 882.3725 2180.616\nMar 1976 674.5057 1744.114\nApr 1976 467.4502 1366.628\nMay 1976 509.7854 1344.809\nJun 1976 553.5233 1434.662\nJul 1976 503.2795 1447.952\nAug 1976 494.2373 1383.932\nSep 1976 529.1840 1453.496\nOct 1976 570.4128 1435.912\nNov 1976 590.4285 1540.551\nDec 1976 587.0971 1572.631\nJan 1977 583.2427 1607.153\nFeb 1977 498.9514 1287.403\nMar 1977 500.4632 1306.324\nApr 1977 569.2076 1685.581\nMay 1977 565.6470 1405.231\nJun 1977 509.3196 1432.968\nJul 1977 548.4062 1414.216\nAug 1977 523.6985 1444.945\nSep 1977 563.3014 1402.720\nOct 1977 495.6653 1427.458\nNov 1977 453.9859 1307.828\nDec 1977 502.3045 1268.618\nJan 1978 535.2658 1415.724\nFeb 1978 633.7605 1790.002\nMar 1978 559.2936 1469.883\nApr 1978 485.5062 1343.715\nMay 1978 590.4080 1509.166\nJun 1978 574.4467 1464.288\nJul 1978 571.2263 1428.398\nAug 1978 542.3579 1424.622\nSep 1978 551.2099 1422.428\nOct 1978 557.6905 1399.404\nNov 1978 479.8979 1199.762\nDec 1978 550.4253 1397.023\nJan 1979 548.9834 1557.853\nFeb 1979 576.9922 1425.717\nMar 1979 549.3468 1393.788\nApr 1979 546.4590 1449.784\nMay 1979 525.9881 1359.843\nJun 1979 550.2481 1330.113\nJul 1979 533.7558 1373.156\nAug 1979 566.6884 1381.653\nSep 1979 552.6500 1377.175\nOct 1979 533.9571 1337.640\nNov 1979 557.9707 1414.101\nDec 1979 475.5049 1038.506\n\n\nThis will perform two seasonal adjustments, one for fdeaths and one for mdeaths. X-13 spec-argument combinations can be applied in the usual way, such as x11 = \"\". Note that if entered that way, they will apply to both series. The vignette on multiple adjustments describes how to specify options for individual series.\n\n\nBackend\nX-13 ships with a batch mode that allows multiple adjustments in a single call to X-13. This is now the default in seasonal (multimode = \"x13\"). Alternatively, X-13 can be called for each series (multimode = \"R\"). The results should be usually the same, but switching to multimode = \"R\" may be useful for debugging:\n\nseas(cbind(fdeaths, mdeaths), multimode = \"x13\")\n\n$fdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"x13\")\n\nCoefficients:\n      Constant      AO1976.Feb  MA-Seasonal-12  \n      -0.01578         0.43345         0.63119  \n\n\n$mdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"x13\")\n\nCoefficients:\n       AO1976.Feb         LS1976.Apr         AO1977.Apr         AO1978.Feb  \n           0.3319            -0.1330             0.1957             0.2305  \n       AO1979.Dec  MA-Nonseasonal-01     MA-Seasonal-12  \n          -0.3149            -0.3854             0.6120  \n\n\n$call\nseas(x = cbind(fdeaths, mdeaths), multimode = \"x13\")\n\nattr(,\"class\")\n[1] \"seas_multi\" \"list\"      \n\nseas(cbind(fdeaths, mdeaths), multimode = \"R\")\n\n$fdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"R\")\n\nCoefficients:\n      Constant      AO1976.Feb  MA-Seasonal-12  \n      -0.01578         0.43345         0.63119  \n\n\n$mdeaths\n\nCall:\nseas(x = cbind(fdeaths, mdeaths), multimode = \"R\")\n\nCoefficients:\n       AO1976.Feb         LS1976.Apr         AO1977.Apr         AO1978.Feb  \n           0.3319            -0.1330             0.1957             0.2305  \n       AO1979.Dec  MA-Nonseasonal-01     MA-Seasonal-12  \n          -0.3149            -0.3854             0.6120  \n\n\n$call\nseas(x = cbind(fdeaths, mdeaths), multimode = \"R\")\n\nattr(,\"class\")\n[1] \"seas_multi\" \"list\"      \n\n\nIn general, multimode = \"x13\" is faster. The following comparison on a MacBook Pro shows a modest speed gain, but bigger differences have been observed on other systems:\nmany &lt;- rep(list(fdeaths), 100)\nsystem.time(seas(many, multimode = \"x13\"))\n#   user  system elapsed\n#  9.415   0.653  10.079\nsystem.time(seas(many, multimode = \"R\"))\n#   user  system elapsed\n# 11.130   1.039  12.324\n\n\ncomposite spec\nSupport for the X-13 batch mode makes it finally possible to use the composite spec – the one feature of X-13 that was missing in seasonal. Sometimes, one has to decide whether seasonal adjustment should be performed on a granular level or on an aggregated level. The composite spec helps you to analyze the problem and to compare the direct and the indirect adjustments.\nThe composite argument is a list with an X-13 specification that is applied on the aggregated series. Specification works identically for other series in seas(), including the application of the defaults. If you provide an empty list, the usual defaults of seas() are used. A minimal composite call looks like this:\n\nseas(\n  cbind(mdeaths, fdeaths),\n  composite = list(),\n  series.comptype = \"add\"\n)\n\n$mdeaths\n\nCall:\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nCoefficients:\n       AO1976.Feb         LS1976.Apr         AO1977.Apr         AO1978.Feb  \n           0.3319            -0.1330             0.1957             0.2305  \n       AO1979.Dec  MA-Nonseasonal-01     MA-Seasonal-12  \n          -0.3149            -0.3854             0.6120  \n\n\n$fdeaths\n\nCall:\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nCoefficients:\n      Constant      AO1976.Feb  MA-Seasonal-12  \n      -0.01578         0.43345         0.63119  \n\n\n$composite\n\nCall:\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nCoefficients:\n         Constant         AO1976.Feb  MA-Nonseasonal-01     MA-Seasonal-12  \n         -0.03133            0.31247           -0.43509            0.99937  \n\n\n$call\nseas(x = cbind(mdeaths, fdeaths), composite = list(), series.comptype = \"add\")\n\nattr(,\"class\")\n[1] \"seas_multi\" \"list\"      \n\n\nYou can verify that the composite refers to the total of mdeaths and fdeaths by running:\n\nseas(ldeaths)\n\n\nCall:\nseas(x = ldeaths)\n\nCoefficients:\n         Constant         AO1976.Feb  MA-Nonseasonal-01     MA-Seasonal-12  \n         -0.03133            0.31247           -0.43509            0.99937  \n\n\nwhere ldeaths is the sum of mdeaths and fdeaths.\n\n\nAcknowledgement\nMany thanks to Severin Thöni and Matthias Bannert, for demonstrating the benefits of the X-13 batch mode. Also to the ETH KOF, for partially funding this development."
  },
  {
    "objectID": "blog/posts/2021-09-18-tsbox-03/index.html",
    "href": "blog/posts/2021-09-18-tsbox-03/index.html",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "",
    "text": "The tsbox package provides a set of tools that are agnostic towards existing time series classes. The tools also allow you to handle time series as plain data frames, thus making it easy to deal with time series in a dplyr or data.table workflow.\nVersion 0.3.1 is now on CRAN and provides several bugfixes and extensions (see here for the full change log). A detailed overview of the package functionality is given in the documentation page (or in an older blog-post)."
  },
  {
    "objectID": "blog/posts/2021-09-18-tsbox-03/index.html#new-and-extended-functionality",
    "href": "blog/posts/2021-09-18-tsbox-03/index.html#new-and-extended-functionality",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "New and extended functionality",
    "text": "New and extended functionality\n\nts_frequency(): changes the frequency of a time series. It is now possible to aggregate any time series to years, quarters, months, weeks, days, hours, minutes or seconds. For low- to high-frequency conversion, the tempdisagg package can now convert low frequency to high frequency and has support for ts-boxable objects. E.g.:\nlibrary(tsbox)\nx &lt;- ts_tbl(EuStockMarkets)\nx\n#&gt; # A tibble: 7,440 × 3\n#&gt;   id    time                 value\n#&gt;   &lt;chr&gt; &lt;dttm&gt;               &lt;dbl&gt;\n#&gt; 1 DAX   1991-07-01 03:18:27  1629.\n#&gt; 2 DAX   1991-07-02 13:01:32  1614.\n#&gt; 3 DAX   1991-07-03 22:44:38  1607.\n#&gt; 4 DAX   1991-07-05 08:27:43  1621.\n#&gt; 5 DAX   1991-07-06 18:10:48  1618.\n#&gt; # … with 7,435 more rows\n\nts_frequency(x, \"week\")\n#&gt; # A tibble: 1,492 × 3\n#&gt;   id    time        value\n#&gt;   &lt;chr&gt; &lt;date&gt;      &lt;dbl&gt;\n#&gt; 1 DAX   1991-06-30  1618.\n#&gt; 2 DAX   1991-07-07  1633.\n#&gt; 3 DAX   1991-07-14  1632.\n#&gt; 4 DAX   1991-07-21  1620.\n#&gt; 5 DAX   1991-07-28  1616.\n#&gt; # … with 1,487 more rows\nts_index(): returns an indexed series, with a value of 1 at the base period. This base period can now be specified more flexibly. E.g., the average of a year can defined as 1 (which is a common use case).\nts_na_interpolation(): A new function that wraps imputeTS::na_interpolation() from the imputeTS package and allows the imputation of missing values for any time series object.\nts_first_of_period(): A new function that replaces the date or time value by the first of the period. This is useful because tsbox usually relies on timestamps being the first of a period. The following monthly series has an offset of 14 days. ts_first_of_period() changes the timestamp to the first date of each month:\nx &lt;- ts_lag(ts_tbl(mdeaths), \"14 days\")\nx\n#&gt; # A tibble: 72 × 2\n#&gt;   time       value\n#&gt;   &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 1974-01-15  2134\n#&gt; 2 1974-02-15  1863\n#&gt; 3 1974-03-15  1877\n#&gt; 4 1974-04-15  1877\n#&gt; 5 1974-05-15  1492\n#&gt; # … with 67 more rows\n\nts_first_of_period(x)\n#&gt; # A tibble: 72 × 2\n#&gt;   time       value\n#&gt;   &lt;date&gt;     &lt;dbl&gt;\n#&gt; 1 1974-01-01  2134\n#&gt; 2 1974-02-01  1863\n#&gt; 3 1974-03-01  1877\n#&gt; 4 1974-04-01  1877\n#&gt; 5 1974-05-01  1492\n#&gt; # … with 67 more rows"
  },
  {
    "objectID": "blog/posts/2021-09-18-tsbox-03/index.html#convert-everything-to-everything",
    "href": "blog/posts/2021-09-18-tsbox-03/index.html#convert-everything-to-everything",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "Convert everything to everything",
    "text": "Convert everything to everything\ntsbox is built around a set of converters, which convert time series stored as ts, xts, data.frame, data.table, tibble, zoo, tsibble, tibbletime, tis, irts or timeSeries to each other:\nlibrary(tsbox)\nx.ts &lt;- ts_c(fdeaths, mdeaths)\nx.xts &lt;- ts_xts(x.ts)\nx.df &lt;- ts_df(x.xts)\nx.dt &lt;- ts_dt(x.df)\nx.tbl &lt;- ts_tbl(x.dt)\nx.zoo &lt;- ts_zoo(x.tbl)\nx.tsibble &lt;- ts_tsibble(x.zoo)\nx.tibbletime &lt;- ts_tibbletime(x.tsibble)\nx.timeSeries &lt;- ts_timeSeries(x.tibbletime)\nx.irts &lt;- ts_irts(x.tibbletime)\nx.tis &lt;- ts_tis(x.irts)\nall.equal(ts_ts(x.tis), x.ts)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "blog/posts/2021-09-18-tsbox-03/index.html#use-same-functions-for-time-series-classes",
    "href": "blog/posts/2021-09-18-tsbox-03/index.html#use-same-functions-for-time-series-classes",
    "title": "tsbox 0.3.1: extended functionality",
    "section": "Use same functions for time series classes",
    "text": "Use same functions for time series classes\nBecause this works reliably, it is easy to define a toolkit that works for all classes. So, whether we want to smooth, scale, differentiate, chain, forecast, regularize, impute or seasonally adjust a time series, we can use the same commands to whatever time series class at hand:\nts_trend(x.ts)   # estimate a trend line\nts_pc(x.xts)     # calculate percentage change rates (period on period)\nts_pcy(x.df)     # calculate percentage change rates (year on year)\nts_lag(x.dt)     # lagged series\nThere are many more. Because they all start with ts_, you can use auto-complete to see what’s around. Most conveniently, there is a time series plot function that works for all classes and frequencies:\nts_plot(\n  `Airline Passengers` = AirPassengers,\n  `Lynx trappings` = ts_tis(lynx),\n  `Deaths from Lung Diseases` = ts_xts(fdeaths),\n  title = \"Airlines, trappings, and deaths\",\n  subtitle = \"Monthly passengers, annual trappings, monthly deaths\"\n)\n\n\n\ntime series plot"
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html",
    "href": "blog/posts/2018-05-01-dbi-2/index.html",
    "title": "Done “Establishing DBI”!?",
    "section": "",
    "text": "The “Establishing DBI” project, funded by the R consortium, started about a year ago. It includes the completion of two new backends, RPostgres and RMariaDB, and quite a few interface extensions and specifications.\nLearn more about DBI, R’s database interface, on https://r-dbi.org.\nThis blog post showcases only the visible changes, a substantial amount of work went into extending the DBI specification and making the three open-source database backends compliant to it. After describing the release of the two new backends RMariaDB and RPostgres, I’ll be discussing the following improvements:\nI conclude with an outlook on things left to do."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#release-of-rpostgres-and-rmariadb",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#release-of-rpostgres-and-rmariadb",
    "title": "Done “Establishing DBI”!?",
    "section": "Release of RPostgres and RMariaDB",
    "text": "Release of RPostgres and RMariaDB\nThe DBI specification has been formulated in the preceding R consortium project, “Improving DBI”. It is both an automated test suite and a human-readable description of behavior, implemented in the DBItest package. For this project, I extended this specification and could also use it to implement RPostgres and RMariaDB: for once, test-driven development was pure pleasure because the tests were already there!\nI took over maintenance of the RPostgres and RMariaDB packages, which are complete rewrites of the RPostgreSQL and RMySQL packages, respectively. These packages use C++ (with Rcpp) as the glue between R and the native database libraries. A reimplementation and release under a different name has made it much easier to fully conform to the DBI specification: only listing temporary tables and casting to blob or character is not supported by RMariaDB (due to a limitation of the DBMS), all other parts of the specification are fully covered.\nProjects that use RPostgreSQL or RMySQL can continue to do so, or switch to the new backends at their own pace (which likely requires some changes to the code). For new projects I recommend RPostgres or RMariaDB to take advantage of the thorougly tested codebases and of the consistency across backends."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#schema-support",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#schema-support",
    "title": "Done “Establishing DBI”!?",
    "section": "Schema support",
    "text": "Schema support\nConsistent access of tables in database schemas was planned for the “Improving DBI” project already, but I have implemented it only recently. It felt safer to see how the interface works on three backends, as opposed to implementing it for just RSQLite and then perhaps having to adapt it.\nThe new Id() function constructs identifiers. All arguments must be named, yet DBI doesn’t specify the argument names because DBMS have an inconsistent notion of namespaces. The objects returned by Id() are “dumb”, they gain meaning only when used in methods such as dbQuoteIdentifier() or dbWriteTable().\nFor listing database objects in schemas, the new dbListObjects() generic can be used. It returns a data frame that contains identifiers (like those created by the Id() function) and a flag that indicates if the identifier is complete (i.e., pointing to a table or view) or a prefix. Incomplete identifiers can be passed to dbListObjects() again, which allows traversing the tree of database objects.\nThe following example assumes a schema my_schema. A table named my_table is created in this schema, objects are listed, and the table is read again.\nlibrary(RPostgres)\npg_conn &lt;- dbConnect(Postgres())\n\ntable_name &lt;- Id(schema = \"my_schema\", table = \"my_table\")\ntable_name\n\n## &lt;Id&gt; schema = my_schema, table = my_table\n\ndata &lt;- data.frame(a = 1:3, b = letters[1:3])\ndbWriteTable(pg_conn, table_name, data)\n\ndbListObjects(pg_conn)\n\n##                               table is_prefix\n## 1    &lt;Id&gt; table = geography_columns     FALSE\n## 2     &lt;Id&gt; table = geometry_columns     FALSE\n## 3      &lt;Id&gt; table = spatial_ref_sys     FALSE\n## 4       &lt;Id&gt; table = raster_columns     FALSE\n## 5     &lt;Id&gt; table = raster_overviews     FALSE\n## 6             &lt;Id&gt; table = topology     FALSE\n## 7                &lt;Id&gt; table = layer     FALSE\n## 8                 &lt;Id&gt; table = temp     FALSE\n## 9            &lt;Id&gt; schema = topology      TRUE\n## 10          &lt;Id&gt; schema = my_schema      TRUE\n## 11 &lt;Id&gt; schema = information_schema      TRUE\n## 12         &lt;Id&gt; schema = pg_catalog      TRUE\n## 13             &lt;Id&gt; schema = public      TRUE\n\ndbListObjects(\n  pg_conn,\n  prefix = Id(schema = \"my_schema\")\n)\n\n##                                       table is_prefix\n## 1 &lt;Id&gt; schema = my_schema, table = my_table     FALSE\n\ndbReadTable(pg_conn, table_name)\n\n##   a b\n## 1 1 a\n## 2 2 b\n## 3 3 c\nIn addition to dbReadTable() and dbWriteTable(), also dbExistsTable() and dbRemoveTable() and the new dbCreateTable() and dbAppendTable() (see below) support an Id() object as table name. The dbQuoteIdentifier() method converts these objects to SQL strings. Some operations (e.g. checking if a table exists) require the inverse, the new dbUnquoteIdentifier() generic takes care of converting valid SQL identifiers to (a list of) Id() objects:\nquoted &lt;- dbQuoteIdentifier(pg_conn, table_name)\nquoted\n\n## &lt;SQL&gt; \"my_schema\".\"my_table\"\n\ndbUnquoteIdentifier(pg_conn, quoted)\n\n## [[1]]\n## &lt;Id&gt; schema = my_schema, table = my_table\nThe new methods work consistently across backends - only RSQLite is currently restricted to the default schema. (Schemas in RSQLite are created by attaching another database, this use case seemed rather exotic but can be supported with the new infrastructure.)"
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#quoting-literal-values",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#quoting-literal-values",
    "title": "Done “Establishing DBI”!?",
    "section": "Quoting literal values",
    "text": "Quoting literal values\nWhen working on the database backends, it has become apparent that quoting strings and identifiers isn’t quite enough. Now there is a way to quote arbitrary values, that is, convert them to a string that can be pasted into an SQL query:\nlibrary(RSQLite)\nsqlite_conn &lt;- dbConnect(SQLite())\n\nlibrary(RMariaDB)\nmariadb_conn &lt;- dbConnect(MariaDB(), dbname = \"test\")\n\ndbQuoteLiteral(sqlite_conn, 1.5)\n\n## &lt;SQL&gt; 1.5\n\ndbQuoteLiteral(mariadb_conn, 1.5)\n\n## &lt;SQL&gt; 1.5\n\ndbQuoteLiteral(pg_conn, 1.5)\n\n## &lt;SQL&gt; 1.5::float8\n\ndbQuoteLiteral(mariadb_conn, Sys.time())\n\n## &lt;SQL&gt; '20180501204025'\n\ndbQuoteLiteral(pg_conn, Sys.time())\n\n## &lt;SQL&gt; '2018-05-01 22:40:25'::timestamp\nThe default implementation works for ANSI SQL compliant DBMS, the method for RPostgres takes advantage of the :: casting operator as seen in the examples."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#more-fine-grained-creation-of-tables",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#more-fine-grained-creation-of-tables",
    "title": "Done “Establishing DBI”!?",
    "section": "More fine-grained creation of tables",
    "text": "More fine-grained creation of tables\nDBI supports storing data frames as tables in the database via dbWriteTable(). This operation consists of multiple steps:\n\nChecking if a table of this name exists, if yes:\n\nIf overwrite = TRUE, removing the table\nIf not, throwing an error\n\nCreating the table with the correct field structure\nPreparing the data for writing\nWriting the data\n\nTo reduce complexity and allow for more options without cluttering the argument list of dbWriteTable(), DBI now provides generics for the individual steps:\n\nThe existing dbRemoveTable() generic has been extended with temporary and fail_if_missing arguments. Setting temporary = TRUE makes sure that only temporaries are removed. By default, trying to remove a table that doesn’t exist fails, setting fail_if_missing = FALSE changes this behavior to a silent success.\nThe new dbCreateTable() generic accepts a data frame or a character vector of DBMS data types and creates a table in the database. It builds upon the existing sqlCreateTable() generic and also supports the temporary argument. If a table by that name already exists, an error is raised.\nThe new dbAppendTable() generic uses a prepared statement (created via sqlAppendTableTemplate()) to efficiently insert rows into the database. This avoids the internal overhead of converting values to SQL literals.\n\nThe following example shows the creation and population of a table with the new methods.\ntable_name\n\n## &lt;Id&gt; schema = my_schema, table = my_table\n\ndbRemoveTable(pg_conn, table_name, fail_if_missing = FALSE)\n\ndbCreateTable(pg_conn, table_name, c(a = \"int8\", b = \"float8\"))\n\ndbAppendTable(pg_conn, table_name, data.frame(a = 1:3, b = 1:3))\n\n## [1] 3\n\nstr(dbReadTable(pg_conn, table_name))\n\n## 'data.frame':    3 obs. of  2 variables:\n##  $ a:integer64 1 2 3\n##  $ b: num  1 2 3\nThe dbWriteTable() methods in the three backends have been adapted to use the new methods."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#support-for-64-bit-integers",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#support-for-64-bit-integers",
    "title": "Done “Establishing DBI”!?",
    "section": "Support for 64-bit integers",
    "text": "Support for 64-bit integers\nAs seen in the previous example, 64-bit integers can be read from the database. The three backends RSQLite, RPostgres and RMariaDB now also support writing 64-bit integers via the bit64 package:\ndata &lt;- data.frame(a = bit64::as.integer64(4:6), b = 4:6)\ndbAppendTable(pg_conn, table_name, data)\n\n## [1] 3\n\nstr(dbReadTable(pg_conn, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a:integer64 1 2 3 4 5 6\n##  $ b: num  1 2 3 4 5 6\nBecause R still lacks support for native 64-bit integers, the bit64 package feels like the best compromise: the returned values can be computed on, or coerced to integer, numeric or even character depending on the application. In some cases, it may be useful to always coerce. This is where the new bigint argument to dbConnect() helps:\npg_conn_int &lt;- dbConnect(Postgres(), bigint = \"integer\")\nstr(dbReadTable(pg_conn_int, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a: int  1 2 3 4 5 6\n##  $ b: num  1 2 3 4 5 6\n\npg_conn_num &lt;- dbConnect(Postgres(), bigint = \"numeric\")\nstr(dbReadTable(pg_conn_num, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a: num  1 2 3 4 5 6\n##  $ b: num  1 2 3 4 5 6\n\npg_conn_chr &lt;- dbConnect(Postgres(), bigint = \"character\")\nstr(dbReadTable(pg_conn_chr, table_name))\n\n## 'data.frame':    6 obs. of  2 variables:\n##  $ a: chr  \"1\" \"2\" \"3\" \"4\" ...\n##  $ b: num  1 2 3 4 5 6\nThe bigint argument works consistently across the three backends RSQLite, RPostgres and RMariaDB, the DBI specification contains a test for and a description of the requirements."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#geometry-columns",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#geometry-columns",
    "title": "Done “Establishing DBI”!?",
    "section": "Geometry columns",
    "text": "Geometry columns\nPostgreSQL has support for user-defined data types, this is used e.g. by PostGIS to store spatial data. Before, user-defined data types were returned as character values, with a warning. Thanks to a contribution by Etienne B. Racine:\n\nthe warnings are gone,\nthe user-defined data type is now stored in an attribute of the column in the data frame,\ndetails on columns with user-defined data types are available in dbColumnInfo().\n\n\ndbCreateTable(\n  pg_conn,\n  \"geom_test\",\n  c(id = \"int4\", geom = \"geometry(Point, 4326)\")\n)\n\ndata &lt;- data.frame(\n  id = 1,\n  geom = \"SRID=4326;POINT(-71.060316 48.432044)\",\n  stringsAsFactors = FALSE\n)\ndbAppendTable(pg_conn, \"geom_test\", data)\n\n## [1] 1\n\nstr(dbReadTable(pg_conn, \"geom_test\"))\n\n## 'data.frame':    1 obs. of  2 variables:\n##  $ id  : int 1\n##  $ geom:Class 'pq_geometry'  chr \"0101000020E61000003CDBA337DCC351C06D37C1374D374840\"\n\nres &lt;- dbSendQuery(pg_conn, \"SELECT * FROM geom_test\")\ndbColumnInfo(res)\n\n##   name      type   .oid .known .typname\n## 1   id   integer     23   TRUE     int4\n## 2 geom character 101529  FALSE geometry\n\ndbClearResult(res)\nSpecial support for geometry columns is currently available only in RPostgres."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#duplicate-column-names",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#duplicate-column-names",
    "title": "Done “Establishing DBI”!?",
    "section": "Duplicate column names",
    "text": "Duplicate column names\nThe specification has been extended to disallow duplicate, empty, or NA column names. The deduplication used by our three backends is similar to that used by tibble::set_tidy_names(), but the DBI specification does not require any particular deduplication mechanism. Syntactic names aren’t required either:\ndbGetQuery(sqlite_conn, \"SELECT 1, 2, 3\")\n\n##   1 2 3\n## 1 1 2 3\n\ndbGetQuery(sqlite_conn, \"SELECT 1 AS a, 2 AS a, 3 AS `a..2`\")\n\n##   a a..2 a..3\n## 1 1    2    3\n\ndbGetQuery(mariadb_conn, \"SELECT 1, 2, 3\")\n\n##   1 2 3\n## 1 1 2 3\n\ndbGetQuery(mariadb_conn, \"SELECT 1 AS a, 2 AS a, 3 AS `a..2`\")\n\n##   a a..2 a..3\n## 1 1    2    3\n\ndbGetQuery(pg_conn, \"SELECT 1, 2, 3\")\n\n##   ?column? ?column?..2 ?column?..3\n## 1        1           2           3\n\ndbGetQuery(pg_conn, 'SELECT 1 AS a, 2 AS a, 3 AS \"a..2\"')\n\n##   a a..2 a..3\n## 1 1    2    3"
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#helpers",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#helpers",
    "title": "Done “Establishing DBI”!?",
    "section": "Helpers",
    "text": "Helpers\nTwo little helper generics have been added.\nThe new dbIsReadOnly() generic (contributed by Anh Le) should return TRUE for a read-only connection. This is not part of the specification yet.\nThe dbCanConnect() tests a set of connection parameters. The default implementation simply connects and then disconnects upon success. For DBMS that can provide more efficient methods of checking connectivity, a lighter-weight implementation of this method may give a better experience.\nNone of the three backends currently provide specialized implementations for these generics."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#code-reuse",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#code-reuse",
    "title": "Done “Establishing DBI”!?",
    "section": "Code reuse",
    "text": "Code reuse\nI have made some efforts to extract common C++ classes for assembling data frames and prepare them for reuse. The C++ source code for the three backends contains files prefixed with Db, these are almost identical across the backends. The planned packaging into the RKazam package had to yield to higher-priority features described above.\nThe situation in the R code is similar: I have found myself copy-pasting code from one backend into another because I didn’t feel it’s ready (or standardized enough) to be included in the DBI package.\nFor both use cases, a code reuse strategy based on copying/updating template files or reconciling files may be more robust than the traditional importing mechanisms offered by R."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#outlook",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#outlook",
    "title": "Done “Establishing DBI”!?",
    "section": "Outlook",
    "text": "Outlook\nThe upcoming CRAN release of DBI, DBItest and the three backends RSQLite, RMariaDB, and RPostgres are important milestones. Stability is important when more and more users and projects use the new backends. Nevertheless, I see quite a few potential improvements that so far were out of scope of the “Improving DBI” and “Establishing DBI” projects:\n\nSupport running the test suite locally, to validate adherence to DBI for a particular installation.\nConsistent fast data import.\nConsistent query placeholders (currently $1 for RPostgres and ? for many other backends).\nSupport for arbitrary data types via hooks.\nAssistance with installation problems on specific architectures, or connectivity problems with certain databases, or other specific issues.\nRework the internal architecture of DBItest to simplify locating test failures.\nImprove the https://r-dbi.org website.\nNon-blocking queries.\n\nI have submitted another proposal to the R Consortium, hoping to receive support with these and other issues."
  },
  {
    "objectID": "blog/posts/2018-05-01-dbi-2/index.html#acknowledgments",
    "href": "blog/posts/2018-05-01-dbi-2/index.html#acknowledgments",
    "title": "Done “Establishing DBI”!?",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI’d like to thank the R Consortium for their generous financial support. Many thanks to the numerous contributors who helped make the past two projects a success.\nThis post appeared previously on https://r-dbi.org."
  },
  {
    "objectID": "blog/posts/2021-10-07-old-texlive/index.html",
    "href": "blog/posts/2021-10-07-old-texlive/index.html",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "",
    "text": "Rendering PDFs with rmarkdown requires a working LaTeX installation, such as tinytex. Occasionally, existing workflows break with the newest version of LaTeX. This post describes how to run an older LaTeX version for just a little while.\nRendering PDFs with rmarkdown requires a working LaTeX installation. The excellent tinytex package helps installing a portable variant of the TeXlive distribution with minimal fuss, on any major operating system. It is really as simple as:"
  },
  {
    "objectID": "blog/posts/2021-10-07-old-texlive/index.html#breaking-changes-in-latex",
    "href": "blog/posts/2021-10-07-old-texlive/index.html#breaking-changes-in-latex",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "Breaking changes in LaTeX?",
    "text": "Breaking changes in LaTeX?\nDespite its age, LaTeX is still a lively maintained and evolving system, with the consequence that some documents that worked on an older version of LaTeX may no longer work with the most recent stack. One such example is the tabu package that breaks for some use cases with TeXlive 2021.\nOn a clean tinytex installation, rendering an .rmd document with the following code starts with installing missing packages:\nknitr::kable(mtcars, booktabs = TRUE, longtable = TRUE) |&gt;\n  kableExtra::kable_styling(full_width = TRUE)\ntlmgr: package repository https://mirror.foobar.to/CTAN/systems/texlive/tlnet (not verified: gpg unavailable)\n[1/1, ??:??/??:??] install: multirow [3k]\nrunning mktexlsr ...\ndone running mktexlsr.\ntlmgr: package log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr.log\ntlmgr: command log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr-commands.log\ntlmgr: package repository https://mirror.foobar.to/CTAN/systems/texlive/tlnet (not verified: gpg unavailable)\n[1/1, ??:??/??:??] install: wrapfig [10k]\nrunning mktexlsr ...\n...\nYes, missing packages are installed on the fly! A full LaTeX distribution has several GB in size. The lazy package installation is a cool feature of the tinytex distribution that allows starting quickly with a minimal installation, without downloading and unpacking the whole thing.\nHowever, compilation gives the following error message:\ntlmgr: package log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr.log\ntlmgr: command log updated: ~/Library/TinyTeX/texmf-var/web2c/tlmgr-commands.log\n! Dimension too large.\n\\LT@max@sel #1#2-&gt;{\\ifdim #2=\\wd \\tw@\n                                      #1\\else \\number \\c@LT@chunks \\fi }{\\th...\nl.317 \\end{longtabu}\n\nError: LaTeX failed to compile test.tex. See https://yihui.org/tinytex/r/#debugging for debugging tips. See test.log for more info."
  },
  {
    "objectID": "blog/posts/2021-10-07-old-texlive/index.html#what-if-we-switch-to-the-previous-version",
    "href": "blog/posts/2021-10-07-old-texlive/index.html#what-if-we-switch-to-the-previous-version",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "What if we switch to the previous version?",
    "text": "What if we switch to the previous version?\n“But it used to work yesterday!” Fine, let’s install the last version that is still based on TeXlive 2020:\ntinytex::install_tinytex(version = \"2021.03\")\nUnfortunately, this breaks the package downloader:\nA new version of TeX Live has been released. If you need to install or update any LaTeX packages, you have to upgrade TinyTeX with tinytex::reinstall_tinytex(). If it fails to upgrade, you might be using a default random CTAN mirror that has not been fully synced to the main CTAN repository, and you need to wait for a few more days or use a CTAN mirror that is known to be up-to-date (see the \"repository\" argument on the help page ?tinytex::install_tinytex).\n\ntlmgr: Local TeX Live (2020) is older than remote repository (2021).\nCross release updates are only supported with\n  update-tlmgr-latest(.sh/.exe) --update\nSee https://tug.org/texlive/upgrade.html for details.\n! LaTeX Error: File `multirow.sty' not found.\n...\nAnd we’re told to upgrade TeXlive, which isn’t helpful in our particular use case."
  },
  {
    "objectID": "blog/posts/2021-10-07-old-texlive/index.html#can-we-make-the-previous-version-work",
    "href": "blog/posts/2021-10-07-old-texlive/index.html#can-we-make-the-previous-version-work",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "Can we make the previous version work?",
    "text": "Can we make the previous version work?\nYes. The solution is to teach tinytex to make do with a historic snapshot of the TeXlive distribution:\ntinytex::tlmgr(\"option repository https://ftp.tu-chemnitz.de/pub/tug/historic/systems/texlive/2020/tlnet-final\")\ntlmgr: setting default package repository to https://ftp.tu-chemnitz.de/pub/tug/historic/systems/texlive/2020/tlnet-final\ntlmgr: updating ~/Library/TinyTeX/tlpkg/texlive.tlpdb\nAfter that, automatic package downloads work again, with a prominent message reminding you that you’re running on a frozen snapshot:\nknitr::kable(mtcars, booktabs = TRUE, longtable = TRUE) |&gt;\n  kableExtra::kable_styling(full_width = TRUE)\nTeX Live 2020 is frozen and will no longer\nbe routinely updated. This happens in preparation for a new release.\n\nIf you're willing to help with pretesting the new release, and we hope\nyou are, (when pretests are available), please read\nhttps://tug.org/texlive/pretest.html.\n\nOtherwise, just wait, and the new release will be ready in due time.\nTeX Live 2020 is frozen and will no longer\nbe routinely updated. This happens in preparation for a new release.\n\nIf you're willing to help with pretesting the new release, and we hope\nyou are, (when pretests are available), please read\nhttps://tug.org/texlive/pretest.html.\n\nOtherwise, just wait, and the new release will be ready in due time.\ntlmgr: package repository https://ftp.tu-chemnitz.de/pub/tug/historic/systems/texlive/2020/tlnet-final (not verified: gpg unavailable)\n[1/1, ??:??/??:??] install: multirow [3k]\nrunning mktexlsr ...\ndone running mktexlsr.\ntlmgr: package log updated: /Users/kirill/Library/TinyTeX/texmf-var/web2c/tlmgr.log\nTeX Live 2020 is frozen and will no longer\nbe routinely updated. This happens in preparation for a new release.\n...\nThe document is now rendered without errors.\nThe tinytex maintainers have confirmed that old bundles remain available for download. Of course the correct solution is to avoid weakly maintained dependencies in your code, and to replace them by better solutions. In reality, this is not always feasible, and breakages may occur without notice.\nHappy freezing!"
  },
  {
    "objectID": "blog/posts/2021-10-07-old-texlive/index.html#details",
    "href": "blog/posts/2021-10-07-old-texlive/index.html#details",
    "title": "Running old versions of TeXlive with tinytex",
    "section": "Details",
    "text": "Details\nOn the TeXlive homepage, the “How to acquire TeX Live: other methods” link has a section “Past releases”. The tlmgr command is shown right there. The “Historic archive” page contains a list of historic mirrors, use a mirror close to you.\nI originally started my search at the “Historic archive” page. It took some time to find the correct command, it is also documented in the tlmgr manual.\nThe tinytex package provides the tlmgr_repo() function as a shortcut to set the repository, but it seems to fail currently for this use case. Thanks Christophe Dervieux and Florian Kohrt for the hints!"
  },
  {
    "objectID": "blog/posts/2020-02-09-tempdisagg/index.html",
    "href": "blog/posts/2020-02-09-tempdisagg/index.html",
    "title": "tempdisagg: converting quarterly time series to daily",
    "section": "",
    "text": "Not having a time series at the desired frequency is a common problem for researchers and analysts. For example, instead of quarterly sales, they only have annual sales. Instead of a daily stock market index, they only have a weekly index. While there is no way to fully make up for the missing data, there are useful workarounds: with the help of one or more high-frequency indicator series, the low-frequency series may be disaggregated into a high-frequency series.\nThe package tempdisagg implements the standard methods for temporal disaggregation: Denton, Denton-Cholette, Chow-Lin, Fernandez and Litterman. Our article on temporal disaggregation of time series in the R-Journal describes the package and the theory of temporal disaggregation in more detail.\nThe package has been around for eight years, enabling the standard year or quarter to month or quarter disaggregation. With version 1.0, there are now some major new features: disaggregation can be performed from any frequency to any frequency. Also, tempdisagg now supports time series classes other than ts."
  },
  {
    "objectID": "blog/posts/2020-02-09-tempdisagg/index.html#time-series-can-be-stored-in-data-frames",
    "href": "blog/posts/2020-02-09-tempdisagg/index.html#time-series-can-be-stored-in-data-frames",
    "title": "tempdisagg: converting quarterly time series to daily",
    "section": "Time series can be stored in data frames",
    "text": "Time series can be stored in data frames\nBecause we are dealing with daily data, we keep the data in a data.frame, rather than in a ts object. Other time series objects, such as xts and tsibble, are possible as well. For conversion and visualization, we use the tsbox package.\nlibrary(tsbox)\nts_plot(gdp.q, title = \"Swiss GDP\", subtitle = \"real, not seasonally adjusted\")\n\n\n\nSeries to disaggregate: quarterly gross domestic product of Switzerland"
  },
  {
    "objectID": "blog/posts/2020-02-09-tempdisagg/index.html#disaggregation-to-daily-frequency",
    "href": "blog/posts/2020-02-09-tempdisagg/index.html#disaggregation-to-daily-frequency",
    "title": "tempdisagg: converting quarterly time series to daily",
    "section": "Disaggregation to daily frequency",
    "text": "Disaggregation to daily frequency\nWhile disaggregation can also be performed without other series, we use Swiss stock market data as an indicator series to disaggregate GDP. Data of the stock market index, the SMI, is also included in tempdisagg. Weekend and holiday values have been interpolated, because td does not allow the presence of missing values.\nts_plot(spi.d, title = \"Swiss Performance Index\", subtitle = \"daily values, interpolated\")\n\n\n\nDaily indicator series: Swiss Performance Index\n\n\n\nThe following uses the Chow-Lin method to disaggregate the series. A high rho parameter takes into account that the two series are unlikely to be co-integrated.\nm.d.stocks &lt;- td(gdp.q ~ spi.d, method = \"chow-lin-fixed\", fixed.rho = 0.9)\nsummary(m.d.stocks)\n##\n## Call:\n## td(formula = gdp.q ~ spi.d, method = \"chow-lin-fixed\", fixed.rho = 0.9)\n##\n## Residuals:\n##    Min     1Q Median     3Q    Max\n## -10656  -1760   1076   3796   8891\n##\n## Coefficients:\n##              Estimate Std. Error t value Pr(&gt;|t|)\n## (Intercept) 1.320e+03  2.856e+01   46.22   &lt;2e-16 ***\n## spi.d       5.512e-02  3.735e-03   14.76   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n##\n## 'chow-lin-fixed' disaggregation with 'sum' conversion\n## 59 low-freq. obs. converted to 5493 high-freq. obs.\n## Adjusted R-squared: 0.7928 AR1-Parameter:   0.9\nAnd here is the result: A daily series of GDP\ngdp.d.stocks &lt;- predict(m.d.stocks)\nts_plot(\n  ts_scale(\n    ts_c(gdp.d.stocks, gdp.q)\n  ),\n  title = \"Daily disaggregated GDP\",\n  subtitle = \"one indicator\"\n)\n\n\n\nSwiss GDP, disaggregated to daily\n\n\n\nLike with all disaggregation methods in tempdisagg, the resulting series fulfills the aggregation constraint (the resulting series is as long as the indicator, and needs to be shortened for a comparison):\nall.equal(\n  ts_span(\n    ts_frequency(gdp.d.stocks, \"quarter\", aggregate = \"sum\"),\n    end = \"2019-07-01\"\n  ),\n  gdp.q\n)\n## [1] TRUE"
  },
  {
    "objectID": "blog/posts/2021-03-16-gfortran-macos/index.html",
    "href": "blog/posts/2021-03-16-gfortran-macos/index.html",
    "title": "gfortran support for R on macOS",
    "section": "",
    "text": "For a long time, gfortran support on macOS could be achieved by installing the homebrew cask gfortran via brew cask install gfortran.\nAs of 2021, both the brew cask command and the cask gfortran are deprecated. Users who have installed this cask already, will not notice since things will continue to work as normal. Only new users who want to install gfortran this way, will get the message that the cask is “not available”. The cask was removed in December 2020 and merged into the gcc formula (which can be installed via brew install gcc). Now, one could go to https://github.com/fxcoudert/gfortran-for-macOS/releases and manually install the respective .dmg file. However, this is not a long-term approach, and usually, one would like to do this via brew, the most popular package manager for macOS.\nUnfortunately, this change did not result in a smooth experience for R users who want to compile packages from source that require a functional gfortran compiler. This requirement does not occur very often, as most users install R package binaries on macOS. These do not require a working gfortran installation.\nHowever, in some cases, when calling install.packages(), a working gfortran installation is needed. And if type = \"source\" is used, it needs to be there.\nThe issue after the integration of gfortran into the gcc formula is that the official R binary installer for macOS expects the gfortran installation at /usr/local/gfortran. This was fulfilled by the old gfortran cask but is not by the new gcc integration. Hence, trying to install the “cluster” package via install.packages(\"cluster\", type = \"source\") will fail and gfortran will not be found:\nThere was a discussion about these changes in the homebrew PR, but the comments that highlighted potential issues seem to have gone unnoticed. Also, some workarounds posted in the thread do not work."
  },
  {
    "objectID": "blog/posts/2021-03-16-gfortran-macos/index.html#so-how-does-one-now-install-gfortran-on-macos-these-days",
    "href": "blog/posts/2021-03-16-gfortran-macos/index.html#so-how-does-one-now-install-gfortran-on-macos-these-days",
    "title": "gfortran support for R on macOS",
    "section": "So how does one now install gfortran on macOS these days?",
    "text": "So how does one now install gfortran on macOS these days?\nIt is likely that one will not need the workaround presented below in the future since it will probably be fixed in the R installer at some point (hopefully). In the meantime, the following helps:\n\nCreate a file ~/.R/Makevars (if it does not exist yet)\nAdd the following to ~/.R/Makevars\nFC      = usr/local/opt/gcc/bin/gfortran\nF77     = /usr/local/opt/gcc/bin/gfortran\nFLIBS   = -L/usr/local/opt/gcc/lib\nRestart R\nTest the changes by calling install.packages(\"cluster\", type = \"source\")\n\nThe output should look like this\n* installing *source* package ‘cluster’ ...\n** package ‘cluster’ successfully unpacked and MD5 sums checked\n** using staged installation\n** libs\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c clara.c -o clara.o\n/usr/local/opt/gcc/bin/gfortran -fno-optimize-sibling-calls  -fPIC  -Wall -g -O2  -c daisy.f -o daisy.o\n/usr/local/opt/gcc/bin/gfortran -fno-optimize-sibling-calls  -fPIC  -Wall -g -O2  -c dysta.f -o dysta.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c fanny.c -o fanny.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c init.c -o init.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c mona.c -o mona.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c pam.c -o pam.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c sildist.c -o sildist.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c spannel.c -o spannel.o\nclang -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I/usr/local/include   -fPIC  -Wall -g -O2  -c twins.c -o twins.o\nclang -dynamiclib -Wl,-headerpad_max_install_names -undefined dynamic_lookup -single_module -multiply_defined suppress -L/Library/Frameworks/R.framework/Resources/lib -L/usr/local/lib -o cluster.so clara.o daisy.o dysta.o fanny.o init.o mona.o pam.o sildist.o spannel.o twins.o -L/usr/local/opt/gcc/lib -F/Library/Frameworks/R.framework/.. -framework R -Wl,-framework -Wl,CoreFoundation\nld: warning: object file (daisy.o) was built for newer macOS version (11.2) than being linked (11.0)\nld: warning: object file (dysta.o) was built for newer macOS version (11.2) than being linked (11.0)\ninstalling to /Users/pjs/Library/R/4.0/library/00LOCK-cluster/00new/cluster/libs\n** R\n** data\n*** moving datasets to lazyload DB\n** inst\n** byte-compile and prepare package for lazy loading\n** help\n*** installing help indices\n** building package indices\n** testing if installed package can be loaded from temporary location\n** checking absolute paths in shared objects and dynamic libraries\n** testing if installed package can be loaded from final location\n** testing if installed package keeps a record of temporary installation path\n* DONE (cluster)\nCaution: After using this approach for a few days, I have seen issues with certain packages (e.g. hsdar). It is unclear if the issues trace back to the packages or the new way of using gfortran. You might want to re-think using this proposed approach and eventually manually install the linked standalone gfortran binary shown earlier in this post."
  },
  {
    "objectID": "blog/posts/2021-03-16-gfortran-macos/index.html#notes",
    "href": "blog/posts/2021-03-16-gfortran-macos/index.html#notes",
    "title": "gfortran support for R on macOS",
    "section": "Notes",
    "text": "Notes\n\nI am not sure about the ld: warning: object file (dysta.o) was built for newer macOS version (11.2) than being linked (11.0) warning, but it does not seem to have a practical impact.\nThis approach was tested with R 4.0.4, macOS 11.2.3 in March 2021.\nIf you still have the old gfortran cask installed, you may want to switch to the new approach as the cask is no longer being updated. Hence, you will run a very outdated gfortran at some point without noticing. You can remove the old cask with brew remove – cask gfortran."
  },
  {
    "objectID": "blog/posts/2018-05-15-tsbox/index.html",
    "href": "blog/posts/2018-05-15-tsbox/index.html",
    "title": "Time series of the world, unite!",
    "section": "",
    "text": "The R ecosystem knows a ridiculous number of time series classes. So, I decided to create a new universal standard that finally covers everyone’s use case…\nOk, just kidding!\ntsbox, just realeased on CRAN, provides a set of tools that are agnostic towards existing time series classes. It is built around a set of converters, which convert time series stored as ts, xts, data.frame, data.table, tibble, zoo, tsibble or timeSeries to each other.\nTo install the stable version from CRAN:\ninstall.packages(\"tsbox\")\nTo get an idea how easy it is to switch from one class to another, consider this:\nlibrary(tsbox)\nx.ts &lt;- ts_c(mdeaths, fdeaths)\nx.xts &lt;- ts_xts(x.ts)\nx.df &lt;- ts_df(x.xts)\nx.tbl &lt;- ts_tbl(x.df)\nx.dt &lt;- ts_tbl(x.tbl)\nx.zoo &lt;- ts_zoo(x.dt)\nx.tsibble &lt;- ts_tsibble(x.zoo)\nx.timeSeries &lt;- ts_timeSeries(x.tsibble)\nWe jump form good old ts objects toxts, store our time series in various data frames and convert them to some highly specialized time series formats.\n\ntsbox is class-agnostic\nBecause these converters work nicely, we can use them to make functions class-agnostic. If a class-agnostic function works for one class, it works for all:\nts_scale(x.ts)\nts_scale(x.xts)\nts_scale(x.df)\nts_scale(x.dt)\nts_scale(x.tbl)\nts_scale normalizes one or multiple series, by subtracting the mean and dividing by the standard deviation. It works like a ‘generic’ function: You can apply it on any time series object, and it will return an object of the same class as its input.\nSo, whether we want to smooth, scale, differentiate, chain-link, forecast, regularize, or seasonally adjust a series, we can use the same commands to whatever time series at hand. tsbox offers a comprehensive toolkit for the basics of time series manipulation. Here are some additional operations:\nts_pc(x.ts)                 # percentage change rates\nts_forecast(x.xts)          # forecast, by exponential smoothing\nts_seas(x.df)               # seasonal adjustment, by X-13\nts_frequency(x.dt, \"year\")  # convert to annual frequency\nts_span(x.tbl, \"-1 year\")   # limit time span to final year\n\n\ntsbox is frequency-agnostic\nThere are many more. Because they all start with ts_, you can use auto-complete to see what’s around. Most conveniently, there is a time series plot function that works for all classes and frequencies:\nts_plot(\n  `Airline Passengers` = AirPassengers,\n  `Lynx trappings` = ts_df(lynx),\n  `Deaths from Lung Diseases` = ts_xts(fdeaths),\n  title = \"Airlines, trappings, and deaths\",\n  subtitle = \"Monthly passengers, annual trappings, monthly deaths\"\n)\n\n\n\nwww.dataseries.org\n\n\nThere is also a version that uses ggplot2 and has the same syntax.\n\n\nTime series in data frames\nYou may have wondered why we treated data frames as a time series class. The spread of dplyr and data.table has given data frames a boost and made them one of the most popular data structures in R. So, storing time series in a data frame is an obvious consequence. And even if you don’t intend to keep time series in data frames, this is still the format in which you import and export your data. tsbox makes it easy to switch from data frames to time series and back.\n\n\nMake existing functions class-agnostic\ntsbox includes tools to make existing functions class-agnostic. To do so, the ts_ function can be used to wrap any function that works with time series. For a function that works on \"ts\" objects, this is as simple as that:\nts_rowsums &lt;- ts_(rowSums)\nts_rowsums(ts_c(mdeaths, fdeaths))\nNote that ts_ returns a function, which can be used with or without a name.\nIn case you are wondering, tsbox uses data.table as a backend and makes use of its incredibly efficient reshaping facilities, its joins, and rolling joins. And thanks to anytime, tsbox will be able to recognize almost any date format without manual intervention.\nSo, if you’ve been struggling with R’s time series class, we hope that you found this blog helpful.\nWebsite: www.tsbox.help"
  }
]