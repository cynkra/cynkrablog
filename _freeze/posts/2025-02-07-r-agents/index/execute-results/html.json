{
  "hash": "6c8aee219dd3cee48bfaab178672e957",
  "result": {
    "engine": "knitr",
    "markdown": "---\ndate: 2025-02-07\nlayout: post\nimage: michael-dziedzic-aQYgUYwnCsM-unsplash.jpg\nauthor: Christoph Sax\ntitle: 'Playing with AI Agents in R'\ncategories:\n- R\n- LLM\n---\n\n\n\n\n\n\n## Intro\n\nIt's local LLM time! ðŸ¥³ What an adventure it has been since I first started [exploring local LLMs](https://blog.cynkra.com/posts/2024-07-27-llama3.1/). With the introduction of various new Llama models, we now have impressive small and large models that run seamlessly on consumer hardware. With deepseek R1, we have access to remarkably good MIT-licensed reasoning models that rival the top models in the industry.\n\nAnd now, we've got a fully-specced M4 Mac Mini for our office, which runs all of this like a charm.\n\nIn this post, I will explore how we can use R to run agent models [through Ollama](https://ollama.com).\n\n## What is an agent?\n\nUnlike normal chats, agents can use tools to do tasks. Tools are pieces of software that can do anything, and it is up to the user to construct the agentic workflow they want. With R, we have a powerful language to configure these workflows and tell the tools what to do.\n\nA typical agentic workflow looks like this:\n\n1. You tell an LLM what you want in plain English and the tools it has available\n2. The LLM figures out what tools it needs to use\n3. The agent uses the tools to get the job done\n4. Iterate, improve or continue with other tasks\n5. Let the LLM explain what it did\n\n## R and Ollama\nFor the following experiments with agents, I'm using plain R and Ollama (which you have to install first, but it's [straightforward.](https://blog.cynkra.com/posts/2024-07-27-llama3.1/))\n\nI know that there is the R package [ellmer](https://ellmer.tidyverse.org), and you can do many things that I am doing here with it.\nWhile [ellmer](https://ellmer.tidyverse.org) is a great package, I prefer to work directly with the Ollama API. In my opinion, this gives us more transparency into the agent mechanics.\n\nMy experiments with agents in R can be found in the [ragent](https://github.com/cynkra/ragent) repository on GitHub. To install the package, use:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nremotes::install_github(\"cynkra/ragent\")\n```\n:::\n\n\n\n\nI will use diagrams to show the conversation flow and the tools that are used.\n\n## A simple chat\n\nGetting started is straightforward. In [llm_chat()](https://github.com/cynkra/ragent/blob/main/R/llm_chat.R), I've used the `httr2` package to wrap the Ollama API. You can use the function to perform normal chats:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ragent)\nllm_chat(\"What is the capital of France?\")\n#> Paris is the capital of France.\n```\n:::\n\n```{mermaid}\nsequenceDiagram\n    autonumber\n    participant User\n    participant LLM\n\n    User->>LLM: Question\n    Note over LLM: No tools needed\n    LLM-->>User: Direct response\n```\n\n\n\n\n\nNo surprise here: The user asks a question (1) and the LLM answers (2). By default, I am using the [`llama3.2:3b`](https://ollama.com/library/llama3.2) model, because it's fast and good enough for this demo. But I can change it to any other, more powerful model that Ollama supports. E.g,:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_chat(\"What is the capital of France?\", model = \"deepseek-r1:14b\")\n```\n:::\n\n\n\n\n\n## Structured output\n\nThe ollama API supports structured output. We can use this to tell the LLM to return a specific format. For example:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_chat(\"What is the capital of France?\", format = list(\n  type = \"object\",\n  properties = list(\n    country = list(type = \"string\"),\n    capital = list(type = \"string\")\n  ),\n  required = c(\"country\", \"capital\")\n))\n#> $country\n#> [1] \"France\"\n#>\n#> $capital\n#> [1] \"Paris\"\n```\n:::\n\n\n\n\nThis structured response makes it easy to process the output programmatically in R, as it will be automatically converted to a list with named elements. It is often a good idea to add a system prompt to guide the LLM to the desired output.\n\n\n## A agent that can calculate\n\nThis is now a sweet setup for tinkering. We can use R to work with this output to do more complex things.\n\nNote that LLMs are not really good at calculating. The small Llama model that I am using here messes this up:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nllm_chat(\"What is 2.111^2.111\")\n#> [1] \"To calculate this, we can use the formula for squaring a binomial:\\n\\n(a + b)^2 = a^2 + 2ab + b^2\\n\\nIn this case, a = 2 and b = 1.111.\\n\\nSo, \\n\\n(2 + 1.111)^2 = \\n(3.111)^2 = 9.377226\\n2*3.111 = 6.222442\\n6.222442 + 9.377226 = 15.600668\\n\\nTherefore, 2.111^2.111 â‰ˆ 15.60\"\n```\n:::\n\n\n\n\nBut we can tell the LLM to use a calculator tool to do the calculation in R. The result   [looks like this](https://github.com/cynkra/ragent/blob/main/R/agent_calculator.R):\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nagent_calculator(\"What is 2.111^2.111\")\n#> **Calculation Result:** 2.111^2.111 = 4.84166414903285\n```\n:::\n\n```{mermaid}\nsequenceDiagram\n    autonumber\n    participant User\n    participant Agent\n    participant Calculator\n\n    User->>Agent: Math question\n    Note over Agent: Analyzes question<br/>Decides to use calculator\n    Agent->>Calculator: Structured calculation request\n    Calculator-->>Agent: Numerical result\n    Note over Agent: Formats response\n    Agent-->>User: Explained calculation\n```\n\n\n\n\nMuch better! This is an agent that:\n\n1. Analyzes the question\n2. Decides to use the calculator tool\n3. Uses the calculator tool to calculate the result in R\n4. Returns the response to the user\n\n## A smarter RAG\n\nA simple RAG (Retrieval Augmented Generation) enhances LLM responses by first searching a knowledge base for relevant information. But our improved RAG goes a step further: after the initial search, the LLM formulates additional questions to gather more context, leading to more comprehensive answers.\n\nIn [our smarter RAG](https://github.com/cynkra/ragent/blob/main/R/agent_rag.R), we build an agent that:\n\n- Search the knowledge base for relevant passages (2)\n- Ask the LLM to provide additional questions to search for (5)\n- Search the knowledge base for the additional questions (7)\n- Use the context to answer the question (10)\n\n\n\n\n```{mermaid}\nsequenceDiagram\n    autonumber\n    participant User\n    participant Agent\n    participant RAG\n    participant Docs\n\n    User->>Agent: Question about docs\n    Note over Agent: Analyzes question\n    Agent->>RAG: Initial search\n    RAG->>Docs: Search in directory\n    Docs-->>RAG: Initial passages\n    RAG-->>Agent: Initial context\n    Note over Agent: Formulates follow-up<br/>questions for context\n    Agent->>RAG: Additional searches\n    RAG->>Docs: Search with new questions\n    Docs-->>RAG: Additional passages\n    RAG-->>Agent: Combined context\n    Note over Agent: Synthesizes all<br/>information\n    Agent-->>User: Complete response<br/>with citations\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nagent_rag(\"What is ggplot2 used for?\", dir = \"docs/\")\n#> Based on the documentation:\n#> ggplot2 is a core package of the tidyverse used for visualization.\n#> [Source: tidyverse.md]\n```\n:::\n\n\n\n\nThis is just the beginning. We can build much more complex agents. For example, we could create a general agent that combines both the calculator and RAG capabilities. This agent would analyze each question and automatically choose the right approach - either searching the knowledge base or performing calculations.\n\n## Conclusion\n\nR is a powerful language for building AI agents. Its functional nature makes tools feel like Lego bricks that we can combine. The way R handles lists and data frames fits nicely with the JSON returned from the API, and the rich R ecosystem provides everything we need to build powerful agents. But what I found most useful is R's interactive workflow that allows me to watch and debug the workings of the LLMs and the tools step by step.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}